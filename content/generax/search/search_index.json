{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"generax \u00a4 generax provides implementations of flow based generative models. The library is built on top of Equinox which removes the need to worry about keeping track of model parameters. key = random . PRNGKey ( 0 ) # JAX random key x = ... # some data # Create a flow model model = NeuralSpline ( input_shape = x . shape [ 1 :], n_flow_layers = 3 , n_blocks = 4 , hidden_size = 32 , working_size = 16 , n_spline_knots = 8 , key = key ) # Data dependent initialization model = model . data_dependent_init ( x , key = key ) # Take multiple samples using vmap keys = random . split ( key , 1000 ) samples = eqx . filter_vmap ( model . sample )( keys ) # Compute the log probability of data log_prob = eqx . filter_vmap ( model . log_prob )( x ) There is also support for probability paths (time-dependent probability distributions) which can be used to train continuous normalizing flows with flow matching. See the examples on flow matching and multi-sample flow matching for more details. Installation \u00a4 generax is available on pip: pip install generax Training \u00a4 Generax provides an easy interface to train these models: trainer = Trainer ( checkpoint_path = 'tmp/model_path' ) model = trainer . train ( model = model , # Generax model objective = my_objective , # Objective function evaluate_model = tester , # Testing function optimizer = optimizer , # Optax optimizer num_steps = 10000 , # Number of training steps data_iterator = train_ds , # Training data iterator double_batch = 1000 , # Train these many batches in a scan loop checkpoint_every = 1000 , # Checkpoint interval test_every = 1000 , # Test interval retrain = True ) # Retrain from checkpoint See the examples folder for more details.","title":"generax"},{"location":"#generax","text":"generax provides implementations of flow based generative models. The library is built on top of Equinox which removes the need to worry about keeping track of model parameters. key = random . PRNGKey ( 0 ) # JAX random key x = ... # some data # Create a flow model model = NeuralSpline ( input_shape = x . shape [ 1 :], n_flow_layers = 3 , n_blocks = 4 , hidden_size = 32 , working_size = 16 , n_spline_knots = 8 , key = key ) # Data dependent initialization model = model . data_dependent_init ( x , key = key ) # Take multiple samples using vmap keys = random . split ( key , 1000 ) samples = eqx . filter_vmap ( model . sample )( keys ) # Compute the log probability of data log_prob = eqx . filter_vmap ( model . log_prob )( x ) There is also support for probability paths (time-dependent probability distributions) which can be used to train continuous normalizing flows with flow matching. See the examples on flow matching and multi-sample flow matching for more details.","title":"generax"},{"location":"#installation","text":"generax is available on pip: pip install generax","title":"Installation"},{"location":"#training","text":"Generax provides an easy interface to train these models: trainer = Trainer ( checkpoint_path = 'tmp/model_path' ) model = trainer . train ( model = model , # Generax model objective = my_objective , # Objective function evaluate_model = tester , # Testing function optimizer = optimizer , # Optax optimizer num_steps = 10000 , # Number of training steps data_iterator = train_ds , # Training data iterator double_batch = 1000 , # Train these many batches in a scan loop checkpoint_every = 1000 , # Checkpoint interval test_every = 1000 , # Test interval retrain = True ) # Retrain from checkpoint See the examples folder for more details.","title":"Training"},{"location":"api/distributions/coupling/","text":"Couplings \u00a4 generax.distributions.coupling.JointCoupling \u00a4 Given two batches of samples from two distributions, this will compute a discrete distribution as done in multisample flow matching \\[q(x_0,x_1) = \\sum_{i,j}\\pi_{i,j}\\delta(x_0 - x_0^i)\\delta(x_1 - x_1^j))\\] Source code in generax/distributions/coupling.py class JointCoupling ( eqx . Module , ABC ): \"\"\"Given two batches of samples from two distributions, this will compute a discrete distribution as done in [multisample flow matching](https://arxiv.org/pdf/2304.14772.pdf) $$q(x_0,x_1) = \\sum_{i,j}\\pi_{i,j}\\delta(x_0 - x_0^i)\\delta(x_1 - x_1^j))$$ \"\"\" batch_size : int x0 : Array x1 : Array logits : Array def __init__ ( self , x0 : Array , x1 : Array ): \"\"\"Initialize the coupling **Arguments**: - x0: A batch of samples from p(x_0) - x1: A batch of samples from p(x_1) \"\"\" self . batch_size = x0 . shape [ 0 ] self . x0 = x0 self . x1 = x1 self . logits = self . compute_logits () assert self . logits . shape == ( self . batch_size , self . batch_size ) @abstractmethod def compute_logits ( self ): \"\"\"Compute $\\log \\pi_{i,j}$\"\"\" pass def sample_x0_given_x1 ( self , rng_key : PRNGKeyArray ) -> Array : \"\"\"Resample from the coupling **Arguments**: - rng_key: The random number generator key **Returns**: A sample from q(x_0|x_1) \"\"\" idx = jax . random . categorical ( rng_key , self . logits , axis = 0 ) return self . x0 [ idx ] __init__ ( self , x0 : Array , x1 : Array ) \u00a4 Initialize the coupling Arguments : - x0: A batch of samples from p(x_0) - x1: A batch of samples from p(x_1) Source code in generax/distributions/coupling.py def __init__ ( self , x0 : Array , x1 : Array ): \"\"\"Initialize the coupling **Arguments**: - x0: A batch of samples from p(x_0) - x1: A batch of samples from p(x_1) \"\"\" self . batch_size = x0 . shape [ 0 ] self . x0 = x0 self . x1 = x1 self . logits = self . compute_logits () assert self . logits . shape == ( self . batch_size , self . batch_size ) compute_logits ( self ) abstractmethod \u00a4 Compute \\(\\log \\pi_{i,j}\\) Source code in generax/distributions/coupling.py @abstractmethod def compute_logits ( self ): \"\"\"Compute $\\log \\pi_{i,j}$\"\"\" pass sample_x0_given_x1 ( self , rng_key : PRNGKeyArray ) -> Array \u00a4 Resample from the coupling Arguments : - rng_key: The random number generator key Returns : A sample from q(x_0|x_1) Source code in generax/distributions/coupling.py def sample_x0_given_x1 ( self , rng_key : PRNGKeyArray ) -> Array : \"\"\"Resample from the coupling **Arguments**: - rng_key: The random number generator key **Returns**: A sample from q(x_0|x_1) \"\"\" idx = jax . random . categorical ( rng_key , self . logits , axis = 0 ) return self . x0 [ idx ] generax.distributions.coupling.UniformCoupling ( JointCoupling ) \u00a4 This is a uniform coupling between two distributions Source code in generax/distributions/coupling.py class UniformCoupling ( JointCoupling ): \"\"\"This is a uniform coupling between two distributions\"\"\" def compute_logits ( self ) -> Array : \"\"\"Compute the logits for the coupling\"\"\" return jnp . ones (( self . batch_size , self . batch_size )) / self . batch_size def sample_x0_given_x1 ( self , rng_key : PRNGKeyArray ) -> Array : return self . x0 __init__ ( self , x0 : Array , x1 : Array ) \u00a4 Initialize the coupling Arguments : - x0: A batch of samples from p(x_0) - x1: A batch of samples from p(x_1) Source code in generax/distributions/coupling.py def __init__ ( self , x0 : Array , x1 : Array ): \"\"\"Initialize the coupling **Arguments**: - x0: A batch of samples from p(x_0) - x1: A batch of samples from p(x_1) \"\"\" self . batch_size = x0 . shape [ 0 ] self . x0 = x0 self . x1 = x1 self . logits = self . compute_logits () assert self . logits . shape == ( self . batch_size , self . batch_size ) compute_logits ( self ) -> Array \u00a4 Compute the logits for the coupling Source code in generax/distributions/coupling.py def compute_logits ( self ) -> Array : \"\"\"Compute the logits for the coupling\"\"\" return jnp . ones (( self . batch_size , self . batch_size )) / self . batch_size generax.distributions.coupling.OTTCoupling ( JointCoupling ) \u00a4 Optimal transport coupling using the ott library . This class uses the sinkhorn solver to compute the optimal transport coupling. Source code in generax/distributions/coupling.py class OTTCoupling ( JointCoupling ): \"\"\"Optimal transport coupling using the [ott library](https://ott-jax.readthedocs.io/en/latest/). This class uses the sinkhorn solver to compute the optimal transport coupling. \"\"\" def compute_logits ( self ) -> Array : \"\"\"Solve for the optimal transport couplings\"\"\" # Create a point cloud object geom = pointcloud . PointCloud ( self . x0 , self . x1 ) # Define the loss function ot_prob = linear_problem . LinearProblem ( geom ) # Create a sinkhorn solver solver = sinkhorn . Sinkhorn () # Solve the OT problem ot = solver ( ot_prob ) # Return the coupling mat = ot . matrix return jnp . log ( mat + 1e-8 ) __init__ ( self , x0 : Array , x1 : Array ) \u00a4 Initialize the coupling Arguments : - x0: A batch of samples from p(x_0) - x1: A batch of samples from p(x_1) Source code in generax/distributions/coupling.py def __init__ ( self , x0 : Array , x1 : Array ): \"\"\"Initialize the coupling **Arguments**: - x0: A batch of samples from p(x_0) - x1: A batch of samples from p(x_1) \"\"\" self . batch_size = x0 . shape [ 0 ] self . x0 = x0 self . x1 = x1 self . logits = self . compute_logits () assert self . logits . shape == ( self . batch_size , self . batch_size ) compute_logits ( self ) -> Array \u00a4 Solve for the optimal transport couplings Source code in generax/distributions/coupling.py def compute_logits ( self ) -> Array : \"\"\"Solve for the optimal transport couplings\"\"\" # Create a point cloud object geom = pointcloud . PointCloud ( self . x0 , self . x1 ) # Define the loss function ot_prob = linear_problem . LinearProblem ( geom ) # Create a sinkhorn solver solver = sinkhorn . Sinkhorn () # Solve the OT problem ot = solver ( ot_prob ) # Return the coupling mat = ot . matrix return jnp . log ( mat + 1e-8 )","title":"Couplings"},{"location":"api/distributions/coupling/#couplings","text":"","title":"Couplings"},{"location":"api/distributions/coupling/#generax.distributions.coupling.JointCoupling","text":"Given two batches of samples from two distributions, this will compute a discrete distribution as done in multisample flow matching \\[q(x_0,x_1) = \\sum_{i,j}\\pi_{i,j}\\delta(x_0 - x_0^i)\\delta(x_1 - x_1^j))\\] Source code in generax/distributions/coupling.py class JointCoupling ( eqx . Module , ABC ): \"\"\"Given two batches of samples from two distributions, this will compute a discrete distribution as done in [multisample flow matching](https://arxiv.org/pdf/2304.14772.pdf) $$q(x_0,x_1) = \\sum_{i,j}\\pi_{i,j}\\delta(x_0 - x_0^i)\\delta(x_1 - x_1^j))$$ \"\"\" batch_size : int x0 : Array x1 : Array logits : Array def __init__ ( self , x0 : Array , x1 : Array ): \"\"\"Initialize the coupling **Arguments**: - x0: A batch of samples from p(x_0) - x1: A batch of samples from p(x_1) \"\"\" self . batch_size = x0 . shape [ 0 ] self . x0 = x0 self . x1 = x1 self . logits = self . compute_logits () assert self . logits . shape == ( self . batch_size , self . batch_size ) @abstractmethod def compute_logits ( self ): \"\"\"Compute $\\log \\pi_{i,j}$\"\"\" pass def sample_x0_given_x1 ( self , rng_key : PRNGKeyArray ) -> Array : \"\"\"Resample from the coupling **Arguments**: - rng_key: The random number generator key **Returns**: A sample from q(x_0|x_1) \"\"\" idx = jax . random . categorical ( rng_key , self . logits , axis = 0 ) return self . x0 [ idx ]","title":"JointCoupling"},{"location":"api/distributions/coupling/#generax.distributions.coupling.UniformCoupling","text":"This is a uniform coupling between two distributions Source code in generax/distributions/coupling.py class UniformCoupling ( JointCoupling ): \"\"\"This is a uniform coupling between two distributions\"\"\" def compute_logits ( self ) -> Array : \"\"\"Compute the logits for the coupling\"\"\" return jnp . ones (( self . batch_size , self . batch_size )) / self . batch_size def sample_x0_given_x1 ( self , rng_key : PRNGKeyArray ) -> Array : return self . x0","title":"UniformCoupling"},{"location":"api/distributions/coupling/#generax.distributions.coupling.OTTCoupling","text":"Optimal transport coupling using the ott library . This class uses the sinkhorn solver to compute the optimal transport coupling. Source code in generax/distributions/coupling.py class OTTCoupling ( JointCoupling ): \"\"\"Optimal transport coupling using the [ott library](https://ott-jax.readthedocs.io/en/latest/). This class uses the sinkhorn solver to compute the optimal transport coupling. \"\"\" def compute_logits ( self ) -> Array : \"\"\"Solve for the optimal transport couplings\"\"\" # Create a point cloud object geom = pointcloud . PointCloud ( self . x0 , self . x1 ) # Define the loss function ot_prob = linear_problem . LinearProblem ( geom ) # Create a sinkhorn solver solver = sinkhorn . Sinkhorn () # Solve the OT problem ot = solver ( ot_prob ) # Return the coupling mat = ot . matrix return jnp . log ( mat + 1e-8 )","title":"OTTCoupling"},{"location":"api/distributions/distributions/","text":"Base \u00a4 generax.distributions.base.ProbabilityDistribution \u00a4 An object that we can sample from and use to evaluate log probabilities. This is an abstract base class. Atributes : input_shape : The shape of samples. Methods : sample_and_log_prob(key) -> (x,log_px) : Sample from the distribution and compute the log probability. sample(key) -> x : Pull a single sample from the model log_prob(x) -> log_px : Compute the log probability of a point under the model score(x) -> dlog_px/dx : Compute the gradient of the log probability of a point under the model Source code in generax/distributions/base.py class ProbabilityDistribution ( eqx . Module , ABC ): \"\"\"An object that we can sample from and use to evaluate log probabilities. This is an abstract base class. **Atributes**: - `input_shape`: The shape of samples. **Methods**: - `sample_and_log_prob(key) -> (x,log_px)`: Sample from the distribution and compute the log probability. - `sample(key) -> x`: Pull a single sample from the model - `log_prob(x) -> log_px`: Compute the log probability of a point under the model - `score(x) -> dlog_px/dx`: Compute the gradient of the log probability of a point under the model \"\"\" input_shape : int = eqx . field ( static = True ) def __init__ ( self , * , input_shape : Tuple [ int ], ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The dimensions of the samples \"\"\" assert isinstance ( input_shape , tuple ) or isinstance ( input_shape , list ) self . input_shape = input_shape @abstractmethod def sample_and_log_prob ( self , key : PRNGKeyArray , y : Optional [ Array ] = None ) -> Array : \"\"\"**Arguments**: - `key`: The random number generator key. **Returns**: A single sample from the model with its log probability. Use eqx.filter_vmap to get more samples! For example, ```python keys = random.split(key, n_samples) x, log_px = eqx.filter_vmap(self.sample_and_log_prob)(keys) ``` \"\"\" pass def sample ( self , key : PRNGKeyArray , y : Optional [ Array ] = None ) -> Array : \"\"\" **Arguments**: - `key`: The random number generator key. **Returns**: Samples from the model Use eqx.filter_vmap to get more samples! For example, ```python keys = random.split(key, n_samples) samples = eqx.filter_vmap(self.sample)(keys) ``` \"\"\" return self . sample_and_log_prob ( key , y )[ 0 ] @abstractmethod def log_prob ( self , x : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array : \"\"\"**Arguments**: - `x`: The point we want to compute logp(x) at. - `y`: The (optional) conditioning information. - `key`: The random number generator key. Can be passed in the event that we're getting a stochastic estimate of the log prob. **Returns**: The log likelihood of x under the model. \"\"\" pass def score ( self , x : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array : \"\"\"**Arguments**: - `x`: The point we want to compute grad logp(x) at. - `y`: The (optional) conditioning information. - `key`: The random number generator key. Can be passed in the event that we're getting a stochastic estimate of the log prob. **Returns**: The log likelihood of x under the model. \"\"\" return eqx . filter_grad ( self . log_prob )( x , y = y , key = key ) def energy ( self , x : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array : return - self . log_prob ( x , y = y , key = key ) __init__ ( self , * , input_shape : Tuple [ int ], ** kwargs ) \u00a4 Arguments : input_shape : The dimensions of the samples Source code in generax/distributions/base.py def __init__ ( self , * , input_shape : Tuple [ int ], ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The dimensions of the samples \"\"\" assert isinstance ( input_shape , tuple ) or isinstance ( input_shape , list ) self . input_shape = input_shape sample_and_log_prob ( self , key : PRNGKeyArray , y : Optional [ Array ] = None ) -> Array abstractmethod \u00a4 Arguments : key : The random number generator key. Returns : A single sample from the model with its log probability. Use eqx.filter_vmap to get more samples! For example, keys = random . split ( key , n_samples ) x , log_px = eqx . filter_vmap ( self . sample_and_log_prob )( keys ) Source code in generax/distributions/base.py @abstractmethod def sample_and_log_prob ( self , key : PRNGKeyArray , y : Optional [ Array ] = None ) -> Array : \"\"\"**Arguments**: - `key`: The random number generator key. **Returns**: A single sample from the model with its log probability. Use eqx.filter_vmap to get more samples! For example, ```python keys = random.split(key, n_samples) x, log_px = eqx.filter_vmap(self.sample_and_log_prob)(keys) ``` \"\"\" pass sample ( self , key : PRNGKeyArray , y : Optional [ Array ] = None ) -> Array \u00a4 Arguments : key : The random number generator key. Returns : Samples from the model Use eqx.filter_vmap to get more samples! For example, keys = random . split ( key , n_samples ) samples = eqx . filter_vmap ( self . sample )( keys ) Source code in generax/distributions/base.py def sample ( self , key : PRNGKeyArray , y : Optional [ Array ] = None ) -> Array : \"\"\" **Arguments**: - `key`: The random number generator key. **Returns**: Samples from the model Use eqx.filter_vmap to get more samples! For example, ```python keys = random.split(key, n_samples) samples = eqx.filter_vmap(self.sample)(keys) ``` \"\"\" return self . sample_and_log_prob ( key , y )[ 0 ] log_prob ( self , x : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array abstractmethod \u00a4 Arguments : x : The point we want to compute logp(x) at. y : The (optional) conditioning information. key : The random number generator key. Can be passed in the event that we're getting a stochastic estimate of the log prob. Returns : The log likelihood of x under the model. Source code in generax/distributions/base.py @abstractmethod def log_prob ( self , x : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array : \"\"\"**Arguments**: - `x`: The point we want to compute logp(x) at. - `y`: The (optional) conditioning information. - `key`: The random number generator key. Can be passed in the event that we're getting a stochastic estimate of the log prob. **Returns**: The log likelihood of x under the model. \"\"\" pass score ( self , x : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array \u00a4 Arguments : x : The point we want to compute grad logp(x) at. y : The (optional) conditioning information. key : The random number generator key. Can be passed in the event that we're getting a stochastic estimate of the log prob. Returns : The log likelihood of x under the model. Source code in generax/distributions/base.py def score ( self , x : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array : \"\"\"**Arguments**: - `x`: The point we want to compute grad logp(x) at. - `y`: The (optional) conditioning information. - `key`: The random number generator key. Can be passed in the event that we're getting a stochastic estimate of the log prob. **Returns**: The log likelihood of x under the model. \"\"\" return eqx . filter_grad ( self . log_prob )( x , y = y , key = key ) generax.distributions.base.BoltzmannDistribution ( ProbabilityDistribution ) \u00a4 An unnormalized probability density function. p(x) = 1/Z*exp(-E(x)) Atributes : input_shape : The shape of samples. Methods : energy(x) -> E : Compute the energy of a point under the model score(x) -> grad log_px = -grad E : Compute the gradient of the log probability of a point under the model Source code in generax/distributions/base.py class BoltzmannDistribution ( ProbabilityDistribution ): \"\"\"An unnormalized probability density function. p(x) = 1/Z*exp(-E(x)) **Atributes**: - `input_shape`: The shape of samples. **Methods**: - `energy(x) -> E`: Compute the energy of a point under the model - `score(x) -> grad log_px = -grad E`: Compute the gradient of the log probability of a point under the model \"\"\" def sample_and_log_prob ( self , key : PRNGKeyArray , y : Optional [ Array ] = None ) -> Array : raise AssertionError ( \"Can't sample from a Boltzmann distribution\" ) def log_prob ( self , x : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array : raise AssertionError ( \"Can't compute log prob of a Boltzmann distribution\" ) @abstractmethod def energy ( self , x : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array : \"\"\"**Arguments**: - `x`: The point we want to compute E(x) at. - `y`: The (optional) conditioning information. - `key`: The random number generator key. Can be passed in the event that we're getting a stochastic estimate of the energy. **Returns**: The energy of x under the model. \"\"\" pass def score ( self , x : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array : \"\"\"**Arguments**: - `x`: The point we want to compute grad logp(x) at. - `y`: The (optional) conditioning information. - `key`: The random number generator key. Can be passed in the event that we're getting a stochastic estimate of the log prob. **Returns**: The log likelihood of x under the model. \"\"\" return - eqx . filter_grad ( self . energy )( x , y = y , key = key ) __init__ ( self , * , input_shape : Tuple [ int ], ** kwargs ) \u00a4 Arguments : input_shape : The dimensions of the samples Source code in generax/distributions/base.py def __init__ ( self , * , input_shape : Tuple [ int ], ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The dimensions of the samples \"\"\" assert isinstance ( input_shape , tuple ) or isinstance ( input_shape , list ) self . input_shape = input_shape energy ( self , x : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array abstractmethod \u00a4 Arguments : x : The point we want to compute E(x) at. y : The (optional) conditioning information. key : The random number generator key. Can be passed in the event that we're getting a stochastic estimate of the energy. Returns : The energy of x under the model. Source code in generax/distributions/base.py @abstractmethod def energy ( self , x : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array : \"\"\"**Arguments**: - `x`: The point we want to compute E(x) at. - `y`: The (optional) conditioning information. - `key`: The random number generator key. Can be passed in the event that we're getting a stochastic estimate of the energy. **Returns**: The energy of x under the model. \"\"\" pass score ( self , x : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array \u00a4 Arguments : x : The point we want to compute grad logp(x) at. y : The (optional) conditioning information. key : The random number generator key. Can be passed in the event that we're getting a stochastic estimate of the log prob. Returns : The log likelihood of x under the model. Source code in generax/distributions/base.py def score ( self , x : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array : \"\"\"**Arguments**: - `x`: The point we want to compute grad logp(x) at. - `y`: The (optional) conditioning information. - `key`: The random number generator key. Can be passed in the event that we're getting a stochastic estimate of the log prob. **Returns**: The log likelihood of x under the model. \"\"\" return - eqx . filter_grad ( self . energy )( x , y = y , key = key ) generax.distributions.base.ProductDistribution ( ProbabilityDistribution ) \u00a4 A product of probability distributions Source code in generax/distributions/base.py class ProductDistribution ( ProbabilityDistribution ): \"\"\"A product of probability distributions \"\"\" dists : Tuple [ ProbabilityDistribution ] def __init__ ( self , * distributions : Tuple [ ProbabilityDistribution ], ** kwargs ): \"\"\"**Arguments**: - `distributions`: The distributions to take the product of. \"\"\" self . dists = distributions # Check that the input shapes are all the same on all but the # first axis and construct the total input shape input_shape = list ( self . dists [ 0 ] . input_shape ) input_shape_end = self . dists [ 0 ] . input_shape [ 1 :] for dist in self . dists [ 1 :]: assert dist . input_shape [ 1 :] == input_shape_end input_shape [ 0 ] += dist . input_shape [ 0 ] input_shape = tuple ( input_shape ) super () . __init__ ( input_shape = input_shape , ** kwargs ) def sample_and_log_prob ( self , key : PRNGKeyArray , y : Optional [ Array ] = None ) -> Array : \"\"\"**Arguments**: - `key`: The random number generator key. **Returns**: A single sample from the model with its log probability. Use eqx.filter_vmap to get more samples! For example, ```python keys = random.split(key, n_samples) x, log_px = eqx.filter_vmap(self.sample_and_log_prob)(keys) ``` \"\"\" # Sample from each of our distributions keys = random . split ( key , len ( self . dists )) xs = [] log_px = 0.0 for i , key in enumerate ( keys ): x , _log_px = self . dists [ i ] . sample_and_log_prob ( key , y = y ) xs . append ( x ) log_px += _log_px # Concatenate the samples along the first axis x = jnp . concatenate ( xs , axis = 0 ) return x , log_px def log_prob ( self , x : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array : \"\"\"**Arguments**: - `x`: The point we want to compute logp(x) at. - `y`: The (optional) conditioning information. - `key`: The random number generator key. Can be passed in the event that we're getting a stochastic estimate of the log prob. **Returns**: The log likelihood of x under the model. \"\"\" assert x . shape == self . input_shape # Figure out how to split the input split_indices = jnp . cumsum ( jnp . array ([ 0 ] + [ dist . input_shape [ 0 ] for dist in self . dists ])) splits = list ( zip ( split_indices [: - 1 ], split_indices [ 1 :])) # Compute the log prob of each sample log_px = 0.0 for i , ( start , end ) in enumerate ( splits ): _x = x [ start : end ] log_px += self . dists [ i ] . log_prob ( _x , y = y , key = key ) return log_px sample ( self , key : PRNGKeyArray , y : Optional [ Array ] = None ) -> Array \u00a4 Inherited from generax.distributions.base.ProbabilityDistribution.sample . Source code in generax/distributions/base.py def sample ( self , key : PRNGKeyArray , y : Optional [ Array ] = None ) -> Array : \"\"\" **Arguments**: - `key`: The random number generator key. **Returns**: Samples from the model Use eqx.filter_vmap to get more samples! For example, ```python keys = random.split(key, n_samples) samples = eqx.filter_vmap(self.sample)(keys) ``` \"\"\" return self . sample_and_log_prob ( key , y )[ 0 ] score ( self , x : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array \u00a4 Inherited from generax.distributions.base.ProbabilityDistribution.score . Source code in generax/distributions/base.py def score ( self , x : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array : \"\"\"**Arguments**: - `x`: The point we want to compute grad logp(x) at. - `y`: The (optional) conditioning information. - `key`: The random number generator key. Can be passed in the event that we're getting a stochastic estimate of the log prob. **Returns**: The log likelihood of x under the model. \"\"\" return eqx . filter_grad ( self . log_prob )( x , y = y , key = key ) __init__ ( self , * distributions : Tuple [ ProbabilityDistribution ], ** kwargs ) \u00a4 Arguments : distributions : The distributions to take the product of. Source code in generax/distributions/base.py def __init__ ( self , * distributions : Tuple [ ProbabilityDistribution ], ** kwargs ): \"\"\"**Arguments**: - `distributions`: The distributions to take the product of. \"\"\" self . dists = distributions # Check that the input shapes are all the same on all but the # first axis and construct the total input shape input_shape = list ( self . dists [ 0 ] . input_shape ) input_shape_end = self . dists [ 0 ] . input_shape [ 1 :] for dist in self . dists [ 1 :]: assert dist . input_shape [ 1 :] == input_shape_end input_shape [ 0 ] += dist . input_shape [ 0 ] input_shape = tuple ( input_shape ) super () . __init__ ( input_shape = input_shape , ** kwargs ) sample_and_log_prob ( self , key : PRNGKeyArray , y : Optional [ Array ] = None ) -> Array \u00a4 Implements generax.distributions.base.ProbabilityDistribution.sample_and_log_prob . Source code in generax/distributions/base.py def sample_and_log_prob ( self , key : PRNGKeyArray , y : Optional [ Array ] = None ) -> Array : \"\"\"**Arguments**: - `key`: The random number generator key. **Returns**: A single sample from the model with its log probability. Use eqx.filter_vmap to get more samples! For example, ```python keys = random.split(key, n_samples) x, log_px = eqx.filter_vmap(self.sample_and_log_prob)(keys) ``` \"\"\" # Sample from each of our distributions keys = random . split ( key , len ( self . dists )) xs = [] log_px = 0.0 for i , key in enumerate ( keys ): x , _log_px = self . dists [ i ] . sample_and_log_prob ( key , y = y ) xs . append ( x ) log_px += _log_px # Concatenate the samples along the first axis x = jnp . concatenate ( xs , axis = 0 ) return x , log_px log_prob ( self , x : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array \u00a4 Implements generax.distributions.base.ProbabilityDistribution.log_prob . Source code in generax/distributions/base.py def log_prob ( self , x : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array : \"\"\"**Arguments**: - `x`: The point we want to compute logp(x) at. - `y`: The (optional) conditioning information. - `key`: The random number generator key. Can be passed in the event that we're getting a stochastic estimate of the log prob. **Returns**: The log likelihood of x under the model. \"\"\" assert x . shape == self . input_shape # Figure out how to split the input split_indices = jnp . cumsum ( jnp . array ([ 0 ] + [ dist . input_shape [ 0 ] for dist in self . dists ])) splits = list ( zip ( split_indices [: - 1 ], split_indices [ 1 :])) # Compute the log prob of each sample log_px = 0.0 for i , ( start , end ) in enumerate ( splits ): _x = x [ start : end ] log_px += self . dists [ i ] . log_prob ( _x , y = y , key = key ) return log_px generax.distributions.base.EmpiricalDistribution ( ProbabilityDistribution ) \u00a4 An empirical distribution. This can be used as a wrapper around data Source code in generax/distributions/base.py class EmpiricalDistribution ( ProbabilityDistribution ): \"\"\"An empirical distribution. This can be used as a wrapper around data \"\"\" data : Array def __init__ ( self , data : Array , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The dimensions of the samples \"\"\" self . data = data input_shape = data . shape [ 1 :] super () . __init__ ( input_shape = input_shape , ** kwargs ) def sample_and_log_prob ( self ): assert 0 , \"Can't compute\" def log_prob ( self ): assert 0 , \"Can't compute\" def sample ( self , key : PRNGKeyArray , y : Optional [ Array ] = None ) -> Array : \"\"\" **Arguments**: - `key`: The random number generator key. **Returns**: Samples from the model Use eqx.filter_vmap to get more samples! For example, ```python keys = random.split(key, n_samples) samples = eqx.filter_vmap(self.sample)(keys) ``` \"\"\" return random . choice ( key , self . data , shape = ( 1 ,))[ 0 ] def train_iterator ( self , key : PRNGKeyArray , batch_size : int ) -> Mapping [ str , Array ]: \"\"\"An iterator over the training data. This is compatible with the Trainer class in this package. Use like: ```python train_iter = empirical_dist.train_iterator(key, batch_size=128) data_batch = next(train_iter) ``` **Arguments**: - `key`: The random number generator key. - `batch_size`: The batch size. **Returns**: An iterator over the training data that yields a dictionary with the key `x` and the value the training data. \"\"\" total_choices = jnp . arange ( self . data . shape [ 0 ]) while True : key , _ = random . split ( key , 2 ) idx = random . choice ( key , total_choices , shape = ( batch_size ,), replace = True ) yield dict ( x = self . data [ idx ]) __init__ ( self , data : Array , ** kwargs ) \u00a4 Arguments : input_shape : The dimensions of the samples Source code in generax/distributions/base.py def __init__ ( self , data : Array , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The dimensions of the samples \"\"\" self . data = data input_shape = data . shape [ 1 :] super () . __init__ ( input_shape = input_shape , ** kwargs ) sample ( self , key : PRNGKeyArray , y : Optional [ Array ] = None ) -> Array \u00a4 Arguments : key : The random number generator key. Returns : Samples from the model Use eqx.filter_vmap to get more samples! For example, keys = random . split ( key , n_samples ) samples = eqx . filter_vmap ( self . sample )( keys ) Source code in generax/distributions/base.py def sample ( self , key : PRNGKeyArray , y : Optional [ Array ] = None ) -> Array : \"\"\" **Arguments**: - `key`: The random number generator key. **Returns**: Samples from the model Use eqx.filter_vmap to get more samples! For example, ```python keys = random.split(key, n_samples) samples = eqx.filter_vmap(self.sample)(keys) ``` \"\"\" return random . choice ( key , self . data , shape = ( 1 ,))[ 0 ] train_iterator ( self , key : PRNGKeyArray , batch_size : int ) -> Mapping [ str , Array ] \u00a4 An iterator over the training data. This is compatible with the Trainer class in this package. Use like: train_iter = empirical_dist . train_iterator ( key , batch_size = 128 ) data_batch = next ( train_iter ) Arguments : key : The random number generator key. batch_size : The batch size. Returns : An iterator over the training data that yields a dictionary with the key x and the value the training data. Source code in generax/distributions/base.py def train_iterator ( self , key : PRNGKeyArray , batch_size : int ) -> Mapping [ str , Array ]: \"\"\"An iterator over the training data. This is compatible with the Trainer class in this package. Use like: ```python train_iter = empirical_dist.train_iterator(key, batch_size=128) data_batch = next(train_iter) ``` **Arguments**: - `key`: The random number generator key. - `batch_size`: The batch size. **Returns**: An iterator over the training data that yields a dictionary with the key `x` and the value the training data. \"\"\" total_choices = jnp . arange ( self . data . shape [ 0 ]) while True : key , _ = random . split ( key , 2 ) idx = random . choice ( key , total_choices , shape = ( batch_size ,), replace = True ) yield dict ( x = self . data [ idx ]) generax.distributions.base.ProbabilityPath ( ProbabilityDistribution ) \u00a4 A time dependent probability distribution. Atributes : input_shape : The dimension of the sampling space. Methods : sample_and_log_prob(key) -> (x,log_px) : Sample from the distribution and compute the log probability. sample(key) -> x : Pull a single sample from the model log_prob(x) -> log_px : Compute the log probability of a point under the model Source code in generax/distributions/base.py class ProbabilityPath ( ProbabilityDistribution ): \"\"\"A time dependent probability distribution. **Atributes**: - `input_shape`: The dimension of the sampling space. **Methods**: - `sample_and_log_prob(key) -> (x,log_px)`: Sample from the distribution and compute the log probability. - `sample(key) -> x`: Pull a single sample from the model - `log_prob(x) -> log_px`: Compute the log probability of a point under the model \"\"\" input_shape : int = eqx . field ( static = True ) def __init__ ( self , * , input_shape : Tuple [ int ], ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The dimensions of the samples \"\"\" assert isinstance ( input_shape , tuple ) or isinstance ( input_shape , list ) self . input_shape = input_shape @abstractmethod def sample_and_log_prob ( self , t : Array , key : PRNGKeyArray ) -> Array : \"\"\"**Arguments**: - `t`: The time at which we want to sample. - `key`: The random number generator key. **Returns**: A single sample from the model with its log probability. \"\"\" pass @abstractmethod def log_prob ( self , t : Array , xt : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array : \"\"\"**Arguments**: - `t`: The time at which we want to sample. - `xt`: The point we want to compute logp(x) at. - `y`: The (optional) conditioning information. - `key`: The random number generator key. Can be passed in the event that we're getting a stochastic estimate of the log prob. **Returns**: The log likelihood of x under the model. \"\"\" pass def sample ( self , t : Array , key : PRNGKeyArray , y : Optional [ Array ] = None ) -> Array : \"\"\" Use eqx.filter_vmap to get more samples! For example, keys = random.split(key, n_samples) samples = eqx.filter_vmap(self.sample, in_axes=(None, 0))(t, keys) **Arguments**: - `t`: The time at which we want to sample. - `key`: The random number generator key. - `n_samples`: The number of samples to draw. If `None`, then we just draw a single sample. **Returns**: Samples from the model \"\"\" return self . sample_and_log_prob ( t , key , y )[ 0 ] def score ( self , t : Array , xt : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array : \"\"\"**Arguments**: - `x`: The point we want to compute grad logp(x) at. - `y`: The (optional) conditioning information. - `key`: The random number generator key. Can be passed in the event that we're getting a stochastic estimate of the log prob. **Returns**: The log likelihood of x under the model. \"\"\" def log_prob ( xt ): return self . log_prob ( t , xt , y = y , key = key ) return eqx . filter_grad ( log_prob )( xt ) @abstractmethod def transform_and_vector_field ( self , t : Array , x0 : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"The vector field that samples evolve on as t changes **Arguments**: - `t`: Time. - `x0`: A point in the base space. - `y`: The (optional) conditioning information. **Returns**: (xt, dxt/dt) \"\"\" pass @abstractmethod def vector_field ( self , t : Array , xt : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"The vector field that samples evolve on as t changes **Arguments**: - `t`: Time. - `xt`: A point in the base space. - `y`: The (optional) conditioning information. **Returns**: dxt/dt \"\"\" pass __init__ ( self , * , input_shape : Tuple [ int ], ** kwargs ) \u00a4 Arguments : input_shape : The dimensions of the samples Source code in generax/distributions/base.py def __init__ ( self , * , input_shape : Tuple [ int ], ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The dimensions of the samples \"\"\" assert isinstance ( input_shape , tuple ) or isinstance ( input_shape , list ) self . input_shape = input_shape sample_and_log_prob ( self , t : Array , key : PRNGKeyArray ) -> Array abstractmethod \u00a4 Arguments : t : The time at which we want to sample. key : The random number generator key. Returns : A single sample from the model with its log probability. Source code in generax/distributions/base.py @abstractmethod def sample_and_log_prob ( self , t : Array , key : PRNGKeyArray ) -> Array : \"\"\"**Arguments**: - `t`: The time at which we want to sample. - `key`: The random number generator key. **Returns**: A single sample from the model with its log probability. \"\"\" pass log_prob ( self , t : Array , xt : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array abstractmethod \u00a4 Arguments : t : The time at which we want to sample. xt : The point we want to compute logp(x) at. y : The (optional) conditioning information. key : The random number generator key. Can be passed in the event that we're getting a stochastic estimate of the log prob. Returns : The log likelihood of x under the model. Source code in generax/distributions/base.py @abstractmethod def log_prob ( self , t : Array , xt : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array : \"\"\"**Arguments**: - `t`: The time at which we want to sample. - `xt`: The point we want to compute logp(x) at. - `y`: The (optional) conditioning information. - `key`: The random number generator key. Can be passed in the event that we're getting a stochastic estimate of the log prob. **Returns**: The log likelihood of x under the model. \"\"\" pass sample ( self , t : Array , key : PRNGKeyArray , y : Optional [ Array ] = None ) -> Array \u00a4 Use eqx.filter_vmap to get more samples! For example, keys = random.split(key, n_samples) samples = eqx.filter_vmap(self.sample, in_axes=(None, 0))(t, keys) Arguments : t : The time at which we want to sample. key : The random number generator key. n_samples : The number of samples to draw. If None , then we just draw a single sample. Returns : Samples from the model Source code in generax/distributions/base.py def sample ( self , t : Array , key : PRNGKeyArray , y : Optional [ Array ] = None ) -> Array : \"\"\" Use eqx.filter_vmap to get more samples! For example, keys = random.split(key, n_samples) samples = eqx.filter_vmap(self.sample, in_axes=(None, 0))(t, keys) **Arguments**: - `t`: The time at which we want to sample. - `key`: The random number generator key. - `n_samples`: The number of samples to draw. If `None`, then we just draw a single sample. **Returns**: Samples from the model \"\"\" return self . sample_and_log_prob ( t , key , y )[ 0 ] score ( self , t : Array , xt : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array \u00a4 Arguments : x : The point we want to compute grad logp(x) at. y : The (optional) conditioning information. key : The random number generator key. Can be passed in the event that we're getting a stochastic estimate of the log prob. Returns : The log likelihood of x under the model. Source code in generax/distributions/base.py def score ( self , t : Array , xt : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array : \"\"\"**Arguments**: - `x`: The point we want to compute grad logp(x) at. - `y`: The (optional) conditioning information. - `key`: The random number generator key. Can be passed in the event that we're getting a stochastic estimate of the log prob. **Returns**: The log likelihood of x under the model. \"\"\" def log_prob ( xt ): return self . log_prob ( t , xt , y = y , key = key ) return eqx . filter_grad ( log_prob )( xt ) transform_and_vector_field ( self , t : Array , x0 : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array abstractmethod \u00a4 The vector field that samples evolve on as t changes Arguments : t : Time. x0 : A point in the base space. y : The (optional) conditioning information. Returns : (xt, dxt/dt) Source code in generax/distributions/base.py @abstractmethod def transform_and_vector_field ( self , t : Array , x0 : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"The vector field that samples evolve on as t changes **Arguments**: - `t`: Time. - `x0`: A point in the base space. - `y`: The (optional) conditioning information. **Returns**: (xt, dxt/dt) \"\"\" pass vector_field ( self , t : Array , xt : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array abstractmethod \u00a4 The vector field that samples evolve on as t changes Arguments : t : Time. xt : A point in the base space. y : The (optional) conditioning information. Returns : dxt/dt Source code in generax/distributions/base.py @abstractmethod def vector_field ( self , t : Array , xt : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"The vector field that samples evolve on as t changes **Arguments**: - `t`: Time. - `xt`: A point in the base space. - `y`: The (optional) conditioning information. **Returns**: dxt/dt \"\"\" pass generax.distributions.base.Gaussian ( ProbabilityDistribution ) \u00a4 This represents a Gaussian distribution Source code in generax/distributions/base.py class Gaussian ( ProbabilityDistribution ): \"\"\"This represents a Gaussian distribution\"\"\" def sample ( self , key : PRNGKeyArray , y : Optional [ Array ] = None ) -> Array : \"\"\"**Arguments**: - `key`: The random number generator key. **Returns**: A single sample from the model. Use eqx.filter_vmap to get more samples. \"\"\" return random . normal ( key , shape = self . input_shape ) def log_prob ( self , x : Array , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The point we want to compute logp(x) at. **Returns**: The log likelihood of x under the model. \"\"\" assert x . shape == self . input_shape return jax . scipy . stats . norm . logpdf ( x ) . sum () def sample_and_log_prob ( self , key : PRNGKeyArray , ** kwargs ) -> Array : \"\"\"**Arguments**: - `key`: The random number generator key. **Returns**: A single sample from the model with its log probability. \"\"\" x = self . sample ( key ) log_px = self . log_prob ( x ) return x , log_px __init__ ( self , * , input_shape : Tuple [ int ], ** kwargs ) \u00a4 Arguments : input_shape : The dimensions of the samples Source code in generax/distributions/base.py def __init__ ( self , * , input_shape : Tuple [ int ], ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The dimensions of the samples \"\"\" assert isinstance ( input_shape , tuple ) or isinstance ( input_shape , list ) self . input_shape = input_shape sample ( self , key : PRNGKeyArray , y : Optional [ Array ] = None ) -> Array \u00a4 Arguments : key : The random number generator key. Returns : A single sample from the model. Use eqx.filter_vmap to get more samples. Source code in generax/distributions/base.py def sample ( self , key : PRNGKeyArray , y : Optional [ Array ] = None ) -> Array : \"\"\"**Arguments**: - `key`: The random number generator key. **Returns**: A single sample from the model. Use eqx.filter_vmap to get more samples. \"\"\" return random . normal ( key , shape = self . input_shape ) log_prob ( self , x : Array , ** kwargs ) -> Array \u00a4 Arguments : x : The point we want to compute logp(x) at. Returns : The log likelihood of x under the model. Source code in generax/distributions/base.py def log_prob ( self , x : Array , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The point we want to compute logp(x) at. **Returns**: The log likelihood of x under the model. \"\"\" assert x . shape == self . input_shape return jax . scipy . stats . norm . logpdf ( x ) . sum ()","title":"Base"},{"location":"api/distributions/distributions/#base","text":"","title":"Base"},{"location":"api/distributions/distributions/#generax.distributions.base.ProbabilityDistribution","text":"An object that we can sample from and use to evaluate log probabilities. This is an abstract base class. Atributes : input_shape : The shape of samples. Methods : sample_and_log_prob(key) -> (x,log_px) : Sample from the distribution and compute the log probability. sample(key) -> x : Pull a single sample from the model log_prob(x) -> log_px : Compute the log probability of a point under the model score(x) -> dlog_px/dx : Compute the gradient of the log probability of a point under the model Source code in generax/distributions/base.py class ProbabilityDistribution ( eqx . Module , ABC ): \"\"\"An object that we can sample from and use to evaluate log probabilities. This is an abstract base class. **Atributes**: - `input_shape`: The shape of samples. **Methods**: - `sample_and_log_prob(key) -> (x,log_px)`: Sample from the distribution and compute the log probability. - `sample(key) -> x`: Pull a single sample from the model - `log_prob(x) -> log_px`: Compute the log probability of a point under the model - `score(x) -> dlog_px/dx`: Compute the gradient of the log probability of a point under the model \"\"\" input_shape : int = eqx . field ( static = True ) def __init__ ( self , * , input_shape : Tuple [ int ], ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The dimensions of the samples \"\"\" assert isinstance ( input_shape , tuple ) or isinstance ( input_shape , list ) self . input_shape = input_shape @abstractmethod def sample_and_log_prob ( self , key : PRNGKeyArray , y : Optional [ Array ] = None ) -> Array : \"\"\"**Arguments**: - `key`: The random number generator key. **Returns**: A single sample from the model with its log probability. Use eqx.filter_vmap to get more samples! For example, ```python keys = random.split(key, n_samples) x, log_px = eqx.filter_vmap(self.sample_and_log_prob)(keys) ``` \"\"\" pass def sample ( self , key : PRNGKeyArray , y : Optional [ Array ] = None ) -> Array : \"\"\" **Arguments**: - `key`: The random number generator key. **Returns**: Samples from the model Use eqx.filter_vmap to get more samples! For example, ```python keys = random.split(key, n_samples) samples = eqx.filter_vmap(self.sample)(keys) ``` \"\"\" return self . sample_and_log_prob ( key , y )[ 0 ] @abstractmethod def log_prob ( self , x : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array : \"\"\"**Arguments**: - `x`: The point we want to compute logp(x) at. - `y`: The (optional) conditioning information. - `key`: The random number generator key. Can be passed in the event that we're getting a stochastic estimate of the log prob. **Returns**: The log likelihood of x under the model. \"\"\" pass def score ( self , x : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array : \"\"\"**Arguments**: - `x`: The point we want to compute grad logp(x) at. - `y`: The (optional) conditioning information. - `key`: The random number generator key. Can be passed in the event that we're getting a stochastic estimate of the log prob. **Returns**: The log likelihood of x under the model. \"\"\" return eqx . filter_grad ( self . log_prob )( x , y = y , key = key ) def energy ( self , x : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array : return - self . log_prob ( x , y = y , key = key )","title":"ProbabilityDistribution"},{"location":"api/distributions/distributions/#generax.distributions.base.BoltzmannDistribution","text":"An unnormalized probability density function. p(x) = 1/Z*exp(-E(x)) Atributes : input_shape : The shape of samples. Methods : energy(x) -> E : Compute the energy of a point under the model score(x) -> grad log_px = -grad E : Compute the gradient of the log probability of a point under the model Source code in generax/distributions/base.py class BoltzmannDistribution ( ProbabilityDistribution ): \"\"\"An unnormalized probability density function. p(x) = 1/Z*exp(-E(x)) **Atributes**: - `input_shape`: The shape of samples. **Methods**: - `energy(x) -> E`: Compute the energy of a point under the model - `score(x) -> grad log_px = -grad E`: Compute the gradient of the log probability of a point under the model \"\"\" def sample_and_log_prob ( self , key : PRNGKeyArray , y : Optional [ Array ] = None ) -> Array : raise AssertionError ( \"Can't sample from a Boltzmann distribution\" ) def log_prob ( self , x : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array : raise AssertionError ( \"Can't compute log prob of a Boltzmann distribution\" ) @abstractmethod def energy ( self , x : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array : \"\"\"**Arguments**: - `x`: The point we want to compute E(x) at. - `y`: The (optional) conditioning information. - `key`: The random number generator key. Can be passed in the event that we're getting a stochastic estimate of the energy. **Returns**: The energy of x under the model. \"\"\" pass def score ( self , x : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array : \"\"\"**Arguments**: - `x`: The point we want to compute grad logp(x) at. - `y`: The (optional) conditioning information. - `key`: The random number generator key. Can be passed in the event that we're getting a stochastic estimate of the log prob. **Returns**: The log likelihood of x under the model. \"\"\" return - eqx . filter_grad ( self . energy )( x , y = y , key = key )","title":"BoltzmannDistribution"},{"location":"api/distributions/distributions/#generax.distributions.base.ProductDistribution","text":"A product of probability distributions Source code in generax/distributions/base.py class ProductDistribution ( ProbabilityDistribution ): \"\"\"A product of probability distributions \"\"\" dists : Tuple [ ProbabilityDistribution ] def __init__ ( self , * distributions : Tuple [ ProbabilityDistribution ], ** kwargs ): \"\"\"**Arguments**: - `distributions`: The distributions to take the product of. \"\"\" self . dists = distributions # Check that the input shapes are all the same on all but the # first axis and construct the total input shape input_shape = list ( self . dists [ 0 ] . input_shape ) input_shape_end = self . dists [ 0 ] . input_shape [ 1 :] for dist in self . dists [ 1 :]: assert dist . input_shape [ 1 :] == input_shape_end input_shape [ 0 ] += dist . input_shape [ 0 ] input_shape = tuple ( input_shape ) super () . __init__ ( input_shape = input_shape , ** kwargs ) def sample_and_log_prob ( self , key : PRNGKeyArray , y : Optional [ Array ] = None ) -> Array : \"\"\"**Arguments**: - `key`: The random number generator key. **Returns**: A single sample from the model with its log probability. Use eqx.filter_vmap to get more samples! For example, ```python keys = random.split(key, n_samples) x, log_px = eqx.filter_vmap(self.sample_and_log_prob)(keys) ``` \"\"\" # Sample from each of our distributions keys = random . split ( key , len ( self . dists )) xs = [] log_px = 0.0 for i , key in enumerate ( keys ): x , _log_px = self . dists [ i ] . sample_and_log_prob ( key , y = y ) xs . append ( x ) log_px += _log_px # Concatenate the samples along the first axis x = jnp . concatenate ( xs , axis = 0 ) return x , log_px def log_prob ( self , x : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array : \"\"\"**Arguments**: - `x`: The point we want to compute logp(x) at. - `y`: The (optional) conditioning information. - `key`: The random number generator key. Can be passed in the event that we're getting a stochastic estimate of the log prob. **Returns**: The log likelihood of x under the model. \"\"\" assert x . shape == self . input_shape # Figure out how to split the input split_indices = jnp . cumsum ( jnp . array ([ 0 ] + [ dist . input_shape [ 0 ] for dist in self . dists ])) splits = list ( zip ( split_indices [: - 1 ], split_indices [ 1 :])) # Compute the log prob of each sample log_px = 0.0 for i , ( start , end ) in enumerate ( splits ): _x = x [ start : end ] log_px += self . dists [ i ] . log_prob ( _x , y = y , key = key ) return log_px","title":"ProductDistribution"},{"location":"api/distributions/distributions/#generax.distributions.base.EmpiricalDistribution","text":"An empirical distribution. This can be used as a wrapper around data Source code in generax/distributions/base.py class EmpiricalDistribution ( ProbabilityDistribution ): \"\"\"An empirical distribution. This can be used as a wrapper around data \"\"\" data : Array def __init__ ( self , data : Array , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The dimensions of the samples \"\"\" self . data = data input_shape = data . shape [ 1 :] super () . __init__ ( input_shape = input_shape , ** kwargs ) def sample_and_log_prob ( self ): assert 0 , \"Can't compute\" def log_prob ( self ): assert 0 , \"Can't compute\" def sample ( self , key : PRNGKeyArray , y : Optional [ Array ] = None ) -> Array : \"\"\" **Arguments**: - `key`: The random number generator key. **Returns**: Samples from the model Use eqx.filter_vmap to get more samples! For example, ```python keys = random.split(key, n_samples) samples = eqx.filter_vmap(self.sample)(keys) ``` \"\"\" return random . choice ( key , self . data , shape = ( 1 ,))[ 0 ] def train_iterator ( self , key : PRNGKeyArray , batch_size : int ) -> Mapping [ str , Array ]: \"\"\"An iterator over the training data. This is compatible with the Trainer class in this package. Use like: ```python train_iter = empirical_dist.train_iterator(key, batch_size=128) data_batch = next(train_iter) ``` **Arguments**: - `key`: The random number generator key. - `batch_size`: The batch size. **Returns**: An iterator over the training data that yields a dictionary with the key `x` and the value the training data. \"\"\" total_choices = jnp . arange ( self . data . shape [ 0 ]) while True : key , _ = random . split ( key , 2 ) idx = random . choice ( key , total_choices , shape = ( batch_size ,), replace = True ) yield dict ( x = self . data [ idx ])","title":"EmpiricalDistribution"},{"location":"api/distributions/distributions/#generax.distributions.base.ProbabilityPath","text":"A time dependent probability distribution. Atributes : input_shape : The dimension of the sampling space. Methods : sample_and_log_prob(key) -> (x,log_px) : Sample from the distribution and compute the log probability. sample(key) -> x : Pull a single sample from the model log_prob(x) -> log_px : Compute the log probability of a point under the model Source code in generax/distributions/base.py class ProbabilityPath ( ProbabilityDistribution ): \"\"\"A time dependent probability distribution. **Atributes**: - `input_shape`: The dimension of the sampling space. **Methods**: - `sample_and_log_prob(key) -> (x,log_px)`: Sample from the distribution and compute the log probability. - `sample(key) -> x`: Pull a single sample from the model - `log_prob(x) -> log_px`: Compute the log probability of a point under the model \"\"\" input_shape : int = eqx . field ( static = True ) def __init__ ( self , * , input_shape : Tuple [ int ], ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The dimensions of the samples \"\"\" assert isinstance ( input_shape , tuple ) or isinstance ( input_shape , list ) self . input_shape = input_shape @abstractmethod def sample_and_log_prob ( self , t : Array , key : PRNGKeyArray ) -> Array : \"\"\"**Arguments**: - `t`: The time at which we want to sample. - `key`: The random number generator key. **Returns**: A single sample from the model with its log probability. \"\"\" pass @abstractmethod def log_prob ( self , t : Array , xt : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array : \"\"\"**Arguments**: - `t`: The time at which we want to sample. - `xt`: The point we want to compute logp(x) at. - `y`: The (optional) conditioning information. - `key`: The random number generator key. Can be passed in the event that we're getting a stochastic estimate of the log prob. **Returns**: The log likelihood of x under the model. \"\"\" pass def sample ( self , t : Array , key : PRNGKeyArray , y : Optional [ Array ] = None ) -> Array : \"\"\" Use eqx.filter_vmap to get more samples! For example, keys = random.split(key, n_samples) samples = eqx.filter_vmap(self.sample, in_axes=(None, 0))(t, keys) **Arguments**: - `t`: The time at which we want to sample. - `key`: The random number generator key. - `n_samples`: The number of samples to draw. If `None`, then we just draw a single sample. **Returns**: Samples from the model \"\"\" return self . sample_and_log_prob ( t , key , y )[ 0 ] def score ( self , t : Array , xt : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array : \"\"\"**Arguments**: - `x`: The point we want to compute grad logp(x) at. - `y`: The (optional) conditioning information. - `key`: The random number generator key. Can be passed in the event that we're getting a stochastic estimate of the log prob. **Returns**: The log likelihood of x under the model. \"\"\" def log_prob ( xt ): return self . log_prob ( t , xt , y = y , key = key ) return eqx . filter_grad ( log_prob )( xt ) @abstractmethod def transform_and_vector_field ( self , t : Array , x0 : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"The vector field that samples evolve on as t changes **Arguments**: - `t`: Time. - `x0`: A point in the base space. - `y`: The (optional) conditioning information. **Returns**: (xt, dxt/dt) \"\"\" pass @abstractmethod def vector_field ( self , t : Array , xt : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"The vector field that samples evolve on as t changes **Arguments**: - `t`: Time. - `xt`: A point in the base space. - `y`: The (optional) conditioning information. **Returns**: dxt/dt \"\"\" pass","title":"ProbabilityPath"},{"location":"api/distributions/distributions/#generax.distributions.base.Gaussian","text":"This represents a Gaussian distribution Source code in generax/distributions/base.py class Gaussian ( ProbabilityDistribution ): \"\"\"This represents a Gaussian distribution\"\"\" def sample ( self , key : PRNGKeyArray , y : Optional [ Array ] = None ) -> Array : \"\"\"**Arguments**: - `key`: The random number generator key. **Returns**: A single sample from the model. Use eqx.filter_vmap to get more samples. \"\"\" return random . normal ( key , shape = self . input_shape ) def log_prob ( self , x : Array , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The point we want to compute logp(x) at. **Returns**: The log likelihood of x under the model. \"\"\" assert x . shape == self . input_shape return jax . scipy . stats . norm . logpdf ( x ) . sum () def sample_and_log_prob ( self , key : PRNGKeyArray , ** kwargs ) -> Array : \"\"\"**Arguments**: - `key`: The random number generator key. **Returns**: A single sample from the model with its log probability. \"\"\" x = self . sample ( key ) log_px = self . log_prob ( x ) return x , log_px","title":"Gaussian"},{"location":"api/distributions/flow_models/","text":"Flows \u00a4 generax.distributions.flow_models.NormalizingFlow ( ProbabilityDistribution ) \u00a4 A normalizing flow is a model that we use to represent probability distributions. See this for an overview. Atributes : transform : A BijectiveTransform object that transforms a variable from the base space to the data space and also computes the change is log pdf. transform(x) -> (z,log_det) : Apply the transformation to the input. prior : The prior probability distribution. Methods : to_base_space(x) -> z : Transform a point from the data space to the base space. sample_and_log_prob(key) -> (x,log_px) : Sample from the distribution and compute the log probability. sample(key) -> x : Pull a single sample from the model log_prob(x) -> log_px : Compute the log probability of a point under the model Source code in generax/distributions/flow_models.py class NormalizingFlow ( ProbabilityDistribution , ABC ): \"\"\"A normalizing flow is a model that we use to represent probability distributions. See [this](https://arxiv.org/pdf/1912.02762.pdf) for an overview. **Atributes**: - `transform`: A `BijectiveTransform` object that transforms a variable from the base space to the data space and also computes the change is log pdf. - `transform(x) -> (z,log_det)`: Apply the transformation to the input. - `prior`: The prior probability distribution. **Methods**: - `to_base_space(x) -> z`: Transform a point from the data space to the base space. - `sample_and_log_prob(key) -> (x,log_px)`: Sample from the distribution and compute the log probability. - `sample(key) -> x`: Pull a single sample from the model - `log_prob(x) -> log_px`: Compute the log probability of a point under the model \"\"\" transform : BijectiveTransform prior : ProbabilityDistribution def __init__ ( self , transform : BijectiveTransform , prior : ProbabilityDistribution , ** kwargs ): \"\"\"**Arguments**: - `transform`: A bijective transformation - `prior`: The prior distribution \"\"\" self . transform = transform self . prior = prior input_shape = self . transform . input_shape super () . __init__ ( input_shape = input_shape , ** kwargs ) def to_base_space ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: A JAX array with shape `(dim,)`. - `y`: The conditioning information **Returns**: A JAX array with shape `(dim,)`. \"\"\" return self . transform ( x , y = y , ** kwargs )[ 0 ] def to_data_space ( self , z : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"**Arguments**: - `z`: A JAX array with shape `(dim,)`. - `y`: The conditioning information **Returns**: A JAX array with shape `(dim,)`. \"\"\" return self . transform ( z , y = y , inverse = True , ** kwargs )[ 0 ] def sample_and_log_prob ( self , key : PRNGKeyArray , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"**Arguments**: - `key`: The random number generator key. - `y`: The conditioning information **Returns**: A single sample from the model. Use vmap to get more samples. \"\"\" z , log_pz = self . prior . sample_and_log_prob ( key ) x , log_det = self . transform ( z , y = y , inverse = True , ** kwargs ) # The log determinant of the inverse transform has a negative sign! return x , log_pz - log_det def log_prob ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The point we want to compute logp(x) at. - `y`: The conditioning information **Returns**: The log likelihood of x under the model. \"\"\" z , log_det = self . transform ( x , y = y , ** kwargs ) log_pz = self . prior . log_prob ( z ) return log_pz + log_det def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on a batch of data. This is one of the few times that $x$ is expected to be batched. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new flow with the parameters initialized. \"\"\" new_layer = self . transform . data_dependent_init ( x , y = y , key = key ) # Turn the new parameters into a new module get_transform = lambda tree : tree . transform return eqx . tree_at ( get_transform , self , new_layer ) __init__ ( self , transform : BijectiveTransform , prior : ProbabilityDistribution , ** kwargs ) \u00a4 Arguments : transform : A bijective transformation prior : The prior distribution Source code in generax/distributions/flow_models.py def __init__ ( self , transform : BijectiveTransform , prior : ProbabilityDistribution , ** kwargs ): \"\"\"**Arguments**: - `transform`: A bijective transformation - `prior`: The prior distribution \"\"\" self . transform = transform self . prior = prior input_shape = self . transform . input_shape super () . __init__ ( input_shape = input_shape , ** kwargs ) sample ( self , key : PRNGKeyArray , y : Optional [ Array ] = None ) -> Array \u00a4 Inherited from generax.distributions.base.ProbabilityDistribution.sample . Source code in generax/distributions/flow_models.py def sample ( self , key : PRNGKeyArray , y : Optional [ Array ] = None ) -> Array : \"\"\" **Arguments**: - `key`: The random number generator key. **Returns**: Samples from the model Use eqx.filter_vmap to get more samples! For example, ```python keys = random.split(key, n_samples) samples = eqx.filter_vmap(self.sample)(keys) ``` \"\"\" return self . sample_and_log_prob ( key , y )[ 0 ] sample_and_log_prob ( self , key : PRNGKeyArray , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Arguments : key : The random number generator key. y : The conditioning information Returns : A single sample from the model. Use vmap to get more samples. Source code in generax/distributions/flow_models.py def sample_and_log_prob ( self , key : PRNGKeyArray , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"**Arguments**: - `key`: The random number generator key. - `y`: The conditioning information **Returns**: A single sample from the model. Use vmap to get more samples. \"\"\" z , log_pz = self . prior . sample_and_log_prob ( key ) x , log_det = self . transform ( z , y = y , inverse = True , ** kwargs ) # The log determinant of the inverse transform has a negative sign! return x , log_pz - log_det log_prob ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Arguments : x : The point we want to compute logp(x) at. y : The conditioning information Returns : The log likelihood of x under the model. Source code in generax/distributions/flow_models.py def log_prob ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The point we want to compute logp(x) at. - `y`: The conditioning information **Returns**: The log likelihood of x under the model. \"\"\" z , log_det = self . transform ( x , y = y , ** kwargs ) log_pz = self . prior . log_prob ( z ) return log_pz + log_det score ( self , x : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array \u00a4 Inherited from generax.distributions.base.ProbabilityDistribution.score . Source code in generax/distributions/flow_models.py def score ( self , x : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array : \"\"\"**Arguments**: - `x`: The point we want to compute grad logp(x) at. - `y`: The (optional) conditioning information. - `key`: The random number generator key. Can be passed in the event that we're getting a stochastic estimate of the log prob. **Returns**: The log likelihood of x under the model. \"\"\" return eqx . filter_grad ( self . log_prob )( x , y = y , key = key ) data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform \u00a4 Initialize the parameters of the layer based on a batch of data. This is one of the few times that \\(x\\) is expected to be batched. Arguments : x : The data to initialize the parameters with. y : The conditioning information key : A jax.random.PRNGKey for initialization Returns : A new flow with the parameters initialized. Source code in generax/distributions/flow_models.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on a batch of data. This is one of the few times that $x$ is expected to be batched. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new flow with the parameters initialized. \"\"\" new_layer = self . transform . data_dependent_init ( x , y = y , key = key ) # Turn the new parameters into a new module get_transform = lambda tree : tree . transform return eqx . tree_at ( get_transform , self , new_layer ) generax.distributions.flow_models.RectangularFlow ( NormalizingFlow ) \u00a4 A flow with an injective transformation, i.e. the base space has a lower dimensionality than the data space. Source code in generax/distributions/flow_models.py class RectangularFlow ( NormalizingFlow ): \"\"\"A flow with an injective transformation, i.e. the base space has a lower dimensionality than the data space. \"\"\" transform : InjectiveTransform output_shape : Tuple [ int ] def __init__ ( self , transform : InjectiveTransform , prior : ProbabilityDistribution , ** kwargs ): \"\"\"**Arguments**: - `transform`: A bijective transformation - `prior`: The prior distribution \"\"\" assert isinstance ( transform , InjectiveTransform ) self . output_shape = transform . output_shape super () . __init__ ( transform = transform , prior = prior , ** kwargs ) def project ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Project a point onto the image of the transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: z \"\"\" return self . transform . project ( x , y = y , ** kwargs ) def sample_and_log_prob ( self , key : PRNGKeyArray , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"**Arguments**: - `key`: The random number generator key. - `y`: The conditioning information **Returns**: A single sample from the model. Use vmap to get more samples. \"\"\" z , log_pz = self . prior . sample_and_log_prob ( key ) x , _ = self . transform ( z , y = y , inverse = True , ** kwargs ) log_det = self . transform . log_determiant ( z , y = y , ** kwargs ) return x , log_pz + log_det def log_prob ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The point we want to compute logp(x) at. - `y`: The conditioning information **Returns**: The log likelihood of x under the model. \"\"\" z , _ = self . transform ( x , y = y , ** kwargs ) log_pz = self . prior . log_prob ( z ) log_det = self . transform . log_determiant ( z , y = y , ** kwargs ) return log_pz + log_det sample ( self , key : PRNGKeyArray , y : Optional [ Array ] = None ) -> Array \u00a4 Arguments : key : The random number generator key. Returns : Samples from the model Use eqx.filter_vmap to get more samples! For example, keys = random . split ( key , n_samples ) samples = eqx . filter_vmap ( self . sample )( keys ) Source code in generax/distributions/flow_models.py def sample ( self , key : PRNGKeyArray , y : Optional [ Array ] = None ) -> Array : \"\"\" **Arguments**: - `key`: The random number generator key. **Returns**: Samples from the model Use eqx.filter_vmap to get more samples! For example, ```python keys = random.split(key, n_samples) samples = eqx.filter_vmap(self.sample)(keys) ``` \"\"\" return self . sample_and_log_prob ( key , y )[ 0 ] score ( self , x : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array \u00a4 Arguments : x : The point we want to compute grad logp(x) at. y : The (optional) conditioning information. key : The random number generator key. Can be passed in the event that we're getting a stochastic estimate of the log prob. Returns : The log likelihood of x under the model. Source code in generax/distributions/flow_models.py def score ( self , x : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array : \"\"\"**Arguments**: - `x`: The point we want to compute grad logp(x) at. - `y`: The (optional) conditioning information. - `key`: The random number generator key. Can be passed in the event that we're getting a stochastic estimate of the log prob. **Returns**: The log likelihood of x under the model. \"\"\" return eqx . filter_grad ( self . log_prob )( x , y = y , key = key ) data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform \u00a4 Inherited from generax.distributions.flow_models.NormalizingFlow.data_dependent_init . Source code in generax/distributions/flow_models.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on a batch of data. This is one of the few times that $x$ is expected to be batched. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new flow with the parameters initialized. \"\"\" new_layer = self . transform . data_dependent_init ( x , y = y , key = key ) # Turn the new parameters into a new module get_transform = lambda tree : tree . transform return eqx . tree_at ( get_transform , self , new_layer ) __init__ ( self , transform : InjectiveTransform , prior : ProbabilityDistribution , ** kwargs ) \u00a4 Arguments : transform : A bijective transformation prior : The prior distribution Source code in generax/distributions/flow_models.py def __init__ ( self , transform : InjectiveTransform , prior : ProbabilityDistribution , ** kwargs ): \"\"\"**Arguments**: - `transform`: A bijective transformation - `prior`: The prior distribution \"\"\" assert isinstance ( transform , InjectiveTransform ) self . output_shape = transform . output_shape super () . __init__ ( transform = transform , prior = prior , ** kwargs ) project ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Project a point onto the image of the transformation. Arguments : x : The input to the transformation y : The conditioning information Returns : z Source code in generax/distributions/flow_models.py def project ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Project a point onto the image of the transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: z \"\"\" return self . transform . project ( x , y = y , ** kwargs ) sample_and_log_prob ( self , key : PRNGKeyArray , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Arguments : key : The random number generator key. y : The conditioning information Returns : A single sample from the model. Use vmap to get more samples. Source code in generax/distributions/flow_models.py def sample_and_log_prob ( self , key : PRNGKeyArray , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"**Arguments**: - `key`: The random number generator key. - `y`: The conditioning information **Returns**: A single sample from the model. Use vmap to get more samples. \"\"\" z , log_pz = self . prior . sample_and_log_prob ( key ) x , _ = self . transform ( z , y = y , inverse = True , ** kwargs ) log_det = self . transform . log_determiant ( z , y = y , ** kwargs ) return x , log_pz + log_det log_prob ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Arguments : x : The point we want to compute logp(x) at. y : The conditioning information Returns : The log likelihood of x under the model. Source code in generax/distributions/flow_models.py def log_prob ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The point we want to compute logp(x) at. - `y`: The conditioning information **Returns**: The log likelihood of x under the model. \"\"\" z , _ = self . transform ( x , y = y , ** kwargs ) log_pz = self . prior . log_prob ( z ) log_det = self . transform . log_determiant ( z , y = y , ** kwargs ) return log_pz + log_det generax.distributions.flow_models.RealNVP ( NormalizingFlow ) \u00a4 RealNVP( args, *kwargs) Source code in generax/distributions/flow_models.py class RealNVP ( NormalizingFlow ): def __init__ ( self , input_shape : Tuple [ int ], n_flow_layers : int = 3 , working_size : int = 16 , hidden_size : int = 32 , n_blocks : int = 4 , cond_shape : Optional [ Tuple [ int ]] = None , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The shape of the input data. - `n_flow_layers`: The number of layers in the flow. - `working_size`: The size of the working space. - `hidden_size`: The size of the hidden layers. - `n_blocks`: The number of blocks in the coupling layers. - `cond_shape`: The shape of the conditioning information. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" transform = RealNVPTransform ( input_shape = input_shape , n_flow_layers = n_flow_layers , working_size = working_size , hidden_size = hidden_size , n_blocks = n_blocks , cond_shape = cond_shape , key = key ) prior = Gaussian ( input_shape = input_shape ) super () . __init__ ( transform = transform , prior = prior , ** kwargs ) __init__ ( self , input_shape : Tuple [ int ], n_flow_layers : int = 3 , working_size : int = 16 , hidden_size : int = 32 , n_blocks : int = 4 , cond_shape : Optional [ Tuple [ int ]] = None , * , key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : input_shape : The shape of the input data. n_flow_layers : The number of layers in the flow. working_size : The size of the working space. hidden_size : The size of the hidden layers. n_blocks : The number of blocks in the coupling layers. cond_shape : The shape of the conditioning information. key : A jax.random.PRNGKey for initialization Source code in generax/distributions/flow_models.py def __init__ ( self , input_shape : Tuple [ int ], n_flow_layers : int = 3 , working_size : int = 16 , hidden_size : int = 32 , n_blocks : int = 4 , cond_shape : Optional [ Tuple [ int ]] = None , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The shape of the input data. - `n_flow_layers`: The number of layers in the flow. - `working_size`: The size of the working space. - `hidden_size`: The size of the hidden layers. - `n_blocks`: The number of blocks in the coupling layers. - `cond_shape`: The shape of the conditioning information. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" transform = RealNVPTransform ( input_shape = input_shape , n_flow_layers = n_flow_layers , working_size = working_size , hidden_size = hidden_size , n_blocks = n_blocks , cond_shape = cond_shape , key = key ) prior = Gaussian ( input_shape = input_shape ) super () . __init__ ( transform = transform , prior = prior , ** kwargs ) generax.distributions.flow_models.NeuralSpline ( NormalizingFlow ) \u00a4 NeuralSpline( args, *kwargs) Source code in generax/distributions/flow_models.py class NeuralSpline ( NormalizingFlow ): def __init__ ( self , input_shape : Tuple [ int ], n_flow_layers : int = 3 , working_size : int = 16 , hidden_size : int = 32 , n_blocks : int = 4 , n_spline_knots : int = 8 , cond_shape : Optional [ Tuple [ int ]] = None , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The shape of the input data. - `n_flow_layers`: The number of layers in the flow. - `working_size`: The size of the working space. - `hidden_size`: The size of the hidden layers. - `n_blocks`: The number of blocks in the coupling layers. - `cond_shape`: The shape of the conditioning information. - `n_splice_knots`: The number of knots in the spline. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" transform = NeuralSplineTransform ( input_shape = input_shape , n_flow_layers = n_flow_layers , working_size = working_size , hidden_size = hidden_size , n_blocks = n_blocks , n_spline_knots = n_spline_knots , cond_shape = cond_shape , key = key ) prior = Gaussian ( input_shape = input_shape ) super () . __init__ ( transform = transform , prior = prior , ** kwargs ) __init__ ( self , input_shape : Tuple [ int ], n_flow_layers : int = 3 , working_size : int = 16 , hidden_size : int = 32 , n_blocks : int = 4 , n_spline_knots : int = 8 , cond_shape : Optional [ Tuple [ int ]] = None , * , key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : input_shape : The shape of the input data. n_flow_layers : The number of layers in the flow. working_size : The size of the working space. hidden_size : The size of the hidden layers. n_blocks : The number of blocks in the coupling layers. cond_shape : The shape of the conditioning information. n_splice_knots : The number of knots in the spline. key : A jax.random.PRNGKey for initialization Source code in generax/distributions/flow_models.py def __init__ ( self , input_shape : Tuple [ int ], n_flow_layers : int = 3 , working_size : int = 16 , hidden_size : int = 32 , n_blocks : int = 4 , n_spline_knots : int = 8 , cond_shape : Optional [ Tuple [ int ]] = None , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The shape of the input data. - `n_flow_layers`: The number of layers in the flow. - `working_size`: The size of the working space. - `hidden_size`: The size of the hidden layers. - `n_blocks`: The number of blocks in the coupling layers. - `cond_shape`: The shape of the conditioning information. - `n_splice_knots`: The number of knots in the spline. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" transform = NeuralSplineTransform ( input_shape = input_shape , n_flow_layers = n_flow_layers , working_size = working_size , hidden_size = hidden_size , n_blocks = n_blocks , n_spline_knots = n_spline_knots , cond_shape = cond_shape , key = key ) prior = Gaussian ( input_shape = input_shape ) super () . __init__ ( transform = transform , prior = prior , ** kwargs ) generax.distributions.flow_models.ContinuousNormalizingFlow ( NormalizingFlow ) \u00a4 This is FFJORD . Source code in generax/distributions/flow_models.py class ContinuousNormalizingFlow ( NormalizingFlow ): \"\"\"This is [FFJORD](https://arxiv.org/pdf/1810.01367.pdf). \"\"\" def __init__ ( self , input_shape : Tuple [ int ], net : eqx . Module = None , cond_shape : Optional [ Tuple [ int ]] = None , * , controller_rtol : Optional [ float ] = 1e-3 , controller_atol : Optional [ float ] = 1e-5 , adjoint = 'recursive_checkpoint' , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The shape of the input data. - `net`: The neural network to use for the vector field. If None, a default network will be used. `net` should accept `net(t, x, y=y)` - `cond_shape`: The shape of the conditioning information. - `key`: A `jax.random.PRNGKey` for initialization - `controller_rtol`: The relative tolerance for the controller. - `controller_atol`: The absolute tolerance for the controller. - `adjoint`: The adjoint method to use. See [this](https://docs.kidger.site/diffrax/api/adjoints/) \"\"\" transform = FFJORDTransform ( input_shape = input_shape , net = net , cond_shape = cond_shape , key = key , controller_rtol = controller_rtol , controller_atol = controller_atol , adjoint = adjoint , ** kwargs ) prior = Gaussian ( input_shape = input_shape ) super () . __init__ ( transform = transform , prior = prior , ** kwargs ) @property def neural_ode ( self ): return self . transform . neural_ode @property def vector_field ( self ): \"\"\"Get the vector field function that samples evolve on as t changes. This is an `eqx.Module` that with the signature `vector_field(t, x, y=y) -> dx/dt`.\"\"\" return self . transform . vector_field @property def net ( self ): \"\"\"Same as `vector_field`\"\"\" return self . vector_field def sample ( self , key : PRNGKeyArray , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"**Arguments**: - `key`: The random number generator key. **Returns**: Samples from the model \"\"\" z = self . prior . sample ( key ) x , _ = self . transform ( z , y = y , inverse = True , log_likelihood = False , ** kwargs ) return x vector_field property readonly \u00a4 Get the vector field function that samples evolve on as t changes. This is an eqx.Module that with the signature vector_field(t, x, y=y) -> dx/dt . net property readonly \u00a4 Same as vector_field __init__ ( self , input_shape : Tuple [ int ], net : Module = None , cond_shape : Optional [ Tuple [ int ]] = None , * , controller_rtol : Optional [ float ] = 0.001 , controller_atol : Optional [ float ] = 1e-05 , adjoint = 'recursive_checkpoint' , key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : input_shape : The shape of the input data. net : The neural network to use for the vector field. If None, a default network will be used. net should accept net(t, x, y=y) cond_shape : The shape of the conditioning information. key : A jax.random.PRNGKey for initialization controller_rtol : The relative tolerance for the controller. controller_atol : The absolute tolerance for the controller. adjoint : The adjoint method to use. See this Source code in generax/distributions/flow_models.py def __init__ ( self , input_shape : Tuple [ int ], net : eqx . Module = None , cond_shape : Optional [ Tuple [ int ]] = None , * , controller_rtol : Optional [ float ] = 1e-3 , controller_atol : Optional [ float ] = 1e-5 , adjoint = 'recursive_checkpoint' , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The shape of the input data. - `net`: The neural network to use for the vector field. If None, a default network will be used. `net` should accept `net(t, x, y=y)` - `cond_shape`: The shape of the conditioning information. - `key`: A `jax.random.PRNGKey` for initialization - `controller_rtol`: The relative tolerance for the controller. - `controller_atol`: The absolute tolerance for the controller. - `adjoint`: The adjoint method to use. See [this](https://docs.kidger.site/diffrax/api/adjoints/) \"\"\" transform = FFJORDTransform ( input_shape = input_shape , net = net , cond_shape = cond_shape , key = key , controller_rtol = controller_rtol , controller_atol = controller_atol , adjoint = adjoint , ** kwargs ) prior = Gaussian ( input_shape = input_shape ) super () . __init__ ( transform = transform , prior = prior , ** kwargs )","title":"Flows"},{"location":"api/distributions/flow_models/#flows","text":"","title":"Flows"},{"location":"api/distributions/flow_models/#generax.distributions.flow_models.NormalizingFlow","text":"A normalizing flow is a model that we use to represent probability distributions. See this for an overview. Atributes : transform : A BijectiveTransform object that transforms a variable from the base space to the data space and also computes the change is log pdf. transform(x) -> (z,log_det) : Apply the transformation to the input. prior : The prior probability distribution. Methods : to_base_space(x) -> z : Transform a point from the data space to the base space. sample_and_log_prob(key) -> (x,log_px) : Sample from the distribution and compute the log probability. sample(key) -> x : Pull a single sample from the model log_prob(x) -> log_px : Compute the log probability of a point under the model Source code in generax/distributions/flow_models.py class NormalizingFlow ( ProbabilityDistribution , ABC ): \"\"\"A normalizing flow is a model that we use to represent probability distributions. See [this](https://arxiv.org/pdf/1912.02762.pdf) for an overview. **Atributes**: - `transform`: A `BijectiveTransform` object that transforms a variable from the base space to the data space and also computes the change is log pdf. - `transform(x) -> (z,log_det)`: Apply the transformation to the input. - `prior`: The prior probability distribution. **Methods**: - `to_base_space(x) -> z`: Transform a point from the data space to the base space. - `sample_and_log_prob(key) -> (x,log_px)`: Sample from the distribution and compute the log probability. - `sample(key) -> x`: Pull a single sample from the model - `log_prob(x) -> log_px`: Compute the log probability of a point under the model \"\"\" transform : BijectiveTransform prior : ProbabilityDistribution def __init__ ( self , transform : BijectiveTransform , prior : ProbabilityDistribution , ** kwargs ): \"\"\"**Arguments**: - `transform`: A bijective transformation - `prior`: The prior distribution \"\"\" self . transform = transform self . prior = prior input_shape = self . transform . input_shape super () . __init__ ( input_shape = input_shape , ** kwargs ) def to_base_space ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: A JAX array with shape `(dim,)`. - `y`: The conditioning information **Returns**: A JAX array with shape `(dim,)`. \"\"\" return self . transform ( x , y = y , ** kwargs )[ 0 ] def to_data_space ( self , z : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"**Arguments**: - `z`: A JAX array with shape `(dim,)`. - `y`: The conditioning information **Returns**: A JAX array with shape `(dim,)`. \"\"\" return self . transform ( z , y = y , inverse = True , ** kwargs )[ 0 ] def sample_and_log_prob ( self , key : PRNGKeyArray , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"**Arguments**: - `key`: The random number generator key. - `y`: The conditioning information **Returns**: A single sample from the model. Use vmap to get more samples. \"\"\" z , log_pz = self . prior . sample_and_log_prob ( key ) x , log_det = self . transform ( z , y = y , inverse = True , ** kwargs ) # The log determinant of the inverse transform has a negative sign! return x , log_pz - log_det def log_prob ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The point we want to compute logp(x) at. - `y`: The conditioning information **Returns**: The log likelihood of x under the model. \"\"\" z , log_det = self . transform ( x , y = y , ** kwargs ) log_pz = self . prior . log_prob ( z ) return log_pz + log_det def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on a batch of data. This is one of the few times that $x$ is expected to be batched. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new flow with the parameters initialized. \"\"\" new_layer = self . transform . data_dependent_init ( x , y = y , key = key ) # Turn the new parameters into a new module get_transform = lambda tree : tree . transform return eqx . tree_at ( get_transform , self , new_layer )","title":"NormalizingFlow"},{"location":"api/distributions/flow_models/#generax.distributions.flow_models.RectangularFlow","text":"A flow with an injective transformation, i.e. the base space has a lower dimensionality than the data space. Source code in generax/distributions/flow_models.py class RectangularFlow ( NormalizingFlow ): \"\"\"A flow with an injective transformation, i.e. the base space has a lower dimensionality than the data space. \"\"\" transform : InjectiveTransform output_shape : Tuple [ int ] def __init__ ( self , transform : InjectiveTransform , prior : ProbabilityDistribution , ** kwargs ): \"\"\"**Arguments**: - `transform`: A bijective transformation - `prior`: The prior distribution \"\"\" assert isinstance ( transform , InjectiveTransform ) self . output_shape = transform . output_shape super () . __init__ ( transform = transform , prior = prior , ** kwargs ) def project ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Project a point onto the image of the transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: z \"\"\" return self . transform . project ( x , y = y , ** kwargs ) def sample_and_log_prob ( self , key : PRNGKeyArray , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"**Arguments**: - `key`: The random number generator key. - `y`: The conditioning information **Returns**: A single sample from the model. Use vmap to get more samples. \"\"\" z , log_pz = self . prior . sample_and_log_prob ( key ) x , _ = self . transform ( z , y = y , inverse = True , ** kwargs ) log_det = self . transform . log_determiant ( z , y = y , ** kwargs ) return x , log_pz + log_det def log_prob ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The point we want to compute logp(x) at. - `y`: The conditioning information **Returns**: The log likelihood of x under the model. \"\"\" z , _ = self . transform ( x , y = y , ** kwargs ) log_pz = self . prior . log_prob ( z ) log_det = self . transform . log_determiant ( z , y = y , ** kwargs ) return log_pz + log_det","title":"RectangularFlow"},{"location":"api/distributions/flow_models/#generax.distributions.flow_models.RealNVP","text":"RealNVP( args, *kwargs) Source code in generax/distributions/flow_models.py class RealNVP ( NormalizingFlow ): def __init__ ( self , input_shape : Tuple [ int ], n_flow_layers : int = 3 , working_size : int = 16 , hidden_size : int = 32 , n_blocks : int = 4 , cond_shape : Optional [ Tuple [ int ]] = None , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The shape of the input data. - `n_flow_layers`: The number of layers in the flow. - `working_size`: The size of the working space. - `hidden_size`: The size of the hidden layers. - `n_blocks`: The number of blocks in the coupling layers. - `cond_shape`: The shape of the conditioning information. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" transform = RealNVPTransform ( input_shape = input_shape , n_flow_layers = n_flow_layers , working_size = working_size , hidden_size = hidden_size , n_blocks = n_blocks , cond_shape = cond_shape , key = key ) prior = Gaussian ( input_shape = input_shape ) super () . __init__ ( transform = transform , prior = prior , ** kwargs )","title":"RealNVP"},{"location":"api/distributions/flow_models/#generax.distributions.flow_models.NeuralSpline","text":"NeuralSpline( args, *kwargs) Source code in generax/distributions/flow_models.py class NeuralSpline ( NormalizingFlow ): def __init__ ( self , input_shape : Tuple [ int ], n_flow_layers : int = 3 , working_size : int = 16 , hidden_size : int = 32 , n_blocks : int = 4 , n_spline_knots : int = 8 , cond_shape : Optional [ Tuple [ int ]] = None , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The shape of the input data. - `n_flow_layers`: The number of layers in the flow. - `working_size`: The size of the working space. - `hidden_size`: The size of the hidden layers. - `n_blocks`: The number of blocks in the coupling layers. - `cond_shape`: The shape of the conditioning information. - `n_splice_knots`: The number of knots in the spline. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" transform = NeuralSplineTransform ( input_shape = input_shape , n_flow_layers = n_flow_layers , working_size = working_size , hidden_size = hidden_size , n_blocks = n_blocks , n_spline_knots = n_spline_knots , cond_shape = cond_shape , key = key ) prior = Gaussian ( input_shape = input_shape ) super () . __init__ ( transform = transform , prior = prior , ** kwargs )","title":"NeuralSpline"},{"location":"api/distributions/flow_models/#generax.distributions.flow_models.ContinuousNormalizingFlow","text":"This is FFJORD . Source code in generax/distributions/flow_models.py class ContinuousNormalizingFlow ( NormalizingFlow ): \"\"\"This is [FFJORD](https://arxiv.org/pdf/1810.01367.pdf). \"\"\" def __init__ ( self , input_shape : Tuple [ int ], net : eqx . Module = None , cond_shape : Optional [ Tuple [ int ]] = None , * , controller_rtol : Optional [ float ] = 1e-3 , controller_atol : Optional [ float ] = 1e-5 , adjoint = 'recursive_checkpoint' , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The shape of the input data. - `net`: The neural network to use for the vector field. If None, a default network will be used. `net` should accept `net(t, x, y=y)` - `cond_shape`: The shape of the conditioning information. - `key`: A `jax.random.PRNGKey` for initialization - `controller_rtol`: The relative tolerance for the controller. - `controller_atol`: The absolute tolerance for the controller. - `adjoint`: The adjoint method to use. See [this](https://docs.kidger.site/diffrax/api/adjoints/) \"\"\" transform = FFJORDTransform ( input_shape = input_shape , net = net , cond_shape = cond_shape , key = key , controller_rtol = controller_rtol , controller_atol = controller_atol , adjoint = adjoint , ** kwargs ) prior = Gaussian ( input_shape = input_shape ) super () . __init__ ( transform = transform , prior = prior , ** kwargs ) @property def neural_ode ( self ): return self . transform . neural_ode @property def vector_field ( self ): \"\"\"Get the vector field function that samples evolve on as t changes. This is an `eqx.Module` that with the signature `vector_field(t, x, y=y) -> dx/dt`.\"\"\" return self . transform . vector_field @property def net ( self ): \"\"\"Same as `vector_field`\"\"\" return self . vector_field def sample ( self , key : PRNGKeyArray , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"**Arguments**: - `key`: The random number generator key. **Returns**: Samples from the model \"\"\" z = self . prior . sample ( key ) x , _ = self . transform ( z , y = y , inverse = True , log_likelihood = False , ** kwargs ) return x","title":"ContinuousNormalizingFlow"},{"location":"api/flows/affine/","text":"Affine \u00a4 generax.flows.affine.Shift ( BijectiveTransform ) \u00a4 This represents a shift transformation This is NICE https://arxiv.org/pdf/1410.8516.pdf when used in a coupling layer. Attributes : - b : The shift parameter. Source code in generax/flows/affine.py class Shift ( BijectiveTransform ): \"\"\"This represents a shift transformation This is NICE https://arxiv.org/pdf/1410.8516.pdf when used in a coupling layer. **Attributes**: - `b`: The shift parameter. \"\"\" b : Array def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) # Initialize the parameters randomly self . b = random . normal ( key , shape = input_shape ) def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'x must be batched' mean , std = misc . mean_and_std ( x , axis = 0 ) # Initialize the parameters so that z will have # zero mean and unit variance b = mean # Turn the new parameters into a new module get_b = lambda tree : tree . b updated_layer = eqx . tree_at ( get_b , self , b ) return updated_layer def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : z = x - self . b else : z = x + self . b log_det = jnp . array ( 0.0 ) return z , log_det __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. key : A jax.random.PRNGKey for initialization Source code in generax/flows/affine.py def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) # Initialize the parameters randomly self . b = random . normal ( key , shape = input_shape ) data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform \u00a4 Initialize the parameters of the layer based on the data. Arguments : x : The data to initialize the parameters with. y : The conditioning information key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/flows/affine.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'x must be batched' mean , std = misc . mean_and_std ( x , axis = 0 ) # Initialize the parameters so that z will have # zero mean and unit variance b = mean # Turn the new parameters into a new module get_b = lambda tree : tree . b updated_layer = eqx . tree_at ( get_b , self , b ) return updated_layer __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation Returns : (z, log_det) Source code in generax/flows/affine.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : z = x - self . b else : z = x + self . b log_det = jnp . array ( 0.0 ) return z , log_det inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Inherited from generax.flows.base.BijectiveTransform.inverse . Source code in generax/flows/affine.py def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs ) generax.flows.affine.ShiftScale ( BijectiveTransform ) \u00a4 This represents a shift and scale transformation. This is RealNVP https://arxiv.org/pdf/1605.08803.pdf when used in a coupling layer. Attributes : - s_unbounded : The unbounded scaling parameter. - b : The shift parameter. Source code in generax/flows/affine.py class ShiftScale ( BijectiveTransform ): \"\"\"This represents a shift and scale transformation. This is RealNVP https://arxiv.org/pdf/1605.08803.pdf when used in a coupling layer. **Attributes**: - `s_unbounded`: The unbounded scaling parameter. - `b`: The shift parameter. \"\"\" s_unbounded : Array b : Array def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) # Initialize the parameters randomly self . s_unbounded , self . b = random . normal ( key , shape = ( 2 ,) + input_shape ) def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'x must be batched' mean , std = misc . mean_and_std ( x , axis = 0 ) std += 1e-4 # Initialize the parameters so that z will have # zero mean and unit variance b = mean s_unbounded = std - 1 / std # Turn the new parameters into a new module get_b = lambda tree : tree . b get_s_unbounded = lambda tree : tree . s_unbounded updated_layer = eqx . tree_at ( get_b , self , b ) updated_layer = eqx . tree_at ( get_s_unbounded , updated_layer , s_unbounded ) return updated_layer def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' # s must be strictly positive s = misc . square_plus ( self . s_unbounded , gamma = 1.0 ) # + 1e-4 log_s = jnp . log ( s ) if inverse == False : z = ( x - self . b ) / s else : z = x * s + self . b if inverse == False : log_det = - log_s . sum () else : log_det = log_s . sum () return z , log_det __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. key : A jax.random.PRNGKey for initialization Source code in generax/flows/affine.py def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) # Initialize the parameters randomly self . s_unbounded , self . b = random . normal ( key , shape = ( 2 ,) + input_shape ) inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Inherited from generax.flows.base.BijectiveTransform.inverse . Source code in generax/flows/affine.py def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs ) data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform \u00a4 Initialize the parameters of the layer based on the data. Arguments : x : The data to initialize the parameters with. y : The conditioning information key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/flows/affine.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'x must be batched' mean , std = misc . mean_and_std ( x , axis = 0 ) std += 1e-4 # Initialize the parameters so that z will have # zero mean and unit variance b = mean s_unbounded = std - 1 / std # Turn the new parameters into a new module get_b = lambda tree : tree . b get_s_unbounded = lambda tree : tree . s_unbounded updated_layer = eqx . tree_at ( get_b , self , b ) updated_layer = eqx . tree_at ( get_s_unbounded , updated_layer , s_unbounded ) return updated_layer __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation Returns : (z, log_det) Source code in generax/flows/affine.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' # s must be strictly positive s = misc . square_plus ( self . s_unbounded , gamma = 1.0 ) # + 1e-4 log_s = jnp . log ( s ) if inverse == False : z = ( x - self . b ) / s else : z = x * s + self . b if inverse == False : log_det = - log_s . sum () else : log_det = log_s . sum () return z , log_det generax.flows.affine.DenseLinear ( BijectiveTransform ) \u00a4 Multiply the last axis by a dense matrix. When applied to images, this is GLOW https://arxiv.org/pdf/1807.03039.pdf Attributes : - W : The weight matrix Source code in generax/flows/affine.py class DenseLinear ( BijectiveTransform ): \"\"\"Multiply the last axis by a dense matrix. When applied to images, this is GLOW https://arxiv.org/pdf/1807.03039.pdf **Attributes**: - `W`: The weight matrix \"\"\" W : Array def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) dim = self . input_shape [ - 1 ] self . W = random . normal ( key , shape = ( dim , dim )) self . W = misc . whiten ( self . W ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : z = jnp . einsum ( 'ij,...j->...i' , self . W , x ) else : W_inv = jnp . linalg . inv ( self . W ) z = jnp . einsum ( 'ij,...j->...i' , W_inv , x ) # Need to multiply the log determinant by the number of times # that we're applying the transformation. if len ( self . input_shape ) > 1 : dim_mult = np . prod ( self . input_shape [: - 1 ]) else : dim_mult = 1 log_det = jnp . linalg . slogdet ( self . W )[ 1 ] * dim_mult if inverse : log_det *= - 1 return z , log_det data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) \u00a4 Inherited from generax.flows.base.BijectiveTransform.data_dependent_init . Source code in generax/flows/affine.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ): \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" return self inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Inherited from generax.flows.base.BijectiveTransform.inverse . Source code in generax/flows/affine.py def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs ) __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. key : A jax.random.PRNGKey for initialization Source code in generax/flows/affine.py def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) dim = self . input_shape [ - 1 ] self . W = random . normal ( key , shape = ( dim , dim )) self . W = misc . whiten ( self . W ) __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Implements generax.flows.base.BijectiveTransform.__call__ . Source code in generax/flows/affine.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : z = jnp . einsum ( 'ij,...j->...i' , self . W , x ) else : W_inv = jnp . linalg . inv ( self . W ) z = jnp . einsum ( 'ij,...j->...i' , W_inv , x ) # Need to multiply the log determinant by the number of times # that we're applying the transformation. if len ( self . input_shape ) > 1 : dim_mult = np . prod ( self . input_shape [: - 1 ]) else : dim_mult = 1 log_det = jnp . linalg . slogdet ( self . W )[ 1 ] * dim_mult if inverse : log_det *= - 1 return z , log_det generax.flows.affine.DenseAffine ( BijectiveTransform ) \u00a4 Multiply the last axis by a dense matrix. When applied to images, this is GLOW https://arxiv.org/pdf/1807.03039.pdf Attributes : - W : The weight matrix - b : The bias vector Source code in generax/flows/affine.py class DenseAffine ( BijectiveTransform ): \"\"\"Multiply the last axis by a dense matrix. When applied to images, this is GLOW https://arxiv.org/pdf/1807.03039.pdf **Attributes**: - `W`: The weight matrix - `b`: The bias vector \"\"\" W : DenseLinear b : Array def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) self . W = DenseLinear ( input_shape = input_shape , key = key , ** kwargs ) self . b = jnp . zeros ( input_shape ) def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'x must be batched' b = - jnp . mean ( x , axis = 0 ) return eqx . tree_at ( lambda tree : tree . b , self , b ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : x = x + self . b z , log_det = self . W ( x , y = y , inverse = False ) else : z , log_det = self . W ( x , y = y , inverse = True ) z = z - self . b return z , log_det inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Inherited from generax.flows.base.BijectiveTransform.inverse . Source code in generax/flows/affine.py def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs ) __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. key : A jax.random.PRNGKey for initialization Source code in generax/flows/affine.py def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) self . W = DenseLinear ( input_shape = input_shape , key = key , ** kwargs ) self . b = jnp . zeros ( input_shape ) data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform \u00a4 Initialize the parameters of the layer based on the data. Arguments : x : The data to initialize the parameters with. y : The conditioning information key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/flows/affine.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'x must be batched' b = - jnp . mean ( x , axis = 0 ) return eqx . tree_at ( lambda tree : tree . b , self , b ) __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Implements generax.flows.base.BijectiveTransform.__call__ . Source code in generax/flows/affine.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : x = x + self . b z , log_det = self . W ( x , y = y , inverse = False ) else : z , log_det = self . W ( x , y = y , inverse = True ) z = z - self . b return z , log_det generax.flows.affine.CaleyOrthogonalMVP ( BijectiveTransform ) \u00a4 Caley transform parametrization of an orthogonal matrix. This performs a matrix vector product with an orthogonal matrix. Attributes : - W : The weight matrix - b : The bias vector Source code in generax/flows/affine.py class CaleyOrthogonalMVP ( BijectiveTransform ): \"\"\"Caley transform parametrization of an orthogonal matrix. This performs a matrix vector product with an orthogonal matrix. **Attributes**: - `W`: The weight matrix - `b`: The bias vector \"\"\" W : Array b : Array def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) dim = self . input_shape [ - 1 ] self . W = random . normal ( key , shape = ( dim , dim )) self . b = jnp . zeros ( input_shape ) def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : assert x . shape [ 1 :] == self . input_shape , 'x must be batched' b = - jnp . mean ( x , axis = 0 ) return eqx . tree_at ( lambda tree : tree . b , self , b ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' A = self . W - self . W . T dim = self . input_shape [ - 1 ] # So that we can multiply with channel dim of images @partial ( jnp . vectorize , signature = '(i,j),(j)->(i)' ) def matmul ( A , x ): return A @x if inverse == False : x += self . b IpA_inv = jnp . linalg . inv ( jnp . eye ( dim ) + A ) y = matmul ( IpA_inv , x ) z = y - matmul ( A , y ) else : ImA_inv = jnp . linalg . inv ( jnp . eye ( dim ) - A ) y = matmul ( ImA_inv , x ) z = y + matmul ( A , y ) z -= self . b log_det = jnp . zeros ( 1 ) return z , log_det inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Inherited from generax.flows.base.BijectiveTransform.inverse . Source code in generax/flows/affine.py def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs ) __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. key : A jax.random.PRNGKey for initialization Source code in generax/flows/affine.py def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) dim = self . input_shape [ - 1 ] self . W = random . normal ( key , shape = ( dim , dim )) self . b = jnp . zeros ( input_shape ) data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform \u00a4 Initialize the parameters of the layer based on the data. Arguments : x : The data to initialize the parameters with. y : The conditioning information key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/flows/affine.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : assert x . shape [ 1 :] == self . input_shape , 'x must be batched' b = - jnp . mean ( x , axis = 0 ) return eqx . tree_at ( lambda tree : tree . b , self , b ) __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Implements generax.flows.base.BijectiveTransform.__call__ . Source code in generax/flows/affine.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' A = self . W - self . W . T dim = self . input_shape [ - 1 ] # So that we can multiply with channel dim of images @partial ( jnp . vectorize , signature = '(i,j),(j)->(i)' ) def matmul ( A , x ): return A @x if inverse == False : x += self . b IpA_inv = jnp . linalg . inv ( jnp . eye ( dim ) + A ) y = matmul ( IpA_inv , x ) z = y - matmul ( A , y ) else : ImA_inv = jnp . linalg . inv ( jnp . eye ( dim ) - A ) y = matmul ( ImA_inv , x ) z = y + matmul ( A , y ) z -= self . b log_det = jnp . zeros ( 1 ) return z , log_det generax.flows.affine.PLUAffine ( BijectiveTransform ) \u00a4 Multiply the last axis by a matrix that is parametrized using the LU decomposition. This is more efficient than the dense parametrization Attributes : - A : The weight matrix components. The top half is the upper triangular matrix, and the bottom half is the lower triangular matrix and the diagonal is ignored. - b : The bias vector Source code in generax/flows/affine.py class PLUAffine ( BijectiveTransform ): \"\"\"Multiply the last axis by a matrix that is parametrized using the LU decomposition. This is more efficient than the dense parametrization **Attributes**: - `A`: The weight matrix components. The top half is the upper triangular matrix, and the bottom half is the lower triangular matrix and the diagonal is ignored. - `b`: The bias vector \"\"\" A : Array b : Array def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) # Initialize so that this will be approximately the identity matrix dim = input_shape [ - 1 ] self . A = random . normal ( key , shape = ( dim , dim )) * 0.01 self . A = self . A . at [ jnp . arange ( dim ), jnp . arange ( dim )] . set ( 1.0 ) self . b = jnp . zeros ( input_shape ) def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'x must be batched' b = - jnp . mean ( x , axis = 0 ) return eqx . tree_at ( lambda tree : tree . b , self , b ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' dim = x . shape [ - 1 ] mask = jnp . ones (( dim , dim ), dtype = bool ) upper_mask = jnp . triu ( mask ) lower_mask = jnp . tril ( mask , k =- 1 ) if inverse == False : x += self . b z = jnp . einsum ( \"ij,...j->...i\" , self . A * upper_mask , x ) z = jnp . einsum ( \"ij,...j->...i\" , self . A * lower_mask , z ) + z else : # vmap in order to handle images L_solve_vmap = L_solve U_solve_vmap = U_solve_with_diag for _ in x . shape [: - 1 ]: L_solve_vmap = jax . vmap ( L_solve_vmap , in_axes = ( None , 0 )) U_solve_vmap = jax . vmap ( U_solve_vmap , in_axes = ( None , 0 )) z = L_solve_vmap ( self . A * lower_mask , x ) z = U_solve_vmap ( self . A * upper_mask , z ) z -= self . b log_det = jnp . log ( jnp . abs ( jnp . diag ( self . A ))) . sum () * misc . list_prod ( x . shape [: - 1 ]) if inverse : log_det *= - 1 return z , log_det inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Inherited from generax.flows.base.BijectiveTransform.inverse . Source code in generax/flows/affine.py def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs ) __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. key : A jax.random.PRNGKey for initialization Source code in generax/flows/affine.py def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) # Initialize so that this will be approximately the identity matrix dim = input_shape [ - 1 ] self . A = random . normal ( key , shape = ( dim , dim )) * 0.01 self . A = self . A . at [ jnp . arange ( dim ), jnp . arange ( dim )] . set ( 1.0 ) self . b = jnp . zeros ( input_shape ) data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform \u00a4 Initialize the parameters of the layer based on the data. Arguments : x : The data to initialize the parameters with. y : The conditioning information key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/flows/affine.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'x must be batched' b = - jnp . mean ( x , axis = 0 ) return eqx . tree_at ( lambda tree : tree . b , self , b ) __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Implements generax.flows.base.BijectiveTransform.__call__ . Source code in generax/flows/affine.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' dim = x . shape [ - 1 ] mask = jnp . ones (( dim , dim ), dtype = bool ) upper_mask = jnp . triu ( mask ) lower_mask = jnp . tril ( mask , k =- 1 ) if inverse == False : x += self . b z = jnp . einsum ( \"ij,...j->...i\" , self . A * upper_mask , x ) z = jnp . einsum ( \"ij,...j->...i\" , self . A * lower_mask , z ) + z else : # vmap in order to handle images L_solve_vmap = L_solve U_solve_vmap = U_solve_with_diag for _ in x . shape [: - 1 ]: L_solve_vmap = jax . vmap ( L_solve_vmap , in_axes = ( None , 0 )) U_solve_vmap = jax . vmap ( U_solve_vmap , in_axes = ( None , 0 )) z = L_solve_vmap ( self . A * lower_mask , x ) z = U_solve_vmap ( self . A * upper_mask , z ) z -= self . b log_det = jnp . log ( jnp . abs ( jnp . diag ( self . A ))) . sum () * misc . list_prod ( x . shape [: - 1 ]) if inverse : log_det *= - 1 return z , log_det generax.flows.affine.ConditionalOptionalTransport ( TimeDependentBijectiveTransform ) \u00a4 Given x1, compute f(t, x0) = t x1 + (1-t) x0. This is the optimal transport map between the two points. Used in flow matching https://arxiv.org/pdf/2210.02747.pdf Non-inverse mode goes t -> 0 while inverse mode goes t -> 1. Attributes : Source code in generax/flows/affine.py class ConditionalOptionalTransport ( TimeDependentBijectiveTransform ): \"\"\"Given x1, compute f(t, x0) = t*x1 + (1-t)*x0. This is the optimal transport map between the two points. Used in flow matching https://arxiv.org/pdf/2210.02747.pdf Non-inverse mode goes t -> 0 while inverse mode goes t -> 1. **Attributes**: \"\"\" def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) def __call__ ( self , t : Array , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `t`: The time point. - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to invert the transformation (0 -> t) **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if y is None : raise ValueError ( f 'Expected a conditional input' ) if y . shape != x . shape : raise ValueError ( f 'Expected y.shape ( { y . shape } ) to match x.shape ( { x . shape } )' ) x1 = y if inverse : x0 = x xt = ( 1 - t ) * x0 + t * x1 log_det = jnp . log ( 1 - t ) return xt , log_det else : xt = x x0 = ( xt - t * x1 ) / ( 1 - t ) log_det = - jnp . log ( 1 - t ) return x0 , log_det def vector_field ( self , t : Array , xt : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"The vector field that samples evolve on as t changes **Arguments**: - `t`: Time. - `x0`: A point in the base space. - `y`: The conditioning information. **Returns**: The vector field that samples evolve on at (t, x). \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if y is None : raise ValueError ( f 'Expected a conditional input' ) if y . shape != x . shape : raise ValueError ( f 'Expected y.shape ( { y . shape } ) to match x.shape ( { x . shape } )' ) return y - x data_dependent_init ( self , t : Array , xt : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) \u00a4 Inherited from generax.flows.base.TimeDependentBijectiveTransform.data_dependent_init . Source code in generax/flows/affine.py def data_dependent_init ( self , t : Array , xt : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ): \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `t`: Time. - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" return self inverse ( self , t : Array , x0 : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Inherited from generax.flows.base.TimeDependentBijectiveTransform.inverse . Source code in generax/flows/affine.py def inverse ( self , t : Array , x0 : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (xt, log_det) \"\"\" return self ( t , x0 , y = y , inverse = True , ** kwargs ) __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. key : A jax.random.PRNGKey for initialization Source code in generax/flows/affine.py def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) __call__ ( self , t : Array , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Arguments : t : The time point. x : The input to the transformation y : The conditioning information inverse : Whether to invert the transformation (0 -> t) Returns : (z, log_det) Source code in generax/flows/affine.py def __call__ ( self , t : Array , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `t`: The time point. - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to invert the transformation (0 -> t) **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if y is None : raise ValueError ( f 'Expected a conditional input' ) if y . shape != x . shape : raise ValueError ( f 'Expected y.shape ( { y . shape } ) to match x.shape ( { x . shape } )' ) x1 = y if inverse : x0 = x xt = ( 1 - t ) * x0 + t * x1 log_det = jnp . log ( 1 - t ) return xt , log_det else : xt = x x0 = ( xt - t * x1 ) / ( 1 - t ) log_det = - jnp . log ( 1 - t ) return x0 , log_det vector_field ( self , t : Array , xt : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 The vector field that samples evolve on as t changes Arguments : t : Time. x0 : A point in the base space. y : The conditioning information. Returns : The vector field that samples evolve on at (t, x). Source code in generax/flows/affine.py def vector_field ( self , t : Array , xt : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"The vector field that samples evolve on as t changes **Arguments**: - `t`: Time. - `x0`: A point in the base space. - `y`: The conditioning information. **Returns**: The vector field that samples evolve on at (t, x). \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if y is None : raise ValueError ( f 'Expected a conditional input' ) if y . shape != x . shape : raise ValueError ( f 'Expected y.shape ( { y . shape } ) to match x.shape ( { x . shape } )' ) return y - x generax.flows.affine.TallDenseLinear ( InjectiveTransform ) \u00a4 Matrix vector product with a tall matrix. Attributes : - W : The weight matrix Source code in generax/flows/affine.py class TallDenseLinear ( InjectiveTransform ): \"\"\"Matrix vector product with a tall matrix. **Attributes**: - `W`: The weight matrix \"\"\" W : Array def __init__ ( self , input_shape : Tuple [ int ], output_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" assert len ( input_shape ) == 1 , 'Only implemented for 1d data' super () . __init__ ( input_shape = input_shape , output_shape = output_shape , ** kwargs ) dim_in = self . input_shape [ - 1 ] dim_out = self . output_shape [ - 1 ] self . W = random . normal ( key , shape = ( dim_in , dim_out )) self . W = misc . whiten ( self . W ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" if inverse == False : assert x . shape == self . input_shape , 'Only works on unbatched data' else : assert x . shape == self . output_shape , 'Only works on unbatched data' if inverse == False : W_pinv = jnp . linalg . pinv ( self . W ) z = jnp . einsum ( 'ij,...j->...i' , W_pinv , x ) else : z = jnp . einsum ( 'ij,...j->...i' , self . W , x ) log_det = - 0.5 * jnp . linalg . slogdet ( self . W . T @self . W )[ 1 ] if inverse : log_det *= - 1 return z , log_det def log_determinant ( self , z : Array , ** kwargs ) -> Array : \"\"\"Compute -0.5*log(det(J^TJ)) **Arguments**: - `z`: An element of the base space **Returns**: The log determinant of (J^TJ)^0.5 \"\"\" log_det = - 0.5 * jnp . linalg . slogdet ( self . W . T @self . W )[ 1 ] return log_det data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) \u00a4 Initialize the parameters of the layer based on the data. Arguments : x : The data to initialize the parameters with. y : The conditioning information key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/flows/affine.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ): \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" return self inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Apply the inverse transformation. Arguments : x : The input to the transformation y : The conditioning information Returns : (z, log_det) Source code in generax/flows/affine.py def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs ) project ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Inherited from generax.flows.base.InjectiveTransform.project . Source code in generax/flows/affine.py def project ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Project a point onto the image of the transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: z \"\"\" z , _ = self ( x , y = y , ** kwargs ) x_proj , _ = self ( z , y = y , inverse = True , ** kwargs ) return x_proj __init__ ( self , input_shape : Tuple [ int ], output_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. key : A jax.random.PRNGKey for initialization Source code in generax/flows/affine.py def __init__ ( self , input_shape : Tuple [ int ], output_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" assert len ( input_shape ) == 1 , 'Only implemented for 1d data' super () . __init__ ( input_shape = input_shape , output_shape = output_shape , ** kwargs ) dim_in = self . input_shape [ - 1 ] dim_out = self . output_shape [ - 1 ] self . W = random . normal ( key , shape = ( dim_in , dim_out )) self . W = misc . whiten ( self . W ) __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation Returns : (z, log_det) Source code in generax/flows/affine.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" if inverse == False : assert x . shape == self . input_shape , 'Only works on unbatched data' else : assert x . shape == self . output_shape , 'Only works on unbatched data' if inverse == False : W_pinv = jnp . linalg . pinv ( self . W ) z = jnp . einsum ( 'ij,...j->...i' , W_pinv , x ) else : z = jnp . einsum ( 'ij,...j->...i' , self . W , x ) log_det = - 0.5 * jnp . linalg . slogdet ( self . W . T @self . W )[ 1 ] if inverse : log_det *= - 1 return z , log_det","title":"Affine"},{"location":"api/flows/affine/#affine","text":"","title":"Affine"},{"location":"api/flows/affine/#generax.flows.affine.Shift","text":"This represents a shift transformation This is NICE https://arxiv.org/pdf/1410.8516.pdf when used in a coupling layer. Attributes : - b : The shift parameter. Source code in generax/flows/affine.py class Shift ( BijectiveTransform ): \"\"\"This represents a shift transformation This is NICE https://arxiv.org/pdf/1410.8516.pdf when used in a coupling layer. **Attributes**: - `b`: The shift parameter. \"\"\" b : Array def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) # Initialize the parameters randomly self . b = random . normal ( key , shape = input_shape ) def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'x must be batched' mean , std = misc . mean_and_std ( x , axis = 0 ) # Initialize the parameters so that z will have # zero mean and unit variance b = mean # Turn the new parameters into a new module get_b = lambda tree : tree . b updated_layer = eqx . tree_at ( get_b , self , b ) return updated_layer def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : z = x - self . b else : z = x + self . b log_det = jnp . array ( 0.0 ) return z , log_det","title":"Shift"},{"location":"api/flows/affine/#generax.flows.affine.ShiftScale","text":"This represents a shift and scale transformation. This is RealNVP https://arxiv.org/pdf/1605.08803.pdf when used in a coupling layer. Attributes : - s_unbounded : The unbounded scaling parameter. - b : The shift parameter. Source code in generax/flows/affine.py class ShiftScale ( BijectiveTransform ): \"\"\"This represents a shift and scale transformation. This is RealNVP https://arxiv.org/pdf/1605.08803.pdf when used in a coupling layer. **Attributes**: - `s_unbounded`: The unbounded scaling parameter. - `b`: The shift parameter. \"\"\" s_unbounded : Array b : Array def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) # Initialize the parameters randomly self . s_unbounded , self . b = random . normal ( key , shape = ( 2 ,) + input_shape ) def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'x must be batched' mean , std = misc . mean_and_std ( x , axis = 0 ) std += 1e-4 # Initialize the parameters so that z will have # zero mean and unit variance b = mean s_unbounded = std - 1 / std # Turn the new parameters into a new module get_b = lambda tree : tree . b get_s_unbounded = lambda tree : tree . s_unbounded updated_layer = eqx . tree_at ( get_b , self , b ) updated_layer = eqx . tree_at ( get_s_unbounded , updated_layer , s_unbounded ) return updated_layer def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' # s must be strictly positive s = misc . square_plus ( self . s_unbounded , gamma = 1.0 ) # + 1e-4 log_s = jnp . log ( s ) if inverse == False : z = ( x - self . b ) / s else : z = x * s + self . b if inverse == False : log_det = - log_s . sum () else : log_det = log_s . sum () return z , log_det","title":"ShiftScale"},{"location":"api/flows/affine/#generax.flows.affine.DenseLinear","text":"Multiply the last axis by a dense matrix. When applied to images, this is GLOW https://arxiv.org/pdf/1807.03039.pdf Attributes : - W : The weight matrix Source code in generax/flows/affine.py class DenseLinear ( BijectiveTransform ): \"\"\"Multiply the last axis by a dense matrix. When applied to images, this is GLOW https://arxiv.org/pdf/1807.03039.pdf **Attributes**: - `W`: The weight matrix \"\"\" W : Array def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) dim = self . input_shape [ - 1 ] self . W = random . normal ( key , shape = ( dim , dim )) self . W = misc . whiten ( self . W ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : z = jnp . einsum ( 'ij,...j->...i' , self . W , x ) else : W_inv = jnp . linalg . inv ( self . W ) z = jnp . einsum ( 'ij,...j->...i' , W_inv , x ) # Need to multiply the log determinant by the number of times # that we're applying the transformation. if len ( self . input_shape ) > 1 : dim_mult = np . prod ( self . input_shape [: - 1 ]) else : dim_mult = 1 log_det = jnp . linalg . slogdet ( self . W )[ 1 ] * dim_mult if inverse : log_det *= - 1 return z , log_det","title":"DenseLinear"},{"location":"api/flows/affine/#generax.flows.affine.DenseAffine","text":"Multiply the last axis by a dense matrix. When applied to images, this is GLOW https://arxiv.org/pdf/1807.03039.pdf Attributes : - W : The weight matrix - b : The bias vector Source code in generax/flows/affine.py class DenseAffine ( BijectiveTransform ): \"\"\"Multiply the last axis by a dense matrix. When applied to images, this is GLOW https://arxiv.org/pdf/1807.03039.pdf **Attributes**: - `W`: The weight matrix - `b`: The bias vector \"\"\" W : DenseLinear b : Array def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) self . W = DenseLinear ( input_shape = input_shape , key = key , ** kwargs ) self . b = jnp . zeros ( input_shape ) def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'x must be batched' b = - jnp . mean ( x , axis = 0 ) return eqx . tree_at ( lambda tree : tree . b , self , b ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : x = x + self . b z , log_det = self . W ( x , y = y , inverse = False ) else : z , log_det = self . W ( x , y = y , inverse = True ) z = z - self . b return z , log_det","title":"DenseAffine"},{"location":"api/flows/affine/#generax.flows.affine.CaleyOrthogonalMVP","text":"Caley transform parametrization of an orthogonal matrix. This performs a matrix vector product with an orthogonal matrix. Attributes : - W : The weight matrix - b : The bias vector Source code in generax/flows/affine.py class CaleyOrthogonalMVP ( BijectiveTransform ): \"\"\"Caley transform parametrization of an orthogonal matrix. This performs a matrix vector product with an orthogonal matrix. **Attributes**: - `W`: The weight matrix - `b`: The bias vector \"\"\" W : Array b : Array def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) dim = self . input_shape [ - 1 ] self . W = random . normal ( key , shape = ( dim , dim )) self . b = jnp . zeros ( input_shape ) def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : assert x . shape [ 1 :] == self . input_shape , 'x must be batched' b = - jnp . mean ( x , axis = 0 ) return eqx . tree_at ( lambda tree : tree . b , self , b ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' A = self . W - self . W . T dim = self . input_shape [ - 1 ] # So that we can multiply with channel dim of images @partial ( jnp . vectorize , signature = '(i,j),(j)->(i)' ) def matmul ( A , x ): return A @x if inverse == False : x += self . b IpA_inv = jnp . linalg . inv ( jnp . eye ( dim ) + A ) y = matmul ( IpA_inv , x ) z = y - matmul ( A , y ) else : ImA_inv = jnp . linalg . inv ( jnp . eye ( dim ) - A ) y = matmul ( ImA_inv , x ) z = y + matmul ( A , y ) z -= self . b log_det = jnp . zeros ( 1 ) return z , log_det","title":"CaleyOrthogonalMVP"},{"location":"api/flows/affine/#generax.flows.affine.PLUAffine","text":"Multiply the last axis by a matrix that is parametrized using the LU decomposition. This is more efficient than the dense parametrization Attributes : - A : The weight matrix components. The top half is the upper triangular matrix, and the bottom half is the lower triangular matrix and the diagonal is ignored. - b : The bias vector Source code in generax/flows/affine.py class PLUAffine ( BijectiveTransform ): \"\"\"Multiply the last axis by a matrix that is parametrized using the LU decomposition. This is more efficient than the dense parametrization **Attributes**: - `A`: The weight matrix components. The top half is the upper triangular matrix, and the bottom half is the lower triangular matrix and the diagonal is ignored. - `b`: The bias vector \"\"\" A : Array b : Array def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) # Initialize so that this will be approximately the identity matrix dim = input_shape [ - 1 ] self . A = random . normal ( key , shape = ( dim , dim )) * 0.01 self . A = self . A . at [ jnp . arange ( dim ), jnp . arange ( dim )] . set ( 1.0 ) self . b = jnp . zeros ( input_shape ) def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'x must be batched' b = - jnp . mean ( x , axis = 0 ) return eqx . tree_at ( lambda tree : tree . b , self , b ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' dim = x . shape [ - 1 ] mask = jnp . ones (( dim , dim ), dtype = bool ) upper_mask = jnp . triu ( mask ) lower_mask = jnp . tril ( mask , k =- 1 ) if inverse == False : x += self . b z = jnp . einsum ( \"ij,...j->...i\" , self . A * upper_mask , x ) z = jnp . einsum ( \"ij,...j->...i\" , self . A * lower_mask , z ) + z else : # vmap in order to handle images L_solve_vmap = L_solve U_solve_vmap = U_solve_with_diag for _ in x . shape [: - 1 ]: L_solve_vmap = jax . vmap ( L_solve_vmap , in_axes = ( None , 0 )) U_solve_vmap = jax . vmap ( U_solve_vmap , in_axes = ( None , 0 )) z = L_solve_vmap ( self . A * lower_mask , x ) z = U_solve_vmap ( self . A * upper_mask , z ) z -= self . b log_det = jnp . log ( jnp . abs ( jnp . diag ( self . A ))) . sum () * misc . list_prod ( x . shape [: - 1 ]) if inverse : log_det *= - 1 return z , log_det","title":"PLUAffine"},{"location":"api/flows/affine/#generax.flows.affine.ConditionalOptionalTransport","text":"Given x1, compute f(t, x0) = t x1 + (1-t) x0. This is the optimal transport map between the two points. Used in flow matching https://arxiv.org/pdf/2210.02747.pdf Non-inverse mode goes t -> 0 while inverse mode goes t -> 1. Attributes : Source code in generax/flows/affine.py class ConditionalOptionalTransport ( TimeDependentBijectiveTransform ): \"\"\"Given x1, compute f(t, x0) = t*x1 + (1-t)*x0. This is the optimal transport map between the two points. Used in flow matching https://arxiv.org/pdf/2210.02747.pdf Non-inverse mode goes t -> 0 while inverse mode goes t -> 1. **Attributes**: \"\"\" def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) def __call__ ( self , t : Array , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `t`: The time point. - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to invert the transformation (0 -> t) **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if y is None : raise ValueError ( f 'Expected a conditional input' ) if y . shape != x . shape : raise ValueError ( f 'Expected y.shape ( { y . shape } ) to match x.shape ( { x . shape } )' ) x1 = y if inverse : x0 = x xt = ( 1 - t ) * x0 + t * x1 log_det = jnp . log ( 1 - t ) return xt , log_det else : xt = x x0 = ( xt - t * x1 ) / ( 1 - t ) log_det = - jnp . log ( 1 - t ) return x0 , log_det def vector_field ( self , t : Array , xt : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"The vector field that samples evolve on as t changes **Arguments**: - `t`: Time. - `x0`: A point in the base space. - `y`: The conditioning information. **Returns**: The vector field that samples evolve on at (t, x). \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if y is None : raise ValueError ( f 'Expected a conditional input' ) if y . shape != x . shape : raise ValueError ( f 'Expected y.shape ( { y . shape } ) to match x.shape ( { x . shape } )' ) return y - x","title":"ConditionalOptionalTransport"},{"location":"api/flows/affine/#generax.flows.affine.TallDenseLinear","text":"Matrix vector product with a tall matrix. Attributes : - W : The weight matrix Source code in generax/flows/affine.py class TallDenseLinear ( InjectiveTransform ): \"\"\"Matrix vector product with a tall matrix. **Attributes**: - `W`: The weight matrix \"\"\" W : Array def __init__ ( self , input_shape : Tuple [ int ], output_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" assert len ( input_shape ) == 1 , 'Only implemented for 1d data' super () . __init__ ( input_shape = input_shape , output_shape = output_shape , ** kwargs ) dim_in = self . input_shape [ - 1 ] dim_out = self . output_shape [ - 1 ] self . W = random . normal ( key , shape = ( dim_in , dim_out )) self . W = misc . whiten ( self . W ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" if inverse == False : assert x . shape == self . input_shape , 'Only works on unbatched data' else : assert x . shape == self . output_shape , 'Only works on unbatched data' if inverse == False : W_pinv = jnp . linalg . pinv ( self . W ) z = jnp . einsum ( 'ij,...j->...i' , W_pinv , x ) else : z = jnp . einsum ( 'ij,...j->...i' , self . W , x ) log_det = - 0.5 * jnp . linalg . slogdet ( self . W . T @self . W )[ 1 ] if inverse : log_det *= - 1 return z , log_det def log_determinant ( self , z : Array , ** kwargs ) -> Array : \"\"\"Compute -0.5*log(det(J^TJ)) **Arguments**: - `z`: An element of the base space **Returns**: The log determinant of (J^TJ)^0.5 \"\"\" log_det = - 0.5 * jnp . linalg . slogdet ( self . W . T @self . W )[ 1 ] return log_det","title":"TallDenseLinear"},{"location":"api/flows/base/","text":"Base \u00a4 generax.flows.base.BijectiveTransform \u00a4 This represents a bijective transformation. Atributes : input_shape : The input shape. Output shape will have the same dimensionality as the input. cond_shape : The shape of the conditioning information. If there is no conditioning information, this is None. Source code in generax/flows/base.py class BijectiveTransform ( eqx . Module , ABC ): \"\"\"This represents a bijective transformation. **Atributes**: - `input_shape`: The input shape. Output shape will have the same dimensionality as the input. - `cond_shape`: The shape of the conditioning information. If there is no conditioning information, this is None. \"\"\" input_shape : Tuple [ int ] = eqx . field ( static = True ) cond_shape : Union [ None , Tuple [ int ]] = eqx . field ( static = True ) def __init__ ( self , * _ , input_shape : Tuple [ int ], cond_shape : Union [ None , Tuple [ int ]] = None , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `cond_shape`: The shape of the conditioning information. If there is no \"\"\" super () . __init__ ( ** kwargs ) assert isinstance ( input_shape , tuple ) or isinstance ( input_shape , list ) self . input_shape = tuple ( input_shape ) if cond_shape is not None : assert isinstance ( cond_shape , tuple ) or isinstance ( cond_shape , list ) self . cond_shape = tuple ( cond_shape ) else : self . cond_shape = None def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ): \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" return self @abstractmethod def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" pass def to_base_space ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: z \"\"\" return self ( x , y = y , ** kwargs )[ 0 ] def to_data_space ( self , z : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `z`: The input to the transformation - `y`: The conditioning information **Returns**: x \"\"\" return self ( z , y = y , inverse = True , ** kwargs )[ 0 ] def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs ) def get_inverse ( self ) -> 'BijectiveTransform' : \"\"\"Get a new `BijectiveTransform` that is the inverse of this one. **Returns**: The inverse transformation. \"\"\" class Wrapper ( eqx . Module ): transform : BijectiveTransform input_shape : Tuple [ int ] cond_shape : Tuple [ int ] def __init__ ( self , transform ): self . transform = transform self . input_shape = transform . input_shape self . cond_shape = transform . cond_shape def __call__ ( self , x , y = None , inverse = False , ** kwargs ): return self . transform ( x , y = y , inverse = not inverse , ** kwargs ) def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ): # Invert first def apply_fun ( x ): return self ( x , y = y )[ 0 ] z = eqx . filter_vmap ( apply_fun )( x ) # Regular data dependent init new_layer = self . transform . data_dependent_init ( z , y = y , key = key ) return new_layer . get_inverse () @property def __wrapped__ ( self ): return self . transform return eqx . module_update_wrapper ( Wrapper ( self )) __init__ ( self , * _ , * , input_shape : Tuple [ int ], cond_shape : Optional [ Tuple [ int ]] = None , ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. cond_shape : The shape of the conditioning information. If there is no Source code in generax/flows/base.py def __init__ ( self , * _ , input_shape : Tuple [ int ], cond_shape : Union [ None , Tuple [ int ]] = None , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `cond_shape`: The shape of the conditioning information. If there is no \"\"\" super () . __init__ ( ** kwargs ) assert isinstance ( input_shape , tuple ) or isinstance ( input_shape , list ) self . input_shape = tuple ( input_shape ) if cond_shape is not None : assert isinstance ( cond_shape , tuple ) or isinstance ( cond_shape , list ) self . cond_shape = tuple ( cond_shape ) else : self . cond_shape = None data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) \u00a4 Initialize the parameters of the layer based on the data. Arguments : x : The data to initialize the parameters with. y : The conditioning information key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/flows/base.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ): \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" return self __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array abstractmethod \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation Returns : (z, log_det) Source code in generax/flows/base.py @abstractmethod def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" pass inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Apply the inverse transformation. Arguments : x : The input to the transformation y : The conditioning information Returns : (z, log_det) Source code in generax/flows/base.py def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs ) get_inverse ( self ) -> BijectiveTransform \u00a4 Get a new BijectiveTransform that is the inverse of this one. Returns : The inverse transformation. Source code in generax/flows/base.py def get_inverse ( self ) -> 'BijectiveTransform' : \"\"\"Get a new `BijectiveTransform` that is the inverse of this one. **Returns**: The inverse transformation. \"\"\" class Wrapper ( eqx . Module ): transform : BijectiveTransform input_shape : Tuple [ int ] cond_shape : Tuple [ int ] def __init__ ( self , transform ): self . transform = transform self . input_shape = transform . input_shape self . cond_shape = transform . cond_shape def __call__ ( self , x , y = None , inverse = False , ** kwargs ): return self . transform ( x , y = y , inverse = not inverse , ** kwargs ) def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ): # Invert first def apply_fun ( x ): return self ( x , y = y )[ 0 ] z = eqx . filter_vmap ( apply_fun )( x ) # Regular data dependent init new_layer = self . transform . data_dependent_init ( z , y = y , key = key ) return new_layer . get_inverse () @property def __wrapped__ ( self ): return self . transform return eqx . module_update_wrapper ( Wrapper ( self )) generax.flows.base.TimeDependentBijectiveTransform ( BijectiveTransform ) \u00a4 Time dependent bijective transform. This will help us build simple probability paths. Non-inverse mode goes t -> 0 while inverse mode goes t -> 1. Atributes : input_shape : The input shape. Output shape will have the same dimensionality as the input. cond_shape : The shape of the conditioning information. If there is no conditioning information, this is None. Source code in generax/flows/base.py class TimeDependentBijectiveTransform ( BijectiveTransform ): \"\"\"Time dependent bijective transform. This will help us build simple probability paths. Non-inverse mode goes t -> 0 while inverse mode goes t -> 1. **Atributes**: - `input_shape`: The input shape. Output shape will have the same dimensionality as the input. - `cond_shape`: The shape of the conditioning information. If there is no conditioning information, this is None. \"\"\" def data_dependent_init ( self , t : Array , xt : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ): \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `t`: Time. - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" return self @abstractmethod def __call__ ( self , t : Array , xt : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `xt`: The input to the transformation. If inverse=True, then should be x0 - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (x0, log_det) \"\"\" pass def inverse ( self , t : Array , x0 : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (xt, log_det) \"\"\" return self ( t , x0 , y = y , inverse = True , ** kwargs ) def to_base_space ( self , t : Array , xt : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: z \"\"\" return self ( t , xt , y = y , ** kwargs )[ 0 ] def to_data_space ( self , t : Array , x0 : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `z`: The input to the transformation - `y`: The conditioning information **Returns**: x \"\"\" return self ( t , x0 , y = y , inverse = True , ** kwargs )[ 0 ] def vector_field ( self , t : Array , xt : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"The vector field that samples evolve on as t changes **Arguments**: - `t`: Time. - `xt`: A point in the data space. - `y`: The conditioning information. **Returns**: `return vt` \"\"\" x0 = self . to_base_space ( t , xt , y = y , ** kwargs ) def ft ( t ): return self . to_data_space ( t , x0 , y = y , ** kwargs ) return jax . jvp ( ft , ( t ,), ( jnp . ones_like ( t ),))[ 1 ] __init__ ( self , * _ , * , input_shape : Tuple [ int ], cond_shape : Optional [ Tuple [ int ]] = None , ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. cond_shape : The shape of the conditioning information. If there is no Source code in generax/flows/base.py def __init__ ( self , * _ , input_shape : Tuple [ int ], cond_shape : Union [ None , Tuple [ int ]] = None , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `cond_shape`: The shape of the conditioning information. If there is no \"\"\" super () . __init__ ( ** kwargs ) assert isinstance ( input_shape , tuple ) or isinstance ( input_shape , list ) self . input_shape = tuple ( input_shape ) if cond_shape is not None : assert isinstance ( cond_shape , tuple ) or isinstance ( cond_shape , list ) self . cond_shape = tuple ( cond_shape ) else : self . cond_shape = None data_dependent_init ( self , t : Array , xt : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) \u00a4 Initialize the parameters of the layer based on the data. Arguments : t : Time. x : The data to initialize the parameters with. y : The conditioning information key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/flows/base.py def data_dependent_init ( self , t : Array , xt : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ): \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `t`: Time. - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" return self __call__ ( self , t : Array , xt : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array abstractmethod \u00a4 Arguments : xt : The input to the transformation. If inverse=True, then should be x0 y : The conditioning information inverse : Whether to inverse the transformation Returns : (x0, log_det) Source code in generax/flows/base.py @abstractmethod def __call__ ( self , t : Array , xt : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `xt`: The input to the transformation. If inverse=True, then should be x0 - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (x0, log_det) \"\"\" pass inverse ( self , t : Array , x0 : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Apply the inverse transformation. Arguments : x : The input to the transformation y : The conditioning information Returns : (xt, log_det) Source code in generax/flows/base.py def inverse ( self , t : Array , x0 : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (xt, log_det) \"\"\" return self ( t , x0 , y = y , inverse = True , ** kwargs ) vector_field ( self , t : Array , xt : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 The vector field that samples evolve on as t changes Arguments : t : Time. xt : A point in the data space. y : The conditioning information. Returns : return vt Source code in generax/flows/base.py def vector_field ( self , t : Array , xt : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"The vector field that samples evolve on as t changes **Arguments**: - `t`: Time. - `xt`: A point in the data space. - `y`: The conditioning information. **Returns**: `return vt` \"\"\" x0 = self . to_base_space ( t , xt , y = y , ** kwargs ) def ft ( t ): return self . to_data_space ( t , x0 , y = y , ** kwargs ) return jax . jvp ( ft , ( t ,), ( jnp . ones_like ( t ),))[ 1 ] generax.flows.base.Repeat ( BijectiveTransform ) \u00a4 A repeated bijective transformations that is vmapped together. The input to this function should be an initializer function for a transform. For example: def make_layer ( key ): return ShiftScale ( input_shape = x_shape , key = key ) layer = Repeat ( make_layer , n_repeats = 3 , key = key ) Attributes : - layers : A vmapped layer in the composition Source code in generax/flows/base.py class Repeat ( BijectiveTransform ): \"\"\"A repeated bijective transformations that is vmapped together. The input to this function should be an initializer function for a transform. For example: ```python def make_layer(key): return ShiftScale(input_shape=x_shape, key=key) layer = Repeat(make_layer, n_repeats=3, key=key) ``` **Attributes**: - `layers`: A vmapped layer in the composition \"\"\" n_repeats : int = eqx . field ( static = True ) layers : BijectiveTransform def __init__ ( self , layer_init : Callable [[ PRNGKeyArray ], BijectiveTransform ], n_repeats : int , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `layers`: A sequence of `BijectiveTransform`. \"\"\" self . n_repeats = n_repeats keys = random . split ( key , n_repeats ) self . layers = eqx . filter_vmap ( layer_init )( keys ) super () . __init__ ( input_shape = self . layers . input_shape , cond_shape = self . layers . cond_shape , ** kwargs ) def to_sequential ( self ) -> Sequential : \"\"\"Convert this to a sequential composition. \"\"\" params , static = eqx . partition ( self . layers , eqx . is_array ) def make_layer ( single_parameters : PyTree ): return eqx . combine ( single_parameters , static ) layers = [] for i in range ( self . n_repeats ): layer = make_layer ( jax . tree_util . tree_map ( lambda x : x [ i ], params )) layers . append ( layer ) return Sequential ( * layers ) def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : seq = self . to_sequential () # Apply the data dependent initalization out_seq_layers = seq . data_dependent_init ( x , y = y , key = key ) # Turn the sequential layers into a repeat layer all_params = [] for i , layer in enumerate ( out_seq_layers ): params , _ = eqx . partition ( layer , eqx . is_array ) all_params . append ( params ) # Combine the parameters back into a single layer params = jax . tree_util . tree_map ( lambda * args : jnp . array ( args ), * all_params ) _ , static = eqx . partition ( self . layers , eqx . is_array ) layers = eqx . combine ( params , static ) get_layers = lambda tree : tree . layers updated_module = eqx . tree_at ( get_layers , self , layers ) return updated_module def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" dynamic , static = eqx . partition ( self . layers , eqx . is_array ) def scan_body ( x , params ): block = eqx . combine ( params , static ) x , log_det = block ( x , y = y , inverse = inverse , ** kwargs ) return x , log_det x , log_dets = jax . lax . scan ( scan_body , x , dynamic , reverse = inverse ) return x , log_dets . sum () inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Inherited from generax.flows.base.BijectiveTransform.inverse . Source code in generax/flows/base.py def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs ) __init__ ( self , layer_init : Callable [[ PRNGKeyArray ], BijectiveTransform ], n_repeats : int , * , key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : layers : A sequence of BijectiveTransform . Source code in generax/flows/base.py def __init__ ( self , layer_init : Callable [[ PRNGKeyArray ], BijectiveTransform ], n_repeats : int , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `layers`: A sequence of `BijectiveTransform`. \"\"\" self . n_repeats = n_repeats keys = random . split ( key , n_repeats ) self . layers = eqx . filter_vmap ( layer_init )( keys ) super () . __init__ ( input_shape = self . layers . input_shape , cond_shape = self . layers . cond_shape , ** kwargs ) to_sequential ( self ) -> Sequential \u00a4 Convert this to a sequential composition. Source code in generax/flows/base.py def to_sequential ( self ) -> Sequential : \"\"\"Convert this to a sequential composition. \"\"\" params , static = eqx . partition ( self . layers , eqx . is_array ) def make_layer ( single_parameters : PyTree ): return eqx . combine ( single_parameters , static ) layers = [] for i in range ( self . n_repeats ): layer = make_layer ( jax . tree_util . tree_map ( lambda x : x [ i ], params )) layers . append ( layer ) return Sequential ( * layers ) data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform \u00a4 Initialize the parameters of the layer based on the data. Arguments : x : The data to initialize the parameters with. y : The conditioning information key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/flows/base.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : seq = self . to_sequential () # Apply the data dependent initalization out_seq_layers = seq . data_dependent_init ( x , y = y , key = key ) # Turn the sequential layers into a repeat layer all_params = [] for i , layer in enumerate ( out_seq_layers ): params , _ = eqx . partition ( layer , eqx . is_array ) all_params . append ( params ) # Combine the parameters back into a single layer params = jax . tree_util . tree_map ( lambda * args : jnp . array ( args ), * all_params ) _ , static = eqx . partition ( self . layers , eqx . is_array ) layers = eqx . combine ( params , static ) get_layers = lambda tree : tree . layers updated_module = eqx . tree_at ( get_layers , self , layers ) return updated_module __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Implements generax.flows.base.BijectiveTransform.__call__ . Source code in generax/flows/base.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" dynamic , static = eqx . partition ( self . layers , eqx . is_array ) def scan_body ( x , params ): block = eqx . combine ( params , static ) x , log_det = block ( x , y = y , inverse = inverse , ** kwargs ) return x , log_det x , log_dets = jax . lax . scan ( scan_body , x , dynamic , reverse = inverse ) return x , log_dets . sum () generax.flows.base.TimeDependentRepeat ( TimeDependentBijectiveTransform ) \u00a4 A time dependent repeated bijective transformations that is vmapped together. The input to this function should be an initializer function for a transform. For example: def make_layer ( key ): return ShiftScale ( input_shape = x_shape , key = key ) layer = Repeat ( make_layer , n_repeats = 3 , key = key ) Attributes : - layers : A vmapped layer in the composition Source code in generax/flows/base.py class TimeDependentRepeat ( TimeDependentBijectiveTransform ): \"\"\"A time dependent repeated bijective transformations that is vmapped together. The input to this function should be an initializer function for a transform. For example: ```python def make_layer(key): return ShiftScale(input_shape=x_shape, key=key) layer = Repeat(make_layer, n_repeats=3, key=key) ``` **Attributes**: - `layers`: A vmapped layer in the composition \"\"\" n_repeats : int = eqx . field ( static = True ) layers : BijectiveTransform def __init__ ( self , layer_init : Callable [[ PRNGKeyArray ], BijectiveTransform ], n_repeats : int , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `layers`: A sequence of `BijectiveTransform`. \"\"\" self . n_repeats = n_repeats keys = random . split ( key , n_repeats ) self . layers = eqx . filter_vmap ( layer_init )( keys ) super () . __init__ ( input_shape = self . layers . input_shape , cond_shape = self . layers . cond_shape , ** kwargs ) def to_sequential ( self ) -> TimeDependentSequential : \"\"\"Convert this to a sequential composition. \"\"\" params , static = eqx . partition ( self . layers , eqx . is_array ) def make_layer ( single_parameters : PyTree ): return eqx . combine ( single_parameters , static ) layers = [] for i in range ( self . n_repeats ): layer = make_layer ( jax . tree_util . tree_map ( lambda x : x [ i ], params )) layers . append ( layer ) return TimeDependentSequential ( * layers ) def data_dependent_init ( self , t : Array , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : seq = self . to_sequential () # Apply the data dependent initalization out_seq_layers = seq . data_dependent_init ( t , x , y = y , key = key ) # Turn the sequential layers into a repeat layer all_params = [] for i , layer in enumerate ( out_seq_layers ): params , _ = eqx . partition ( layer , eqx . is_array ) all_params . append ( params ) # Combine the parameters back into a single layer params = jax . tree_util . tree_map ( lambda * args : jnp . array ( args ), * all_params ) _ , static = eqx . partition ( self . layers , eqx . is_array ) layers = eqx . combine ( params , static ) get_layers = lambda tree : tree . layers updated_module = eqx . tree_at ( get_layers , self , layers ) return updated_module def __call__ ( self , t : Array , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" dynamic , static = eqx . partition ( self . layers , eqx . is_array ) def scan_body ( x , params ): block = eqx . combine ( params , static ) x , log_det = block ( t , x , y = y , inverse = inverse , ** kwargs ) return x , log_det x , log_dets = jax . lax . scan ( scan_body , x , dynamic , reverse = inverse ) return x , log_dets . sum () inverse ( self , t : Array , x0 : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Inherited from generax.flows.base.TimeDependentBijectiveTransform.inverse . Source code in generax/flows/base.py def inverse ( self , t : Array , x0 : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (xt, log_det) \"\"\" return self ( t , x0 , y = y , inverse = True , ** kwargs ) vector_field ( self , t : Array , xt : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Inherited from generax.flows.base.TimeDependentBijectiveTransform.vector_field . Source code in generax/flows/base.py def vector_field ( self , t : Array , xt : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"The vector field that samples evolve on as t changes **Arguments**: - `t`: Time. - `xt`: A point in the data space. - `y`: The conditioning information. **Returns**: `return vt` \"\"\" x0 = self . to_base_space ( t , xt , y = y , ** kwargs ) def ft ( t ): return self . to_data_space ( t , x0 , y = y , ** kwargs ) return jax . jvp ( ft , ( t ,), ( jnp . ones_like ( t ),))[ 1 ] __init__ ( self , layer_init : Callable [[ PRNGKeyArray ], BijectiveTransform ], n_repeats : int , * , key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : layers : A sequence of BijectiveTransform . Source code in generax/flows/base.py def __init__ ( self , layer_init : Callable [[ PRNGKeyArray ], BijectiveTransform ], n_repeats : int , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `layers`: A sequence of `BijectiveTransform`. \"\"\" self . n_repeats = n_repeats keys = random . split ( key , n_repeats ) self . layers = eqx . filter_vmap ( layer_init )( keys ) super () . __init__ ( input_shape = self . layers . input_shape , cond_shape = self . layers . cond_shape , ** kwargs ) to_sequential ( self ) -> TimeDependentSequential \u00a4 Convert this to a sequential composition. Source code in generax/flows/base.py def to_sequential ( self ) -> TimeDependentSequential : \"\"\"Convert this to a sequential composition. \"\"\" params , static = eqx . partition ( self . layers , eqx . is_array ) def make_layer ( single_parameters : PyTree ): return eqx . combine ( single_parameters , static ) layers = [] for i in range ( self . n_repeats ): layer = make_layer ( jax . tree_util . tree_map ( lambda x : x [ i ], params )) layers . append ( layer ) return TimeDependentSequential ( * layers ) data_dependent_init ( self , t : Array , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform \u00a4 Initialize the parameters of the layer based on the data. Arguments : t : Time. x : The data to initialize the parameters with. y : The conditioning information key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/flows/base.py def data_dependent_init ( self , t : Array , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : seq = self . to_sequential () # Apply the data dependent initalization out_seq_layers = seq . data_dependent_init ( t , x , y = y , key = key ) # Turn the sequential layers into a repeat layer all_params = [] for i , layer in enumerate ( out_seq_layers ): params , _ = eqx . partition ( layer , eqx . is_array ) all_params . append ( params ) # Combine the parameters back into a single layer params = jax . tree_util . tree_map ( lambda * args : jnp . array ( args ), * all_params ) _ , static = eqx . partition ( self . layers , eqx . is_array ) layers = eqx . combine ( params , static ) get_layers = lambda tree : tree . layers updated_module = eqx . tree_at ( get_layers , self , layers ) return updated_module __call__ ( self , t : Array , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation Returns : (z, log_det) Source code in generax/flows/base.py def __call__ ( self , t : Array , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" dynamic , static = eqx . partition ( self . layers , eqx . is_array ) def scan_body ( x , params ): block = eqx . combine ( params , static ) x , log_det = block ( t , x , y = y , inverse = inverse , ** kwargs ) return x , log_det x , log_dets = jax . lax . scan ( scan_body , x , dynamic , reverse = inverse ) return x , log_dets . sum () generax.flows.base.Sequential ( BijectiveTransform ) \u00a4 A sequence of bijective transformations. Accepts a sequence of BijectiveTransform initializers. # Intented usage: layer1 = MyTransform ( ... ) layer2 = MyTransform ( ... ) transform = Sequential ( layer1 , layer2 ) Attributes : - n_layers : The number of layers in the composition - layers : A tuple of the layers in the composition Source code in generax/flows/base.py class Sequential ( BijectiveTransform ): \"\"\"A sequence of bijective transformations. Accepts a sequence of `BijectiveTransform` initializers. ```python # Intented usage: layer1 = MyTransform(...) layer2 = MyTransform(...) transform = Sequential(layer1, layer2) ``` **Attributes**: - `n_layers`: The number of layers in the composition - `layers`: A tuple of the layers in the composition \"\"\" n_layers : int = eqx . field ( static = True ) layers : Tuple [ BijectiveTransform ] def __init__ ( self , * layers : Sequence [ BijectiveTransform ], ** kwargs ): \"\"\"**Arguments**: - `layers`: A sequence of `BijectiveTransform`. \"\"\" input_shape = layers [ 0 ] . input_shape cond_shape = layers [ 0 ] . cond_shape # Check that all of the layers have the same cond shape for layer in layers : assert layer . cond_shape == cond_shape super () . __init__ ( input_shape = input_shape , cond_shape = cond_shape , ** kwargs ) self . layers = tuple ( layers ) self . n_layers = len ( layers ) def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' # We need to initialize each of the layers keys = random . split ( key , self . n_layers ) new_layers = [] for i , ( layer , key ) in enumerate ( zip ( self . layers , keys )): new_layer = layer . data_dependent_init ( x = x , y = y , key = key ) new_layers . append ( new_layer ) x , _ = eqx . filter_vmap ( new_layer )( x ) new_layers = tuple ( new_layers ) # Turn the new parameters into a new module get_layers = lambda tree : tree . layers updated_layer = eqx . tree_at ( get_layers , self , new_layers ) return updated_layer def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" delta_logpx = 0.0 layers = reversed ( self . layers ) if inverse else self . layers for layer in layers : x , log_det = layer ( x , y = y , inverse = inverse , ** kwargs ) delta_logpx += log_det return x , delta_logpx def __getitem__ ( self , i : Union [ int , slice ]) -> Callable : if isinstance ( i , int ): return self . layers [ i ] elif isinstance ( i , slice ): return Sequential ( self . layers [ i ]) else : raise TypeError ( f \"Indexing with type { type ( i ) } is not supported\" ) def __iter__ ( self ): yield from self . layers def __len__ ( self ): return len ( self . layers ) inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Inherited from generax.flows.base.BijectiveTransform.inverse . Source code in generax/flows/base.py def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs ) __init__ ( self , * layers : Sequence [ BijectiveTransform ], ** kwargs ) \u00a4 Arguments : layers : A sequence of BijectiveTransform . Source code in generax/flows/base.py def __init__ ( self , * layers : Sequence [ BijectiveTransform ], ** kwargs ): \"\"\"**Arguments**: - `layers`: A sequence of `BijectiveTransform`. \"\"\" input_shape = layers [ 0 ] . input_shape cond_shape = layers [ 0 ] . cond_shape # Check that all of the layers have the same cond shape for layer in layers : assert layer . cond_shape == cond_shape super () . __init__ ( input_shape = input_shape , cond_shape = cond_shape , ** kwargs ) self . layers = tuple ( layers ) self . n_layers = len ( layers ) data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform \u00a4 Initialize the parameters of the layer based on the data. Arguments : x : The data to initialize the parameters with. y : The conditioning information key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/flows/base.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' # We need to initialize each of the layers keys = random . split ( key , self . n_layers ) new_layers = [] for i , ( layer , key ) in enumerate ( zip ( self . layers , keys )): new_layer = layer . data_dependent_init ( x = x , y = y , key = key ) new_layers . append ( new_layer ) x , _ = eqx . filter_vmap ( new_layer )( x ) new_layers = tuple ( new_layers ) # Turn the new parameters into a new module get_layers = lambda tree : tree . layers updated_layer = eqx . tree_at ( get_layers , self , new_layers ) return updated_layer __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Implements generax.flows.base.BijectiveTransform.__call__ . Source code in generax/flows/base.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" delta_logpx = 0.0 layers = reversed ( self . layers ) if inverse else self . layers for layer in layers : x , log_det = layer ( x , y = y , inverse = inverse , ** kwargs ) delta_logpx += log_det return x , delta_logpx generax.flows.base.TimeDependentSequential ( TimeDependentBijectiveTransform ) \u00a4 A sequence of bijective transformations. Accepts a sequence of BijectiveTransform initializers. # Intented usage: layer1 = MyTransform ( ... ) layer2 = MyTransform ( ... ) transform = Sequential ( layer1 , layer2 ) Attributes : - n_layers : The number of layers in the composition - layers : A tuple of the layers in the composition Source code in generax/flows/base.py class TimeDependentSequential ( TimeDependentBijectiveTransform ): \"\"\"A sequence of bijective transformations. Accepts a sequence of `BijectiveTransform` initializers. ```python # Intented usage: layer1 = MyTransform(...) layer2 = MyTransform(...) transform = Sequential(layer1, layer2) ``` **Attributes**: - `n_layers`: The number of layers in the composition - `layers`: A tuple of the layers in the composition \"\"\" n_layers : int = eqx . field ( static = True ) layers : Tuple [ BijectiveTransform ] def __init__ ( self , * layers : Sequence [ BijectiveTransform ], ** kwargs ): \"\"\"**Arguments**: - `layers`: A sequence of `BijectiveTransform`. \"\"\" input_shape = layers [ 0 ] . input_shape cond_shape = layers [ 0 ] . cond_shape # Check that all of the layers have the same cond shape for layer in layers : assert layer . cond_shape == cond_shape super () . __init__ ( input_shape = input_shape , cond_shape = cond_shape , ** kwargs ) self . layers = tuple ( layers ) self . n_layers = len ( layers ) def data_dependent_init ( self , t : Array , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' # We need to initialize each of the layers keys = random . split ( key , self . n_layers ) new_layers = [] for i , ( layer , key ) in enumerate ( zip ( self . layers , keys )): new_layer = layer . data_dependent_init ( t , x = x , y = y , key = key ) new_layers . append ( new_layer ) x , _ = eqx . filter_vmap ( new_layer )( t , x ) new_layers = tuple ( new_layers ) # Turn the new parameters into a new module get_layers = lambda tree : tree . layers updated_layer = eqx . tree_at ( get_layers , self , new_layers ) return updated_layer def __call__ ( self , t : Array , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" delta_logpx = 0.0 layers = reversed ( self . layers ) if inverse else self . layers for layer in layers : x , log_det = layer ( t , x , y = y , inverse = inverse , ** kwargs ) delta_logpx += log_det return x , delta_logpx def __getitem__ ( self , i : Union [ int , slice ]) -> Callable : if isinstance ( i , int ): return self . layers [ i ] elif isinstance ( i , slice ): return Sequential ( self . layers [ i ]) else : raise TypeError ( f \"Indexing with type { type ( i ) } is not supported\" ) def __iter__ ( self ): yield from self . layers def __len__ ( self ): return len ( self . layers ) inverse ( self , t : Array , x0 : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Inherited from generax.flows.base.TimeDependentBijectiveTransform.inverse . Source code in generax/flows/base.py def inverse ( self , t : Array , x0 : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (xt, log_det) \"\"\" return self ( t , x0 , y = y , inverse = True , ** kwargs ) vector_field ( self , t : Array , xt : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Inherited from generax.flows.base.TimeDependentBijectiveTransform.vector_field . Source code in generax/flows/base.py def vector_field ( self , t : Array , xt : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"The vector field that samples evolve on as t changes **Arguments**: - `t`: Time. - `xt`: A point in the data space. - `y`: The conditioning information. **Returns**: `return vt` \"\"\" x0 = self . to_base_space ( t , xt , y = y , ** kwargs ) def ft ( t ): return self . to_data_space ( t , x0 , y = y , ** kwargs ) return jax . jvp ( ft , ( t ,), ( jnp . ones_like ( t ),))[ 1 ] __init__ ( self , * layers : Sequence [ BijectiveTransform ], ** kwargs ) \u00a4 Arguments : layers : A sequence of BijectiveTransform . Source code in generax/flows/base.py def __init__ ( self , * layers : Sequence [ BijectiveTransform ], ** kwargs ): \"\"\"**Arguments**: - `layers`: A sequence of `BijectiveTransform`. \"\"\" input_shape = layers [ 0 ] . input_shape cond_shape = layers [ 0 ] . cond_shape # Check that all of the layers have the same cond shape for layer in layers : assert layer . cond_shape == cond_shape super () . __init__ ( input_shape = input_shape , cond_shape = cond_shape , ** kwargs ) self . layers = tuple ( layers ) self . n_layers = len ( layers ) data_dependent_init ( self , t : Array , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform \u00a4 Initialize the parameters of the layer based on the data. Arguments : x : The data to initialize the parameters with. y : The conditioning information key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/flows/base.py def data_dependent_init ( self , t : Array , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' # We need to initialize each of the layers keys = random . split ( key , self . n_layers ) new_layers = [] for i , ( layer , key ) in enumerate ( zip ( self . layers , keys )): new_layer = layer . data_dependent_init ( t , x = x , y = y , key = key ) new_layers . append ( new_layer ) x , _ = eqx . filter_vmap ( new_layer )( t , x ) new_layers = tuple ( new_layers ) # Turn the new parameters into a new module get_layers = lambda tree : tree . layers updated_layer = eqx . tree_at ( get_layers , self , new_layers ) return updated_layer __call__ ( self , t : Array , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation Returns : (z, log_det) Source code in generax/flows/base.py def __call__ ( self , t : Array , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" delta_logpx = 0.0 layers = reversed ( self . layers ) if inverse else self . layers for layer in layers : x , log_det = layer ( t , x , y = y , inverse = inverse , ** kwargs ) delta_logpx += log_det return x , delta_logpx generax.flows.base.InjectiveTransform ( BijectiveTransform ) \u00a4 This represents an injective transformation. This is a special case of a bijective transformation. Atributes : input_shape : The input shape. output_shape : The output shape. cond_shape : The shape of the conditioning information. If there is no conditioning information, this is None. Source code in generax/flows/base.py class InjectiveTransform ( BijectiveTransform , ABC ): \"\"\"This represents an injective transformation. This is a special case of a bijective transformation. **Atributes**: - `input_shape`: The input shape. - `output_shape`: The output shape. - `cond_shape`: The shape of the conditioning information. If there is no conditioning information, this is None. \"\"\" output_shape : Tuple [ int ] = eqx . field ( static = True ) def __init__ ( self , * _ , input_shape : Tuple [ int ], output_shape : Tuple [ int ], cond_shape : Union [ None , Tuple [ int ]] = None , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. - `output_shape`: The output shape. - `cond_shape`: The shape of the conditioning information. If there is no \"\"\" super () . __init__ ( input_shape = input_shape , cond_shape = cond_shape , ** kwargs ) assert isinstance ( output_shape , tuple ) or isinstance ( output_shape , list ) self . output_shape = output_shape def project ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Project a point onto the image of the transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: z \"\"\" z , _ = self ( x , y = y , ** kwargs ) x_proj , _ = self ( z , y = y , inverse = True , ** kwargs ) return x_proj def log_determinant ( self , z : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Compute -0.5*log(det(J^TJ)) **Arguments**: - `z`: An element of the base space **Returns**: The log determinant of (J^TJ)^-0.5 \"\"\" def jvp ( v_flat ): v = v_flat . reshape ( z . shape ) _ , ( Jv ) = jax . jvp ( self . to_data_space , ( z ,), ( v ,)) return Jv . ravel () z_dim = util . list_prod ( z . shape ) eye = jnp . eye ( z_dim ) J = jax . vmap ( jvp , in_axes = 1 , out_axes = 1 )( eye ) return - 0.5 * jnp . linalg . slogdet ( J . T @J )[ 1 ] def log_determinant_surrogate ( z : Array , transform : eqx . Module , method : str = 'brute_force' , key : PRNGKeyArray = None , ** kwargs ) -> Array : \"\"\"Compute a term that has the same expected gradient as `-0.5*log_det(J^TJ))`. If `method='brute_force'`, then this is just -0.5*log(det(J^TJ)). If `method='iterative'`, then this is a term that has the same expected gradient. **Arguments**: - `z`: An element of the base space - `method`: How to compute the log determinant. Options are: - `brute_force`: Compute the entire Jacobian - `iterative`: Use conjugate gradient (https://arxiv.org/pdf/2106.01413.pdf) - `key`: A `jax.random.PRNGKey` for initialization. Needed for some methods **Returns**: The log determinant of J^TJ or a term that has the same gradient \"\"\" def jvp ( v_flat ): v = v_flat . reshape ( z . shape ) _ , ( Jv ) = jax . jvp ( transform , ( z ,), ( v ,)) return Jv . ravel () if method == 'brute_force' : z_dim = util . list_prod ( z . shape ) eye = jnp . eye ( z_dim ) J = jax . vmap ( jvp , in_axes = 1 , out_axes = 1 )( eye ) return - 0.5 * jnp . linalg . slogdet ( J . T @J )[ 1 ] elif method == 'iterative' : def vjp ( v_flat ): x , vjp = jax . vjp ( transform , z ) v = v_flat . reshape ( x . shape ) return vjp ( v )[ 0 ] . ravel () def vjp_jvp ( v_flat ): return vjp ( jvp ( v_flat )) v = random . normal ( key , shape = z . shape ) operator = lx . FunctionLinearOperator ( vjp_jvp , v , tags = lx . positive_semidefinite_tag ) solver = lx . CG ( rtol = 1e-3 , atol = 1e-6 ) JTJinv_v = lx . linear_solve ( operator , v , solver ) . value JTJ_v = vjp_jvp ( v ) return - 0.5 * jnp . vdot ( jax . lax . stop_gradient ( JTJinv_v ), JTJ_v ) data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) \u00a4 Inherited from generax.flows.base.BijectiveTransform.data_dependent_init . Source code in generax/flows/base.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ): \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" return self __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array abstractmethod \u00a4 Implements generax.flows.base.BijectiveTransform.__call__ . Source code in generax/flows/base.py @abstractmethod def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" pass inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Inherited from generax.flows.base.BijectiveTransform.inverse . Source code in generax/flows/base.py def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs ) __init__ ( self , * _ , * , input_shape : Tuple [ int ], output_shape : Tuple [ int ], cond_shape : Optional [ Tuple [ int ]] = None , ** kwargs ) \u00a4 Arguments : input_shape : The input shape. output_shape : The output shape. cond_shape : The shape of the conditioning information. If there is no Source code in generax/flows/base.py def __init__ ( self , * _ , input_shape : Tuple [ int ], output_shape : Tuple [ int ], cond_shape : Union [ None , Tuple [ int ]] = None , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. - `output_shape`: The output shape. - `cond_shape`: The shape of the conditioning information. If there is no \"\"\" super () . __init__ ( input_shape = input_shape , cond_shape = cond_shape , ** kwargs ) assert isinstance ( output_shape , tuple ) or isinstance ( output_shape , list ) self . output_shape = output_shape project ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Project a point onto the image of the transformation. Arguments : x : The input to the transformation y : The conditioning information Returns : z Source code in generax/flows/base.py def project ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Project a point onto the image of the transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: z \"\"\" z , _ = self ( x , y = y , ** kwargs ) x_proj , _ = self ( z , y = y , inverse = True , ** kwargs ) return x_proj log_determinant ( self , z : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Compute -0.5*log(det(J^TJ)) Arguments : z : An element of the base space Returns : The log determinant of (J^TJ)^-0.5 Source code in generax/flows/base.py def log_determinant ( self , z : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Compute -0.5*log(det(J^TJ)) **Arguments**: - `z`: An element of the base space **Returns**: The log determinant of (J^TJ)^-0.5 \"\"\" def jvp ( v_flat ): v = v_flat . reshape ( z . shape ) _ , ( Jv ) = jax . jvp ( self . to_data_space , ( z ,), ( v ,)) return Jv . ravel () z_dim = util . list_prod ( z . shape ) eye = jnp . eye ( z_dim ) J = jax . vmap ( jvp , in_axes = 1 , out_axes = 1 )( eye ) return - 0.5 * jnp . linalg . slogdet ( J . T @J )[ 1 ] log_determinant_surrogate ( z : Array , transform : Module , method : str = 'brute_force' , key : PRNGKeyArray = None , ** kwargs ) -> Array \u00a4 Compute a term that has the same expected gradient as -0.5*log_det(J^TJ)) . If method='brute_force' , then this is just -0.5*log(det(J^TJ)). If method='iterative' , then this is a term that has the same expected gradient. Arguments : z : An element of the base space method : How to compute the log determinant. Options are: brute_force : Compute the entire Jacobian iterative : Use conjugate gradient (https://arxiv.org/pdf/2106.01413.pdf) key : A jax.random.PRNGKey for initialization. Needed for some methods Returns : The log determinant of J^TJ or a term that has the same gradient Source code in generax/flows/base.py def log_determinant_surrogate ( z : Array , transform : eqx . Module , method : str = 'brute_force' , key : PRNGKeyArray = None , ** kwargs ) -> Array : \"\"\"Compute a term that has the same expected gradient as `-0.5*log_det(J^TJ))`. If `method='brute_force'`, then this is just -0.5*log(det(J^TJ)). If `method='iterative'`, then this is a term that has the same expected gradient. **Arguments**: - `z`: An element of the base space - `method`: How to compute the log determinant. Options are: - `brute_force`: Compute the entire Jacobian - `iterative`: Use conjugate gradient (https://arxiv.org/pdf/2106.01413.pdf) - `key`: A `jax.random.PRNGKey` for initialization. Needed for some methods **Returns**: The log determinant of J^TJ or a term that has the same gradient \"\"\" def jvp ( v_flat ): v = v_flat . reshape ( z . shape ) _ , ( Jv ) = jax . jvp ( transform , ( z ,), ( v ,)) return Jv . ravel () if method == 'brute_force' : z_dim = util . list_prod ( z . shape ) eye = jnp . eye ( z_dim ) J = jax . vmap ( jvp , in_axes = 1 , out_axes = 1 )( eye ) return - 0.5 * jnp . linalg . slogdet ( J . T @J )[ 1 ] elif method == 'iterative' : def vjp ( v_flat ): x , vjp = jax . vjp ( transform , z ) v = v_flat . reshape ( x . shape ) return vjp ( v )[ 0 ] . ravel () def vjp_jvp ( v_flat ): return vjp ( jvp ( v_flat )) v = random . normal ( key , shape = z . shape ) operator = lx . FunctionLinearOperator ( vjp_jvp , v , tags = lx . positive_semidefinite_tag ) solver = lx . CG ( rtol = 1e-3 , atol = 1e-6 ) JTJinv_v = lx . linear_solve ( operator , v , solver ) . value JTJ_v = vjp_jvp ( v ) return - 0.5 * jnp . vdot ( jax . lax . stop_gradient ( JTJinv_v ), JTJ_v ) generax.flows.base.InjectiveSequential ( Sequential , InjectiveTransform ) \u00a4 A sequence of injective or bijective transformations. Source code in generax/flows/base.py class InjectiveSequential ( Sequential , InjectiveTransform ): \"\"\"A sequence of injective or bijective transformations. \"\"\" def __init__ ( self , * layers : Sequence [ BijectiveTransform ], ** kwargs ): \"\"\"**Arguments**: - `layers`: A sequence of `BijectiveTransform`. \"\"\" input_shape = layers [ 0 ] . input_shape cond_shape = layers [ 0 ] . cond_shape # Check that all of the layers have the same cond shape # and that the output shape of each layer matches the input shape of the next layer layer_iter = iter ( zip ( layers [: - 1 ], layers [ 1 :])) for l1 , l2 in layer_iter : assert l1 . cond_shape == cond_shape if isinstance ( l1 , InjectiveTransform ): assert l1 . output_shape == l2 . input_shape assert l2 . cond_shape == cond_shape if isinstance ( l2 , InjectiveTransform ): output_shape = l2 . output_shape else : output_shape = l2 . input_shape InjectiveTransform . __init__ ( self , input_shape = input_shape , output_shape = output_shape , cond_shape = cond_shape , ** kwargs ) self . layers = tuple ( layers ) self . n_layers = len ( layers ) inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Apply the inverse transformation. Arguments : x : The input to the transformation y : The conditioning information Returns : (z, log_det) Source code in generax/flows/base.py def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs ) data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform \u00a4 Inherited from generax.flows.base.Sequential.data_dependent_init . Source code in generax/flows/base.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' # We need to initialize each of the layers keys = random . split ( key , self . n_layers ) new_layers = [] for i , ( layer , key ) in enumerate ( zip ( self . layers , keys )): new_layer = layer . data_dependent_init ( x = x , y = y , key = key ) new_layers . append ( new_layer ) x , _ = eqx . filter_vmap ( new_layer )( x ) new_layers = tuple ( new_layers ) # Turn the new parameters into a new module get_layers = lambda tree : tree . layers updated_layer = eqx . tree_at ( get_layers , self , new_layers ) return updated_layer __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Inherited from generax.flows.base.Sequential.__call__ . Source code in generax/flows/base.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" delta_logpx = 0.0 layers = reversed ( self . layers ) if inverse else self . layers for layer in layers : x , log_det = layer ( x , y = y , inverse = inverse , ** kwargs ) delta_logpx += log_det return x , delta_logpx __init__ ( self , * layers : Sequence [ BijectiveTransform ], ** kwargs ) \u00a4 Arguments : layers : A sequence of BijectiveTransform . Source code in generax/flows/base.py def __init__ ( self , * layers : Sequence [ BijectiveTransform ], ** kwargs ): \"\"\"**Arguments**: - `layers`: A sequence of `BijectiveTransform`. \"\"\" input_shape = layers [ 0 ] . input_shape cond_shape = layers [ 0 ] . cond_shape # Check that all of the layers have the same cond shape # and that the output shape of each layer matches the input shape of the next layer layer_iter = iter ( zip ( layers [: - 1 ], layers [ 1 :])) for l1 , l2 in layer_iter : assert l1 . cond_shape == cond_shape if isinstance ( l1 , InjectiveTransform ): assert l1 . output_shape == l2 . input_shape assert l2 . cond_shape == cond_shape if isinstance ( l2 , InjectiveTransform ): output_shape = l2 . output_shape else : output_shape = l2 . input_shape InjectiveTransform . __init__ ( self , input_shape = input_shape , output_shape = output_shape , cond_shape = cond_shape , ** kwargs ) self . layers = tuple ( layers ) self . n_layers = len ( layers ) generax . flows . coupling . TimeDependentWrapper property readonly \u00a4","title":"Base"},{"location":"api/flows/base/#base","text":"","title":"Base"},{"location":"api/flows/base/#generax.flows.base.BijectiveTransform","text":"This represents a bijective transformation. Atributes : input_shape : The input shape. Output shape will have the same dimensionality as the input. cond_shape : The shape of the conditioning information. If there is no conditioning information, this is None. Source code in generax/flows/base.py class BijectiveTransform ( eqx . Module , ABC ): \"\"\"This represents a bijective transformation. **Atributes**: - `input_shape`: The input shape. Output shape will have the same dimensionality as the input. - `cond_shape`: The shape of the conditioning information. If there is no conditioning information, this is None. \"\"\" input_shape : Tuple [ int ] = eqx . field ( static = True ) cond_shape : Union [ None , Tuple [ int ]] = eqx . field ( static = True ) def __init__ ( self , * _ , input_shape : Tuple [ int ], cond_shape : Union [ None , Tuple [ int ]] = None , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `cond_shape`: The shape of the conditioning information. If there is no \"\"\" super () . __init__ ( ** kwargs ) assert isinstance ( input_shape , tuple ) or isinstance ( input_shape , list ) self . input_shape = tuple ( input_shape ) if cond_shape is not None : assert isinstance ( cond_shape , tuple ) or isinstance ( cond_shape , list ) self . cond_shape = tuple ( cond_shape ) else : self . cond_shape = None def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ): \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" return self @abstractmethod def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" pass def to_base_space ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: z \"\"\" return self ( x , y = y , ** kwargs )[ 0 ] def to_data_space ( self , z : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `z`: The input to the transformation - `y`: The conditioning information **Returns**: x \"\"\" return self ( z , y = y , inverse = True , ** kwargs )[ 0 ] def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs ) def get_inverse ( self ) -> 'BijectiveTransform' : \"\"\"Get a new `BijectiveTransform` that is the inverse of this one. **Returns**: The inverse transformation. \"\"\" class Wrapper ( eqx . Module ): transform : BijectiveTransform input_shape : Tuple [ int ] cond_shape : Tuple [ int ] def __init__ ( self , transform ): self . transform = transform self . input_shape = transform . input_shape self . cond_shape = transform . cond_shape def __call__ ( self , x , y = None , inverse = False , ** kwargs ): return self . transform ( x , y = y , inverse = not inverse , ** kwargs ) def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ): # Invert first def apply_fun ( x ): return self ( x , y = y )[ 0 ] z = eqx . filter_vmap ( apply_fun )( x ) # Regular data dependent init new_layer = self . transform . data_dependent_init ( z , y = y , key = key ) return new_layer . get_inverse () @property def __wrapped__ ( self ): return self . transform return eqx . module_update_wrapper ( Wrapper ( self ))","title":"BijectiveTransform"},{"location":"api/flows/base/#generax.flows.base.TimeDependentBijectiveTransform","text":"Time dependent bijective transform. This will help us build simple probability paths. Non-inverse mode goes t -> 0 while inverse mode goes t -> 1. Atributes : input_shape : The input shape. Output shape will have the same dimensionality as the input. cond_shape : The shape of the conditioning information. If there is no conditioning information, this is None. Source code in generax/flows/base.py class TimeDependentBijectiveTransform ( BijectiveTransform ): \"\"\"Time dependent bijective transform. This will help us build simple probability paths. Non-inverse mode goes t -> 0 while inverse mode goes t -> 1. **Atributes**: - `input_shape`: The input shape. Output shape will have the same dimensionality as the input. - `cond_shape`: The shape of the conditioning information. If there is no conditioning information, this is None. \"\"\" def data_dependent_init ( self , t : Array , xt : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ): \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `t`: Time. - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" return self @abstractmethod def __call__ ( self , t : Array , xt : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `xt`: The input to the transformation. If inverse=True, then should be x0 - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (x0, log_det) \"\"\" pass def inverse ( self , t : Array , x0 : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (xt, log_det) \"\"\" return self ( t , x0 , y = y , inverse = True , ** kwargs ) def to_base_space ( self , t : Array , xt : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: z \"\"\" return self ( t , xt , y = y , ** kwargs )[ 0 ] def to_data_space ( self , t : Array , x0 : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `z`: The input to the transformation - `y`: The conditioning information **Returns**: x \"\"\" return self ( t , x0 , y = y , inverse = True , ** kwargs )[ 0 ] def vector_field ( self , t : Array , xt : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"The vector field that samples evolve on as t changes **Arguments**: - `t`: Time. - `xt`: A point in the data space. - `y`: The conditioning information. **Returns**: `return vt` \"\"\" x0 = self . to_base_space ( t , xt , y = y , ** kwargs ) def ft ( t ): return self . to_data_space ( t , x0 , y = y , ** kwargs ) return jax . jvp ( ft , ( t ,), ( jnp . ones_like ( t ),))[ 1 ]","title":"TimeDependentBijectiveTransform"},{"location":"api/flows/base/#generax.flows.base.Repeat","text":"A repeated bijective transformations that is vmapped together. The input to this function should be an initializer function for a transform. For example: def make_layer ( key ): return ShiftScale ( input_shape = x_shape , key = key ) layer = Repeat ( make_layer , n_repeats = 3 , key = key ) Attributes : - layers : A vmapped layer in the composition Source code in generax/flows/base.py class Repeat ( BijectiveTransform ): \"\"\"A repeated bijective transformations that is vmapped together. The input to this function should be an initializer function for a transform. For example: ```python def make_layer(key): return ShiftScale(input_shape=x_shape, key=key) layer = Repeat(make_layer, n_repeats=3, key=key) ``` **Attributes**: - `layers`: A vmapped layer in the composition \"\"\" n_repeats : int = eqx . field ( static = True ) layers : BijectiveTransform def __init__ ( self , layer_init : Callable [[ PRNGKeyArray ], BijectiveTransform ], n_repeats : int , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `layers`: A sequence of `BijectiveTransform`. \"\"\" self . n_repeats = n_repeats keys = random . split ( key , n_repeats ) self . layers = eqx . filter_vmap ( layer_init )( keys ) super () . __init__ ( input_shape = self . layers . input_shape , cond_shape = self . layers . cond_shape , ** kwargs ) def to_sequential ( self ) -> Sequential : \"\"\"Convert this to a sequential composition. \"\"\" params , static = eqx . partition ( self . layers , eqx . is_array ) def make_layer ( single_parameters : PyTree ): return eqx . combine ( single_parameters , static ) layers = [] for i in range ( self . n_repeats ): layer = make_layer ( jax . tree_util . tree_map ( lambda x : x [ i ], params )) layers . append ( layer ) return Sequential ( * layers ) def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : seq = self . to_sequential () # Apply the data dependent initalization out_seq_layers = seq . data_dependent_init ( x , y = y , key = key ) # Turn the sequential layers into a repeat layer all_params = [] for i , layer in enumerate ( out_seq_layers ): params , _ = eqx . partition ( layer , eqx . is_array ) all_params . append ( params ) # Combine the parameters back into a single layer params = jax . tree_util . tree_map ( lambda * args : jnp . array ( args ), * all_params ) _ , static = eqx . partition ( self . layers , eqx . is_array ) layers = eqx . combine ( params , static ) get_layers = lambda tree : tree . layers updated_module = eqx . tree_at ( get_layers , self , layers ) return updated_module def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" dynamic , static = eqx . partition ( self . layers , eqx . is_array ) def scan_body ( x , params ): block = eqx . combine ( params , static ) x , log_det = block ( x , y = y , inverse = inverse , ** kwargs ) return x , log_det x , log_dets = jax . lax . scan ( scan_body , x , dynamic , reverse = inverse ) return x , log_dets . sum ()","title":"Repeat"},{"location":"api/flows/base/#generax.flows.base.TimeDependentRepeat","text":"A time dependent repeated bijective transformations that is vmapped together. The input to this function should be an initializer function for a transform. For example: def make_layer ( key ): return ShiftScale ( input_shape = x_shape , key = key ) layer = Repeat ( make_layer , n_repeats = 3 , key = key ) Attributes : - layers : A vmapped layer in the composition Source code in generax/flows/base.py class TimeDependentRepeat ( TimeDependentBijectiveTransform ): \"\"\"A time dependent repeated bijective transformations that is vmapped together. The input to this function should be an initializer function for a transform. For example: ```python def make_layer(key): return ShiftScale(input_shape=x_shape, key=key) layer = Repeat(make_layer, n_repeats=3, key=key) ``` **Attributes**: - `layers`: A vmapped layer in the composition \"\"\" n_repeats : int = eqx . field ( static = True ) layers : BijectiveTransform def __init__ ( self , layer_init : Callable [[ PRNGKeyArray ], BijectiveTransform ], n_repeats : int , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `layers`: A sequence of `BijectiveTransform`. \"\"\" self . n_repeats = n_repeats keys = random . split ( key , n_repeats ) self . layers = eqx . filter_vmap ( layer_init )( keys ) super () . __init__ ( input_shape = self . layers . input_shape , cond_shape = self . layers . cond_shape , ** kwargs ) def to_sequential ( self ) -> TimeDependentSequential : \"\"\"Convert this to a sequential composition. \"\"\" params , static = eqx . partition ( self . layers , eqx . is_array ) def make_layer ( single_parameters : PyTree ): return eqx . combine ( single_parameters , static ) layers = [] for i in range ( self . n_repeats ): layer = make_layer ( jax . tree_util . tree_map ( lambda x : x [ i ], params )) layers . append ( layer ) return TimeDependentSequential ( * layers ) def data_dependent_init ( self , t : Array , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : seq = self . to_sequential () # Apply the data dependent initalization out_seq_layers = seq . data_dependent_init ( t , x , y = y , key = key ) # Turn the sequential layers into a repeat layer all_params = [] for i , layer in enumerate ( out_seq_layers ): params , _ = eqx . partition ( layer , eqx . is_array ) all_params . append ( params ) # Combine the parameters back into a single layer params = jax . tree_util . tree_map ( lambda * args : jnp . array ( args ), * all_params ) _ , static = eqx . partition ( self . layers , eqx . is_array ) layers = eqx . combine ( params , static ) get_layers = lambda tree : tree . layers updated_module = eqx . tree_at ( get_layers , self , layers ) return updated_module def __call__ ( self , t : Array , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" dynamic , static = eqx . partition ( self . layers , eqx . is_array ) def scan_body ( x , params ): block = eqx . combine ( params , static ) x , log_det = block ( t , x , y = y , inverse = inverse , ** kwargs ) return x , log_det x , log_dets = jax . lax . scan ( scan_body , x , dynamic , reverse = inverse ) return x , log_dets . sum ()","title":"TimeDependentRepeat"},{"location":"api/flows/base/#generax.flows.base.Sequential","text":"A sequence of bijective transformations. Accepts a sequence of BijectiveTransform initializers. # Intented usage: layer1 = MyTransform ( ... ) layer2 = MyTransform ( ... ) transform = Sequential ( layer1 , layer2 ) Attributes : - n_layers : The number of layers in the composition - layers : A tuple of the layers in the composition Source code in generax/flows/base.py class Sequential ( BijectiveTransform ): \"\"\"A sequence of bijective transformations. Accepts a sequence of `BijectiveTransform` initializers. ```python # Intented usage: layer1 = MyTransform(...) layer2 = MyTransform(...) transform = Sequential(layer1, layer2) ``` **Attributes**: - `n_layers`: The number of layers in the composition - `layers`: A tuple of the layers in the composition \"\"\" n_layers : int = eqx . field ( static = True ) layers : Tuple [ BijectiveTransform ] def __init__ ( self , * layers : Sequence [ BijectiveTransform ], ** kwargs ): \"\"\"**Arguments**: - `layers`: A sequence of `BijectiveTransform`. \"\"\" input_shape = layers [ 0 ] . input_shape cond_shape = layers [ 0 ] . cond_shape # Check that all of the layers have the same cond shape for layer in layers : assert layer . cond_shape == cond_shape super () . __init__ ( input_shape = input_shape , cond_shape = cond_shape , ** kwargs ) self . layers = tuple ( layers ) self . n_layers = len ( layers ) def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' # We need to initialize each of the layers keys = random . split ( key , self . n_layers ) new_layers = [] for i , ( layer , key ) in enumerate ( zip ( self . layers , keys )): new_layer = layer . data_dependent_init ( x = x , y = y , key = key ) new_layers . append ( new_layer ) x , _ = eqx . filter_vmap ( new_layer )( x ) new_layers = tuple ( new_layers ) # Turn the new parameters into a new module get_layers = lambda tree : tree . layers updated_layer = eqx . tree_at ( get_layers , self , new_layers ) return updated_layer def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" delta_logpx = 0.0 layers = reversed ( self . layers ) if inverse else self . layers for layer in layers : x , log_det = layer ( x , y = y , inverse = inverse , ** kwargs ) delta_logpx += log_det return x , delta_logpx def __getitem__ ( self , i : Union [ int , slice ]) -> Callable : if isinstance ( i , int ): return self . layers [ i ] elif isinstance ( i , slice ): return Sequential ( self . layers [ i ]) else : raise TypeError ( f \"Indexing with type { type ( i ) } is not supported\" ) def __iter__ ( self ): yield from self . layers def __len__ ( self ): return len ( self . layers )","title":"Sequential"},{"location":"api/flows/base/#generax.flows.base.TimeDependentSequential","text":"A sequence of bijective transformations. Accepts a sequence of BijectiveTransform initializers. # Intented usage: layer1 = MyTransform ( ... ) layer2 = MyTransform ( ... ) transform = Sequential ( layer1 , layer2 ) Attributes : - n_layers : The number of layers in the composition - layers : A tuple of the layers in the composition Source code in generax/flows/base.py class TimeDependentSequential ( TimeDependentBijectiveTransform ): \"\"\"A sequence of bijective transformations. Accepts a sequence of `BijectiveTransform` initializers. ```python # Intented usage: layer1 = MyTransform(...) layer2 = MyTransform(...) transform = Sequential(layer1, layer2) ``` **Attributes**: - `n_layers`: The number of layers in the composition - `layers`: A tuple of the layers in the composition \"\"\" n_layers : int = eqx . field ( static = True ) layers : Tuple [ BijectiveTransform ] def __init__ ( self , * layers : Sequence [ BijectiveTransform ], ** kwargs ): \"\"\"**Arguments**: - `layers`: A sequence of `BijectiveTransform`. \"\"\" input_shape = layers [ 0 ] . input_shape cond_shape = layers [ 0 ] . cond_shape # Check that all of the layers have the same cond shape for layer in layers : assert layer . cond_shape == cond_shape super () . __init__ ( input_shape = input_shape , cond_shape = cond_shape , ** kwargs ) self . layers = tuple ( layers ) self . n_layers = len ( layers ) def data_dependent_init ( self , t : Array , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' # We need to initialize each of the layers keys = random . split ( key , self . n_layers ) new_layers = [] for i , ( layer , key ) in enumerate ( zip ( self . layers , keys )): new_layer = layer . data_dependent_init ( t , x = x , y = y , key = key ) new_layers . append ( new_layer ) x , _ = eqx . filter_vmap ( new_layer )( t , x ) new_layers = tuple ( new_layers ) # Turn the new parameters into a new module get_layers = lambda tree : tree . layers updated_layer = eqx . tree_at ( get_layers , self , new_layers ) return updated_layer def __call__ ( self , t : Array , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" delta_logpx = 0.0 layers = reversed ( self . layers ) if inverse else self . layers for layer in layers : x , log_det = layer ( t , x , y = y , inverse = inverse , ** kwargs ) delta_logpx += log_det return x , delta_logpx def __getitem__ ( self , i : Union [ int , slice ]) -> Callable : if isinstance ( i , int ): return self . layers [ i ] elif isinstance ( i , slice ): return Sequential ( self . layers [ i ]) else : raise TypeError ( f \"Indexing with type { type ( i ) } is not supported\" ) def __iter__ ( self ): yield from self . layers def __len__ ( self ): return len ( self . layers )","title":"TimeDependentSequential"},{"location":"api/flows/base/#generax.flows.base.InjectiveTransform","text":"This represents an injective transformation. This is a special case of a bijective transformation. Atributes : input_shape : The input shape. output_shape : The output shape. cond_shape : The shape of the conditioning information. If there is no conditioning information, this is None. Source code in generax/flows/base.py class InjectiveTransform ( BijectiveTransform , ABC ): \"\"\"This represents an injective transformation. This is a special case of a bijective transformation. **Atributes**: - `input_shape`: The input shape. - `output_shape`: The output shape. - `cond_shape`: The shape of the conditioning information. If there is no conditioning information, this is None. \"\"\" output_shape : Tuple [ int ] = eqx . field ( static = True ) def __init__ ( self , * _ , input_shape : Tuple [ int ], output_shape : Tuple [ int ], cond_shape : Union [ None , Tuple [ int ]] = None , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. - `output_shape`: The output shape. - `cond_shape`: The shape of the conditioning information. If there is no \"\"\" super () . __init__ ( input_shape = input_shape , cond_shape = cond_shape , ** kwargs ) assert isinstance ( output_shape , tuple ) or isinstance ( output_shape , list ) self . output_shape = output_shape def project ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Project a point onto the image of the transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: z \"\"\" z , _ = self ( x , y = y , ** kwargs ) x_proj , _ = self ( z , y = y , inverse = True , ** kwargs ) return x_proj def log_determinant ( self , z : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Compute -0.5*log(det(J^TJ)) **Arguments**: - `z`: An element of the base space **Returns**: The log determinant of (J^TJ)^-0.5 \"\"\" def jvp ( v_flat ): v = v_flat . reshape ( z . shape ) _ , ( Jv ) = jax . jvp ( self . to_data_space , ( z ,), ( v ,)) return Jv . ravel () z_dim = util . list_prod ( z . shape ) eye = jnp . eye ( z_dim ) J = jax . vmap ( jvp , in_axes = 1 , out_axes = 1 )( eye ) return - 0.5 * jnp . linalg . slogdet ( J . T @J )[ 1 ] def log_determinant_surrogate ( z : Array , transform : eqx . Module , method : str = 'brute_force' , key : PRNGKeyArray = None , ** kwargs ) -> Array : \"\"\"Compute a term that has the same expected gradient as `-0.5*log_det(J^TJ))`. If `method='brute_force'`, then this is just -0.5*log(det(J^TJ)). If `method='iterative'`, then this is a term that has the same expected gradient. **Arguments**: - `z`: An element of the base space - `method`: How to compute the log determinant. Options are: - `brute_force`: Compute the entire Jacobian - `iterative`: Use conjugate gradient (https://arxiv.org/pdf/2106.01413.pdf) - `key`: A `jax.random.PRNGKey` for initialization. Needed for some methods **Returns**: The log determinant of J^TJ or a term that has the same gradient \"\"\" def jvp ( v_flat ): v = v_flat . reshape ( z . shape ) _ , ( Jv ) = jax . jvp ( transform , ( z ,), ( v ,)) return Jv . ravel () if method == 'brute_force' : z_dim = util . list_prod ( z . shape ) eye = jnp . eye ( z_dim ) J = jax . vmap ( jvp , in_axes = 1 , out_axes = 1 )( eye ) return - 0.5 * jnp . linalg . slogdet ( J . T @J )[ 1 ] elif method == 'iterative' : def vjp ( v_flat ): x , vjp = jax . vjp ( transform , z ) v = v_flat . reshape ( x . shape ) return vjp ( v )[ 0 ] . ravel () def vjp_jvp ( v_flat ): return vjp ( jvp ( v_flat )) v = random . normal ( key , shape = z . shape ) operator = lx . FunctionLinearOperator ( vjp_jvp , v , tags = lx . positive_semidefinite_tag ) solver = lx . CG ( rtol = 1e-3 , atol = 1e-6 ) JTJinv_v = lx . linear_solve ( operator , v , solver ) . value JTJ_v = vjp_jvp ( v ) return - 0.5 * jnp . vdot ( jax . lax . stop_gradient ( JTJinv_v ), JTJ_v )","title":"InjectiveTransform"},{"location":"api/flows/base/#generax.flows.base.InjectiveSequential","text":"A sequence of injective or bijective transformations. Source code in generax/flows/base.py class InjectiveSequential ( Sequential , InjectiveTransform ): \"\"\"A sequence of injective or bijective transformations. \"\"\" def __init__ ( self , * layers : Sequence [ BijectiveTransform ], ** kwargs ): \"\"\"**Arguments**: - `layers`: A sequence of `BijectiveTransform`. \"\"\" input_shape = layers [ 0 ] . input_shape cond_shape = layers [ 0 ] . cond_shape # Check that all of the layers have the same cond shape # and that the output shape of each layer matches the input shape of the next layer layer_iter = iter ( zip ( layers [: - 1 ], layers [ 1 :])) for l1 , l2 in layer_iter : assert l1 . cond_shape == cond_shape if isinstance ( l1 , InjectiveTransform ): assert l1 . output_shape == l2 . input_shape assert l2 . cond_shape == cond_shape if isinstance ( l2 , InjectiveTransform ): output_shape = l2 . output_shape else : output_shape = l2 . input_shape InjectiveTransform . __init__ ( self , input_shape = input_shape , output_shape = output_shape , cond_shape = cond_shape , ** kwargs ) self . layers = tuple ( layers ) self . n_layers = len ( layers )","title":"InjectiveSequential"},{"location":"api/flows/base/#generax.flows.coupling.TimeDependentWrapper","text":"","title":"TimeDependentWrapper"},{"location":"api/flows/compositions/","text":"Models \u00a4 generax.flows.models.GeneralTransform ( Repeat ) \u00a4 GeneralTransform( args, *kwargs) Source code in generax/flows/models.py class GeneralTransform ( Repeat ): def __init__ ( self , TransformType : type , input_shape : Tuple [ int ], n_flow_layers : int = 3 , working_size : int = 16 , hidden_size : int = 32 , n_blocks : int = 4 , filter_shape : Optional [ Tuple [ int ]] = ( 3 , 3 ), cond_shape : Optional [ Tuple [ int ]] = None , coupling_split_dim : Optional [ int ] = None , reverse_conditioning : Optional [ bool ] = False , create_net : Optional [ Callable [[ PRNGKeyArray ], Any ]] = None , * , key : PRNGKeyArray , ** kwargs ): def init_transform ( transform_input_shape , key ): return TransformType ( input_shape = transform_input_shape , cond_shape = cond_shape , key = key ) def _create_net ( net_input_shape , net_output_size , key ): return ResNet ( input_shape = net_input_shape , working_size = working_size , hidden_size = hidden_size , out_size = net_output_size , n_blocks = n_blocks , filter_shape = filter_shape , cond_shape = cond_shape , key = key ) create_net = create_net if create_net is not None else _create_net def make_single_flow_layer ( key : PRNGKeyArray ) -> Sequential : k1 , k2 , k3 = random . split ( key , 3 ) layers = [] layer = Coupling ( init_transform , create_net , input_shape = input_shape , cond_shape = cond_shape , split_dim = coupling_split_dim , reverse_conditioning = reverse_conditioning , key = k1 ) layers . append ( layer ) layers . append ( PLUAffine ( input_shape = input_shape , cond_shape = cond_shape , key = k2 )) layers . append ( ShiftScale ( input_shape = input_shape , cond_shape = cond_shape , key = k3 )) return Sequential ( * layers , ** kwargs ) super () . __init__ ( make_single_flow_layer , n_flow_layers , key = key ) __init__ ( self , TransformType : type , input_shape : Tuple [ int ], n_flow_layers : int = 3 , working_size : int = 16 , hidden_size : int = 32 , n_blocks : int = 4 , filter_shape : Optional [ Tuple [ int ]] = ( 3 , 3 ), cond_shape : Optional [ Tuple [ int ]] = None , coupling_split_dim : Optional [ int ] = None , reverse_conditioning : Optional [ bool ] = False , create_net : Optional [ Callable [[ PRNGKeyArray ], Any ]] = None , * , key : PRNGKeyArray , ** kwargs ) \u00a4 Source code in generax/flows/models.py def __init__ ( self , TransformType : type , input_shape : Tuple [ int ], n_flow_layers : int = 3 , working_size : int = 16 , hidden_size : int = 32 , n_blocks : int = 4 , filter_shape : Optional [ Tuple [ int ]] = ( 3 , 3 ), cond_shape : Optional [ Tuple [ int ]] = None , coupling_split_dim : Optional [ int ] = None , reverse_conditioning : Optional [ bool ] = False , create_net : Optional [ Callable [[ PRNGKeyArray ], Any ]] = None , * , key : PRNGKeyArray , ** kwargs ): def init_transform ( transform_input_shape , key ): return TransformType ( input_shape = transform_input_shape , cond_shape = cond_shape , key = key ) def _create_net ( net_input_shape , net_output_size , key ): return ResNet ( input_shape = net_input_shape , working_size = working_size , hidden_size = hidden_size , out_size = net_output_size , n_blocks = n_blocks , filter_shape = filter_shape , cond_shape = cond_shape , key = key ) create_net = create_net if create_net is not None else _create_net def make_single_flow_layer ( key : PRNGKeyArray ) -> Sequential : k1 , k2 , k3 = random . split ( key , 3 ) layers = [] layer = Coupling ( init_transform , create_net , input_shape = input_shape , cond_shape = cond_shape , split_dim = coupling_split_dim , reverse_conditioning = reverse_conditioning , key = k1 ) layers . append ( layer ) layers . append ( PLUAffine ( input_shape = input_shape , cond_shape = cond_shape , key = k2 )) layers . append ( ShiftScale ( input_shape = input_shape , cond_shape = cond_shape , key = k3 )) return Sequential ( * layers , ** kwargs ) super () . __init__ ( make_single_flow_layer , n_flow_layers , key = key ) inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Apply the inverse transformation. Arguments : x : The input to the transformation y : The conditioning information Returns : (z, log_det) Source code in generax/flows/models.py def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs ) data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform \u00a4 Inherited from generax.flows.base.Repeat.data_dependent_init . Source code in generax/flows/models.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : seq = self . to_sequential () # Apply the data dependent initalization out_seq_layers = seq . data_dependent_init ( x , y = y , key = key ) # Turn the sequential layers into a repeat layer all_params = [] for i , layer in enumerate ( out_seq_layers ): params , _ = eqx . partition ( layer , eqx . is_array ) all_params . append ( params ) # Combine the parameters back into a single layer params = jax . tree_util . tree_map ( lambda * args : jnp . array ( args ), * all_params ) _ , static = eqx . partition ( self . layers , eqx . is_array ) layers = eqx . combine ( params , static ) get_layers = lambda tree : tree . layers updated_module = eqx . tree_at ( get_layers , self , layers ) return updated_module __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Inherited from generax.flows.base.Repeat.__call__ . Source code in generax/flows/models.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" dynamic , static = eqx . partition ( self . layers , eqx . is_array ) def scan_body ( x , params ): block = eqx . combine ( params , static ) x , log_det = block ( x , y = y , inverse = inverse , ** kwargs ) return x , log_det x , log_dets = jax . lax . scan ( scan_body , x , dynamic , reverse = inverse ) return x , log_dets . sum () generax.flows.models.NICETransform ( GeneralTransform ) \u00a4 NICETransform( args, *kwargs) Source code in generax/flows/models.py class NICETransform ( GeneralTransform ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( TransformType = Shift , * args , ** kwargs ) __init__ ( self , * args , ** kwargs ) \u00a4 Source code in generax/flows/models.py def __init__ ( self , * args , ** kwargs ): super () . __init__ ( TransformType = Shift , * args , ** kwargs ) inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Apply the inverse transformation. Arguments : x : The input to the transformation y : The conditioning information Returns : (z, log_det) Source code in generax/flows/models.py def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs ) data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform \u00a4 Initialize the parameters of the layer based on the data. Arguments : x : The data to initialize the parameters with. y : The conditioning information key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/flows/models.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : seq = self . to_sequential () # Apply the data dependent initalization out_seq_layers = seq . data_dependent_init ( x , y = y , key = key ) # Turn the sequential layers into a repeat layer all_params = [] for i , layer in enumerate ( out_seq_layers ): params , _ = eqx . partition ( layer , eqx . is_array ) all_params . append ( params ) # Combine the parameters back into a single layer params = jax . tree_util . tree_map ( lambda * args : jnp . array ( args ), * all_params ) _ , static = eqx . partition ( self . layers , eqx . is_array ) layers = eqx . combine ( params , static ) get_layers = lambda tree : tree . layers updated_module = eqx . tree_at ( get_layers , self , layers ) return updated_module __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation Returns : (z, log_det) Source code in generax/flows/models.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" dynamic , static = eqx . partition ( self . layers , eqx . is_array ) def scan_body ( x , params ): block = eqx . combine ( params , static ) x , log_det = block ( x , y = y , inverse = inverse , ** kwargs ) return x , log_det x , log_dets = jax . lax . scan ( scan_body , x , dynamic , reverse = inverse ) return x , log_dets . sum () generax.flows.models.RealNVPTransform ( GeneralTransform ) \u00a4 RealNVPTransform( args, *kwargs) Source code in generax/flows/models.py class RealNVPTransform ( GeneralTransform ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( TransformType = ShiftScale , * args , ** kwargs ) __init__ ( self , * args , ** kwargs ) \u00a4 Source code in generax/flows/models.py def __init__ ( self , * args , ** kwargs ): super () . __init__ ( TransformType = ShiftScale , * args , ** kwargs ) inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Apply the inverse transformation. Arguments : x : The input to the transformation y : The conditioning information Returns : (z, log_det) Source code in generax/flows/models.py def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs ) data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform \u00a4 Initialize the parameters of the layer based on the data. Arguments : x : The data to initialize the parameters with. y : The conditioning information key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/flows/models.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : seq = self . to_sequential () # Apply the data dependent initalization out_seq_layers = seq . data_dependent_init ( x , y = y , key = key ) # Turn the sequential layers into a repeat layer all_params = [] for i , layer in enumerate ( out_seq_layers ): params , _ = eqx . partition ( layer , eqx . is_array ) all_params . append ( params ) # Combine the parameters back into a single layer params = jax . tree_util . tree_map ( lambda * args : jnp . array ( args ), * all_params ) _ , static = eqx . partition ( self . layers , eqx . is_array ) layers = eqx . combine ( params , static ) get_layers = lambda tree : tree . layers updated_module = eqx . tree_at ( get_layers , self , layers ) return updated_module __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation Returns : (z, log_det) Source code in generax/flows/models.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" dynamic , static = eqx . partition ( self . layers , eqx . is_array ) def scan_body ( x , params ): block = eqx . combine ( params , static ) x , log_det = block ( x , y = y , inverse = inverse , ** kwargs ) return x , log_det x , log_dets = jax . lax . scan ( scan_body , x , dynamic , reverse = inverse ) return x , log_dets . sum () generax.flows.models.NeuralSplineTransform ( GeneralTransform ) \u00a4 NeuralSplineTransform( args, *kwargs) Source code in generax/flows/models.py class NeuralSplineTransform ( GeneralTransform ): def __init__ ( self , * args , n_spline_knots : int = 8 , ** kwargs ): super () . __init__ ( TransformType = partial ( RationalQuadraticSpline , K = n_spline_knots ), * args , ** kwargs ) __init__ ( self , * args , * , n_spline_knots : int = 8 , ** kwargs ) \u00a4 Source code in generax/flows/models.py def __init__ ( self , * args , n_spline_knots : int = 8 , ** kwargs ): super () . __init__ ( TransformType = partial ( RationalQuadraticSpline , K = n_spline_knots ), * args , ** kwargs ) inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Apply the inverse transformation. Arguments : x : The input to the transformation y : The conditioning information Returns : (z, log_det) Source code in generax/flows/models.py def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs ) data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform \u00a4 Initialize the parameters of the layer based on the data. Arguments : x : The data to initialize the parameters with. y : The conditioning information key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/flows/models.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : seq = self . to_sequential () # Apply the data dependent initalization out_seq_layers = seq . data_dependent_init ( x , y = y , key = key ) # Turn the sequential layers into a repeat layer all_params = [] for i , layer in enumerate ( out_seq_layers ): params , _ = eqx . partition ( layer , eqx . is_array ) all_params . append ( params ) # Combine the parameters back into a single layer params = jax . tree_util . tree_map ( lambda * args : jnp . array ( args ), * all_params ) _ , static = eqx . partition ( self . layers , eqx . is_array ) layers = eqx . combine ( params , static ) get_layers = lambda tree : tree . layers updated_module = eqx . tree_at ( get_layers , self , layers ) return updated_module __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation Returns : (z, log_det) Source code in generax/flows/models.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" dynamic , static = eqx . partition ( self . layers , eqx . is_array ) def scan_body ( x , params ): block = eqx . combine ( params , static ) x , log_det = block ( x , y = y , inverse = inverse , ** kwargs ) return x , log_det x , log_dets = jax . lax . scan ( scan_body , x , dynamic , reverse = inverse ) return x , log_dets . sum () generax.flows.models.GeneralImageTransform ( Repeat ) \u00a4 GeneralImageTransform( args, *kwargs) Source code in generax/flows/models.py class GeneralImageTransform ( Repeat ): def __init__ ( self , TransformType : type , input_shape : Tuple [ int ], n_flow_layers : int = 3 , cond_shape : Optional [ Tuple [ int ]] = None , unet : Optional [ bool ] = True , coupling_split_dim : Optional [ int ] = None , reverse_conditioning : Optional [ bool ] = False , * , key : PRNGKeyArray , ** kwargs ): def init_transform ( transform_input_shape , key ): return TransformType ( input_shape = transform_input_shape , cond_shape = cond_shape , key = key ) if unet : def create_net ( net_input_shape , net_output_size , key ): H , W , C = net_input_shape return UNet ( input_shape = net_input_shape , dim = kwargs . pop ( 'dim' , 32 ), out_channels = net_output_size // ( H * W ), dim_mults = kwargs . pop ( 'dim_mults' , ( 1 , 2 , 4 )), resnet_block_groups = kwargs . pop ( 'resnet_block_groups' , 8 ), attn_heads = kwargs . pop ( 'attn_heads' , 4 ), attn_dim_head = kwargs . pop ( 'attn_dim_head' , 32 ), cond_shape = cond_shape , time_dependent = False , key = key ) else : def create_net ( net_input_shape , net_output_size , key ): return Encoder ( input_shape = net_input_shape , dim = kwargs . pop ( 'dim' , 32 ), dim_mults = kwargs . pop ( 'dim_mults' , ( 1 , 2 , 4 )), resnet_block_groups = kwargs . pop ( 'resnet_block_groups' , 8 ), attn_heads = kwargs . pop ( 'attn_heads' , 4 ), attn_dim_head = kwargs . pop ( 'attn_dim_head' , 32 ), out_size = net_output_size , cond_shape = cond_shape , key = key ) def make_single_flow_layer ( key : PRNGKeyArray ) -> Sequential : k1 , k2 , k3 = random . split ( key , 3 ) layers = [] layer = Coupling ( init_transform , create_net , input_shape = input_shape , cond_shape = cond_shape , split_dim = coupling_split_dim , reverse_conditioning = reverse_conditioning , key = k1 ) layers . append ( layer ) layers . append ( OneByOneConv ( input_shape = input_shape , key = k2 )) layers . append ( ShiftScale ( input_shape = input_shape , cond_shape = cond_shape , key = k3 )) return Sequential ( * layers , ** kwargs ) super () . __init__ ( make_single_flow_layer , n_flow_layers , key = key ) __init__ ( self , TransformType : type , input_shape : Tuple [ int ], n_flow_layers : int = 3 , cond_shape : Optional [ Tuple [ int ]] = None , unet : Optional [ bool ] = True , coupling_split_dim : Optional [ int ] = None , reverse_conditioning : Optional [ bool ] = False , * , key : PRNGKeyArray , ** kwargs ) \u00a4 Source code in generax/flows/models.py def __init__ ( self , TransformType : type , input_shape : Tuple [ int ], n_flow_layers : int = 3 , cond_shape : Optional [ Tuple [ int ]] = None , unet : Optional [ bool ] = True , coupling_split_dim : Optional [ int ] = None , reverse_conditioning : Optional [ bool ] = False , * , key : PRNGKeyArray , ** kwargs ): def init_transform ( transform_input_shape , key ): return TransformType ( input_shape = transform_input_shape , cond_shape = cond_shape , key = key ) if unet : def create_net ( net_input_shape , net_output_size , key ): H , W , C = net_input_shape return UNet ( input_shape = net_input_shape , dim = kwargs . pop ( 'dim' , 32 ), out_channels = net_output_size // ( H * W ), dim_mults = kwargs . pop ( 'dim_mults' , ( 1 , 2 , 4 )), resnet_block_groups = kwargs . pop ( 'resnet_block_groups' , 8 ), attn_heads = kwargs . pop ( 'attn_heads' , 4 ), attn_dim_head = kwargs . pop ( 'attn_dim_head' , 32 ), cond_shape = cond_shape , time_dependent = False , key = key ) else : def create_net ( net_input_shape , net_output_size , key ): return Encoder ( input_shape = net_input_shape , dim = kwargs . pop ( 'dim' , 32 ), dim_mults = kwargs . pop ( 'dim_mults' , ( 1 , 2 , 4 )), resnet_block_groups = kwargs . pop ( 'resnet_block_groups' , 8 ), attn_heads = kwargs . pop ( 'attn_heads' , 4 ), attn_dim_head = kwargs . pop ( 'attn_dim_head' , 32 ), out_size = net_output_size , cond_shape = cond_shape , key = key ) def make_single_flow_layer ( key : PRNGKeyArray ) -> Sequential : k1 , k2 , k3 = random . split ( key , 3 ) layers = [] layer = Coupling ( init_transform , create_net , input_shape = input_shape , cond_shape = cond_shape , split_dim = coupling_split_dim , reverse_conditioning = reverse_conditioning , key = k1 ) layers . append ( layer ) layers . append ( OneByOneConv ( input_shape = input_shape , key = k2 )) layers . append ( ShiftScale ( input_shape = input_shape , cond_shape = cond_shape , key = k3 )) return Sequential ( * layers , ** kwargs ) super () . __init__ ( make_single_flow_layer , n_flow_layers , key = key ) inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Apply the inverse transformation. Arguments : x : The input to the transformation y : The conditioning information Returns : (z, log_det) Source code in generax/flows/models.py def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs ) data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform \u00a4 Inherited from generax.flows.base.Repeat.data_dependent_init . Source code in generax/flows/models.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : seq = self . to_sequential () # Apply the data dependent initalization out_seq_layers = seq . data_dependent_init ( x , y = y , key = key ) # Turn the sequential layers into a repeat layer all_params = [] for i , layer in enumerate ( out_seq_layers ): params , _ = eqx . partition ( layer , eqx . is_array ) all_params . append ( params ) # Combine the parameters back into a single layer params = jax . tree_util . tree_map ( lambda * args : jnp . array ( args ), * all_params ) _ , static = eqx . partition ( self . layers , eqx . is_array ) layers = eqx . combine ( params , static ) get_layers = lambda tree : tree . layers updated_module = eqx . tree_at ( get_layers , self , layers ) return updated_module __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Inherited from generax.flows.base.Repeat.__call__ . Source code in generax/flows/models.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" dynamic , static = eqx . partition ( self . layers , eqx . is_array ) def scan_body ( x , params ): block = eqx . combine ( params , static ) x , log_det = block ( x , y = y , inverse = inverse , ** kwargs ) return x , log_det x , log_dets = jax . lax . scan ( scan_body , x , dynamic , reverse = inverse ) return x , log_dets . sum () generax.flows.models.NICEImageTransform ( GeneralImageTransform ) \u00a4 NICEImageTransform( args, *kwargs) Source code in generax/flows/models.py class NICEImageTransform ( GeneralImageTransform ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( TransformType = Shift , * args , ** kwargs ) inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Apply the inverse transformation. Arguments : x : The input to the transformation y : The conditioning information Returns : (z, log_det) Source code in generax/flows/models.py def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs ) __init__ ( self , * args , ** kwargs ) \u00a4 Source code in generax/flows/models.py def __init__ ( self , * args , ** kwargs ): super () . __init__ ( TransformType = Shift , * args , ** kwargs ) data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform \u00a4 Initialize the parameters of the layer based on the data. Arguments : x : The data to initialize the parameters with. y : The conditioning information key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/flows/models.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : seq = self . to_sequential () # Apply the data dependent initalization out_seq_layers = seq . data_dependent_init ( x , y = y , key = key ) # Turn the sequential layers into a repeat layer all_params = [] for i , layer in enumerate ( out_seq_layers ): params , _ = eqx . partition ( layer , eqx . is_array ) all_params . append ( params ) # Combine the parameters back into a single layer params = jax . tree_util . tree_map ( lambda * args : jnp . array ( args ), * all_params ) _ , static = eqx . partition ( self . layers , eqx . is_array ) layers = eqx . combine ( params , static ) get_layers = lambda tree : tree . layers updated_module = eqx . tree_at ( get_layers , self , layers ) return updated_module __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation Returns : (z, log_det) Source code in generax/flows/models.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" dynamic , static = eqx . partition ( self . layers , eqx . is_array ) def scan_body ( x , params ): block = eqx . combine ( params , static ) x , log_det = block ( x , y = y , inverse = inverse , ** kwargs ) return x , log_det x , log_dets = jax . lax . scan ( scan_body , x , dynamic , reverse = inverse ) return x , log_dets . sum () generax.flows.models.RealNVPImageTransform ( GeneralImageTransform ) \u00a4 RealNVPImageTransform( args, *kwargs) Source code in generax/flows/models.py class RealNVPImageTransform ( GeneralImageTransform ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( TransformType = ShiftScale , * args , ** kwargs ) inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Apply the inverse transformation. Arguments : x : The input to the transformation y : The conditioning information Returns : (z, log_det) Source code in generax/flows/models.py def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs ) __init__ ( self , * args , ** kwargs ) \u00a4 Source code in generax/flows/models.py def __init__ ( self , * args , ** kwargs ): super () . __init__ ( TransformType = ShiftScale , * args , ** kwargs ) data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform \u00a4 Initialize the parameters of the layer based on the data. Arguments : x : The data to initialize the parameters with. y : The conditioning information key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/flows/models.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : seq = self . to_sequential () # Apply the data dependent initalization out_seq_layers = seq . data_dependent_init ( x , y = y , key = key ) # Turn the sequential layers into a repeat layer all_params = [] for i , layer in enumerate ( out_seq_layers ): params , _ = eqx . partition ( layer , eqx . is_array ) all_params . append ( params ) # Combine the parameters back into a single layer params = jax . tree_util . tree_map ( lambda * args : jnp . array ( args ), * all_params ) _ , static = eqx . partition ( self . layers , eqx . is_array ) layers = eqx . combine ( params , static ) get_layers = lambda tree : tree . layers updated_module = eqx . tree_at ( get_layers , self , layers ) return updated_module __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation Returns : (z, log_det) Source code in generax/flows/models.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" dynamic , static = eqx . partition ( self . layers , eqx . is_array ) def scan_body ( x , params ): block = eqx . combine ( params , static ) x , log_det = block ( x , y = y , inverse = inverse , ** kwargs ) return x , log_det x , log_dets = jax . lax . scan ( scan_body , x , dynamic , reverse = inverse ) return x , log_dets . sum () generax.flows.models.NeuralSplineImageTransform ( GeneralImageTransform ) \u00a4 NeuralSplineImageTransform( args, *kwargs) Source code in generax/flows/models.py class NeuralSplineImageTransform ( GeneralImageTransform ): def __init__ ( self , * args , n_spline_knots : int = 8 , ** kwargs ): super () . __init__ ( TransformType = partial ( RationalQuadraticSpline , K = n_spline_knots ), * args , ** kwargs ) inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Apply the inverse transformation. Arguments : x : The input to the transformation y : The conditioning information Returns : (z, log_det) Source code in generax/flows/models.py def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs ) __init__ ( self , * args , * , n_spline_knots : int = 8 , ** kwargs ) \u00a4 Source code in generax/flows/models.py def __init__ ( self , * args , n_spline_knots : int = 8 , ** kwargs ): super () . __init__ ( TransformType = partial ( RationalQuadraticSpline , K = n_spline_knots ), * args , ** kwargs ) data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform \u00a4 Initialize the parameters of the layer based on the data. Arguments : x : The data to initialize the parameters with. y : The conditioning information key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/flows/models.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : seq = self . to_sequential () # Apply the data dependent initalization out_seq_layers = seq . data_dependent_init ( x , y = y , key = key ) # Turn the sequential layers into a repeat layer all_params = [] for i , layer in enumerate ( out_seq_layers ): params , _ = eqx . partition ( layer , eqx . is_array ) all_params . append ( params ) # Combine the parameters back into a single layer params = jax . tree_util . tree_map ( lambda * args : jnp . array ( args ), * all_params ) _ , static = eqx . partition ( self . layers , eqx . is_array ) layers = eqx . combine ( params , static ) get_layers = lambda tree : tree . layers updated_module = eqx . tree_at ( get_layers , self , layers ) return updated_module __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation Returns : (z, log_det) Source code in generax/flows/models.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" dynamic , static = eqx . partition ( self . layers , eqx . is_array ) def scan_body ( x , params ): block = eqx . combine ( params , static ) x , log_det = block ( x , y = y , inverse = inverse , ** kwargs ) return x , log_det x , log_dets = jax . lax . scan ( scan_body , x , dynamic , reverse = inverse ) return x , log_dets . sum ()","title":"Models"},{"location":"api/flows/compositions/#models","text":"","title":"Models"},{"location":"api/flows/compositions/#generax.flows.models.GeneralTransform","text":"GeneralTransform( args, *kwargs) Source code in generax/flows/models.py class GeneralTransform ( Repeat ): def __init__ ( self , TransformType : type , input_shape : Tuple [ int ], n_flow_layers : int = 3 , working_size : int = 16 , hidden_size : int = 32 , n_blocks : int = 4 , filter_shape : Optional [ Tuple [ int ]] = ( 3 , 3 ), cond_shape : Optional [ Tuple [ int ]] = None , coupling_split_dim : Optional [ int ] = None , reverse_conditioning : Optional [ bool ] = False , create_net : Optional [ Callable [[ PRNGKeyArray ], Any ]] = None , * , key : PRNGKeyArray , ** kwargs ): def init_transform ( transform_input_shape , key ): return TransformType ( input_shape = transform_input_shape , cond_shape = cond_shape , key = key ) def _create_net ( net_input_shape , net_output_size , key ): return ResNet ( input_shape = net_input_shape , working_size = working_size , hidden_size = hidden_size , out_size = net_output_size , n_blocks = n_blocks , filter_shape = filter_shape , cond_shape = cond_shape , key = key ) create_net = create_net if create_net is not None else _create_net def make_single_flow_layer ( key : PRNGKeyArray ) -> Sequential : k1 , k2 , k3 = random . split ( key , 3 ) layers = [] layer = Coupling ( init_transform , create_net , input_shape = input_shape , cond_shape = cond_shape , split_dim = coupling_split_dim , reverse_conditioning = reverse_conditioning , key = k1 ) layers . append ( layer ) layers . append ( PLUAffine ( input_shape = input_shape , cond_shape = cond_shape , key = k2 )) layers . append ( ShiftScale ( input_shape = input_shape , cond_shape = cond_shape , key = k3 )) return Sequential ( * layers , ** kwargs ) super () . __init__ ( make_single_flow_layer , n_flow_layers , key = key )","title":"GeneralTransform"},{"location":"api/flows/compositions/#generax.flows.models.NICETransform","text":"NICETransform( args, *kwargs) Source code in generax/flows/models.py class NICETransform ( GeneralTransform ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( TransformType = Shift , * args , ** kwargs )","title":"NICETransform"},{"location":"api/flows/compositions/#generax.flows.models.RealNVPTransform","text":"RealNVPTransform( args, *kwargs) Source code in generax/flows/models.py class RealNVPTransform ( GeneralTransform ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( TransformType = ShiftScale , * args , ** kwargs )","title":"RealNVPTransform"},{"location":"api/flows/compositions/#generax.flows.models.NeuralSplineTransform","text":"NeuralSplineTransform( args, *kwargs) Source code in generax/flows/models.py class NeuralSplineTransform ( GeneralTransform ): def __init__ ( self , * args , n_spline_knots : int = 8 , ** kwargs ): super () . __init__ ( TransformType = partial ( RationalQuadraticSpline , K = n_spline_knots ), * args , ** kwargs )","title":"NeuralSplineTransform"},{"location":"api/flows/compositions/#generax.flows.models.GeneralImageTransform","text":"GeneralImageTransform( args, *kwargs) Source code in generax/flows/models.py class GeneralImageTransform ( Repeat ): def __init__ ( self , TransformType : type , input_shape : Tuple [ int ], n_flow_layers : int = 3 , cond_shape : Optional [ Tuple [ int ]] = None , unet : Optional [ bool ] = True , coupling_split_dim : Optional [ int ] = None , reverse_conditioning : Optional [ bool ] = False , * , key : PRNGKeyArray , ** kwargs ): def init_transform ( transform_input_shape , key ): return TransformType ( input_shape = transform_input_shape , cond_shape = cond_shape , key = key ) if unet : def create_net ( net_input_shape , net_output_size , key ): H , W , C = net_input_shape return UNet ( input_shape = net_input_shape , dim = kwargs . pop ( 'dim' , 32 ), out_channels = net_output_size // ( H * W ), dim_mults = kwargs . pop ( 'dim_mults' , ( 1 , 2 , 4 )), resnet_block_groups = kwargs . pop ( 'resnet_block_groups' , 8 ), attn_heads = kwargs . pop ( 'attn_heads' , 4 ), attn_dim_head = kwargs . pop ( 'attn_dim_head' , 32 ), cond_shape = cond_shape , time_dependent = False , key = key ) else : def create_net ( net_input_shape , net_output_size , key ): return Encoder ( input_shape = net_input_shape , dim = kwargs . pop ( 'dim' , 32 ), dim_mults = kwargs . pop ( 'dim_mults' , ( 1 , 2 , 4 )), resnet_block_groups = kwargs . pop ( 'resnet_block_groups' , 8 ), attn_heads = kwargs . pop ( 'attn_heads' , 4 ), attn_dim_head = kwargs . pop ( 'attn_dim_head' , 32 ), out_size = net_output_size , cond_shape = cond_shape , key = key ) def make_single_flow_layer ( key : PRNGKeyArray ) -> Sequential : k1 , k2 , k3 = random . split ( key , 3 ) layers = [] layer = Coupling ( init_transform , create_net , input_shape = input_shape , cond_shape = cond_shape , split_dim = coupling_split_dim , reverse_conditioning = reverse_conditioning , key = k1 ) layers . append ( layer ) layers . append ( OneByOneConv ( input_shape = input_shape , key = k2 )) layers . append ( ShiftScale ( input_shape = input_shape , cond_shape = cond_shape , key = k3 )) return Sequential ( * layers , ** kwargs ) super () . __init__ ( make_single_flow_layer , n_flow_layers , key = key )","title":"GeneralImageTransform"},{"location":"api/flows/compositions/#generax.flows.models.NICEImageTransform","text":"NICEImageTransform( args, *kwargs) Source code in generax/flows/models.py class NICEImageTransform ( GeneralImageTransform ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( TransformType = Shift , * args , ** kwargs )","title":"NICEImageTransform"},{"location":"api/flows/compositions/#generax.flows.models.RealNVPImageTransform","text":"RealNVPImageTransform( args, *kwargs) Source code in generax/flows/models.py class RealNVPImageTransform ( GeneralImageTransform ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( TransformType = ShiftScale , * args , ** kwargs )","title":"RealNVPImageTransform"},{"location":"api/flows/compositions/#generax.flows.models.NeuralSplineImageTransform","text":"NeuralSplineImageTransform( args, *kwargs) Source code in generax/flows/models.py class NeuralSplineImageTransform ( GeneralImageTransform ): def __init__ ( self , * args , n_spline_knots : int = 8 , ** kwargs ): super () . __init__ ( TransformType = partial ( RationalQuadraticSpline , K = n_spline_knots ), * args , ** kwargs )","title":"NeuralSplineImageTransform"},{"location":"api/flows/conv/","text":"Convolution \u00a4 generax.flows.conv.CircularConv ( BijectiveTransform ) \u00a4 Circular convolution. Equivalent to a regular convolution with circular padding. https://papers.nips.cc/paper/2019/file/b1f62fa99de9f27a048344d55c5ef7a6-Paper.pdf Source code in generax/flows/conv.py class CircularConv ( BijectiveTransform ): \"\"\"Circular convolution. Equivalent to a regular convolution with circular padding. https://papers.nips.cc/paper/2019/file/b1f62fa99de9f27a048344d55c5ef7a6-Paper.pdf \"\"\" filter_shape : Tuple [ int ] = eqx . field ( static = True ) w : Array def __init__ ( self , input_shape : Tuple [ int ], filter_shape : Tuple [ int ] = ( 3 , 3 ), * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization - `filter_shape`: Height and width for the convolutional filter, (Kx, Ky). The full kernel will have shape (Kx, Ky, C, C) \"\"\" assert len ( filter_shape ) == 2 self . filter_shape = filter_shape super () . __init__ ( input_shape = input_shape , ** kwargs ) H , W , C = input_shape w = random . normal ( key , shape = self . filter_shape + ( C , C )) self . w = jax . vmap ( jax . vmap ( util . whiten ))( w ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\" See http://developer.download.nvidia.com/compute/cuda/2_2/sdk/website/projects/convolutionFFT2D/doc/convolutionFFT2D.pdf **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' H , W , C = x . shape Kx , Ky , _ , _ = self . w . shape # See how much we need to roll the filter W_x = ( Kx - 1 ) // 2 W_y = ( Ky - 1 ) // 2 # Pad the filter to match the fft size and roll it so that its center is at (0,0) W_padded = jnp . pad ( self . w [:: - 1 ,:: - 1 ,:,:], (( 0 , H - Kx ), ( 0 , W - Ky ), ( 0 , 0 ), ( 0 , 0 ))) W_padded = jnp . roll ( W_padded , ( - W_x , - W_y ), axis = ( 0 , 1 )) # Apply the FFT to get the convolution if inverse == False : image_fft = fft_channel_vmap ( x ) else : image_fft = fft_channel_vmap ( x ) W_fft = fft_double_channel_vmap ( W_padded ) if inverse == True : z_fft = jnp . einsum ( \"abij,abj->abi\" , W_fft , image_fft ) z = ifft_channel_vmap ( z_fft ) . real else : # For deconv, we need to invert the W over the channel dims W_fft_inv = inv_height_width_vmap ( W_fft ) x_fft = jnp . einsum ( \"abij,abj->abi\" , W_fft_inv , image_fft ) z = ifft_channel_vmap ( x_fft ) . real # The log determinant is the log det of the frequencies over the channel dims log_det = - slogdet_height_width_vmap ( W_fft ) . sum () if inverse : log_det = - log_det return z , log_det __init__ ( self , input_shape : Tuple [ int ], filter_shape : Tuple [ int ] = ( 3 , 3 ), * , key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. key : A jax.random.PRNGKey for initialization filter_shape : Height and width for the convolutional filter, (Kx, Ky). The full kernel will have shape (Kx, Ky, C, C) Source code in generax/flows/conv.py def __init__ ( self , input_shape : Tuple [ int ], filter_shape : Tuple [ int ] = ( 3 , 3 ), * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization - `filter_shape`: Height and width for the convolutional filter, (Kx, Ky). The full kernel will have shape (Kx, Ky, C, C) \"\"\" assert len ( filter_shape ) == 2 self . filter_shape = filter_shape super () . __init__ ( input_shape = input_shape , ** kwargs ) H , W , C = input_shape w = random . normal ( key , shape = self . filter_shape + ( C , C )) self . w = jax . vmap ( jax . vmap ( util . whiten ))( w ) data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) \u00a4 Inherited from generax.flows.base.BijectiveTransform.data_dependent_init . Source code in generax/flows/conv.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ): \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" return self __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 See http://developer.download.nvidia.com/compute/cuda/2_2/sdk/website/projects/convolutionFFT2D/doc/convolutionFFT2D.pdf Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation Returns : (z, log_det) Source code in generax/flows/conv.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\" See http://developer.download.nvidia.com/compute/cuda/2_2/sdk/website/projects/convolutionFFT2D/doc/convolutionFFT2D.pdf **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' H , W , C = x . shape Kx , Ky , _ , _ = self . w . shape # See how much we need to roll the filter W_x = ( Kx - 1 ) // 2 W_y = ( Ky - 1 ) // 2 # Pad the filter to match the fft size and roll it so that its center is at (0,0) W_padded = jnp . pad ( self . w [:: - 1 ,:: - 1 ,:,:], (( 0 , H - Kx ), ( 0 , W - Ky ), ( 0 , 0 ), ( 0 , 0 ))) W_padded = jnp . roll ( W_padded , ( - W_x , - W_y ), axis = ( 0 , 1 )) # Apply the FFT to get the convolution if inverse == False : image_fft = fft_channel_vmap ( x ) else : image_fft = fft_channel_vmap ( x ) W_fft = fft_double_channel_vmap ( W_padded ) if inverse == True : z_fft = jnp . einsum ( \"abij,abj->abi\" , W_fft , image_fft ) z = ifft_channel_vmap ( z_fft ) . real else : # For deconv, we need to invert the W over the channel dims W_fft_inv = inv_height_width_vmap ( W_fft ) x_fft = jnp . einsum ( \"abij,abj->abi\" , W_fft_inv , image_fft ) z = ifft_channel_vmap ( x_fft ) . real # The log determinant is the log det of the frequencies over the channel dims log_det = - slogdet_height_width_vmap ( W_fft ) . sum () if inverse : log_det = - log_det return z , log_det inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Inherited from generax.flows.base.BijectiveTransform.inverse . Source code in generax/flows/conv.py def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs ) generax.flows.conv.CaleyOrthogonalConv ( BijectiveTransform ) \u00a4 Caley parametrization of an orthogonal convolution. https://arxiv.org/pdf/2104.07167.pdf Source code in generax/flows/conv.py class CaleyOrthogonalConv ( BijectiveTransform ): \"\"\"Caley parametrization of an orthogonal convolution. https://arxiv.org/pdf/2104.07167.pdf \"\"\" filter_shape : Tuple [ int ] = eqx . field ( static = True ) v : Array g : Array def __init__ ( self , input_shape : Tuple [ int ], filter_shape : Tuple [ int ] = ( 3 , 3 ), * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization - `filter_shape`: Height and width for the convolutional filter, (Kx, Ky). The full kernel will have shape (Kx, Ky, C, C) \"\"\" assert len ( filter_shape ) == 2 self . filter_shape = filter_shape super () . __init__ ( input_shape = input_shape , ** kwargs ) H , W , C = input_shape k1 , k2 = random . split ( key , 2 ) self . v = random . normal ( k1 , shape = self . filter_shape + ( C , C )) self . g = random . normal ( k2 , shape = ()) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\" **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' H , W , C = x . shape w = self . g * self . v / jnp . linalg . norm ( self . v ) Kx , Ky , _ , _ = w . shape # See how much we need to roll the filter W_x = ( Kx - 1 ) // 2 W_y = ( Ky - 1 ) // 2 # Pad the filter to match the fft size and roll it so that its center is at (0,0) W_padded = jnp . pad ( w [:: - 1 ,:: - 1 ,:,:], (( 0 , H - Kx ), ( 0 , W - Ky ), ( 0 , 0 ), ( 0 , 0 ))) W_padded = jnp . roll ( W_padded , ( - W_x , - W_y ), axis = ( 0 , 1 )) # Apply the FFT to get the convolution if inverse == False : image_fft = fft_channel_vmap ( x ) else : image_fft = fft_channel_vmap ( x ) W_fft = fft_double_channel_vmap ( W_padded ) A_fft = W_fft - W_fft . conj () . transpose (( 0 , 1 , 3 , 2 )) I = jnp . eye ( W_fft . shape [ - 1 ]) if inverse == True : IpA_inv = inv_height_width_vmap ( I [ None , None ] + A_fft ) y_fft = jnp . einsum ( \"abij,abj->abi\" , IpA_inv , image_fft ) z_fft = y_fft - jnp . einsum ( \"abij,abj->abi\" , A_fft , y_fft ) z = ifft_channel_vmap ( z_fft ) . real else : ImA_inv = inv_height_width_vmap ( I [ None , None ] - A_fft ) y_fft = jnp . einsum ( \"abij,abj->abi\" , ImA_inv , image_fft ) z_fft = y_fft + jnp . einsum ( \"abij,abj->abi\" , A_fft , y_fft ) z = ifft_channel_vmap ( z_fft ) . real log_det = jnp . array ( 0.0 ) return z , log_det data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) \u00a4 Inherited from generax.flows.base.BijectiveTransform.data_dependent_init . Source code in generax/flows/conv.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ): \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" return self inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Inherited from generax.flows.base.BijectiveTransform.inverse . Source code in generax/flows/conv.py def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs ) __init__ ( self , input_shape : Tuple [ int ], filter_shape : Tuple [ int ] = ( 3 , 3 ), * , key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. key : A jax.random.PRNGKey for initialization filter_shape : Height and width for the convolutional filter, (Kx, Ky). The full kernel will have shape (Kx, Ky, C, C) Source code in generax/flows/conv.py def __init__ ( self , input_shape : Tuple [ int ], filter_shape : Tuple [ int ] = ( 3 , 3 ), * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization - `filter_shape`: Height and width for the convolutional filter, (Kx, Ky). The full kernel will have shape (Kx, Ky, C, C) \"\"\" assert len ( filter_shape ) == 2 self . filter_shape = filter_shape super () . __init__ ( input_shape = input_shape , ** kwargs ) H , W , C = input_shape k1 , k2 = random . split ( key , 2 ) self . v = random . normal ( k1 , shape = self . filter_shape + ( C , C )) self . g = random . normal ( k2 , shape = ()) __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation Returns : (z, log_det) Source code in generax/flows/conv.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\" **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' H , W , C = x . shape w = self . g * self . v / jnp . linalg . norm ( self . v ) Kx , Ky , _ , _ = w . shape # See how much we need to roll the filter W_x = ( Kx - 1 ) // 2 W_y = ( Ky - 1 ) // 2 # Pad the filter to match the fft size and roll it so that its center is at (0,0) W_padded = jnp . pad ( w [:: - 1 ,:: - 1 ,:,:], (( 0 , H - Kx ), ( 0 , W - Ky ), ( 0 , 0 ), ( 0 , 0 ))) W_padded = jnp . roll ( W_padded , ( - W_x , - W_y ), axis = ( 0 , 1 )) # Apply the FFT to get the convolution if inverse == False : image_fft = fft_channel_vmap ( x ) else : image_fft = fft_channel_vmap ( x ) W_fft = fft_double_channel_vmap ( W_padded ) A_fft = W_fft - W_fft . conj () . transpose (( 0 , 1 , 3 , 2 )) I = jnp . eye ( W_fft . shape [ - 1 ]) if inverse == True : IpA_inv = inv_height_width_vmap ( I [ None , None ] + A_fft ) y_fft = jnp . einsum ( \"abij,abj->abi\" , IpA_inv , image_fft ) z_fft = y_fft - jnp . einsum ( \"abij,abj->abi\" , A_fft , y_fft ) z = ifft_channel_vmap ( z_fft ) . real else : ImA_inv = inv_height_width_vmap ( I [ None , None ] - A_fft ) y_fft = jnp . einsum ( \"abij,abj->abi\" , ImA_inv , image_fft ) z_fft = y_fft + jnp . einsum ( \"abij,abj->abi\" , A_fft , y_fft ) z = ifft_channel_vmap ( z_fft ) . real log_det = jnp . array ( 0.0 ) return z , log_det generax.flows.conv.OneByOneConv ( BijectiveTransform ) \u00a4 1x1 convolution. Uses a dense parametrization because the channel dimension will probably never be that big. Costs O(C^3). Used in GLOW https://arxiv.org/pdf/1807.03039.pdf Source code in generax/flows/conv.py class OneByOneConv ( BijectiveTransform ): \"\"\" 1x1 convolution. Uses a dense parametrization because the channel dimension will probably never be that big. Costs O(C^3). Used in GLOW https://arxiv.org/pdf/1807.03039.pdf \"\"\" w : Array def __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) H , W , C = input_shape w = random . normal ( key , shape = ( C , C )) self . w = util . whiten ( w ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\" **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' H , W , C = x . shape # Using lax.conv instead of matrix multiplication over the channel dimension # is faster and also more numerically stable for some reason. # Run the flow if inverse == False : z = util . conv ( self . w [ None , None ,:,:], x ) else : w_inv = jnp . linalg . inv ( self . w ) z = util . conv ( w_inv [ None , None ,:,:], x ) log_det = jnp . linalg . slogdet ( self . w )[ 1 ] * H * W if inverse : log_det = - log_det return z , log_det data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) \u00a4 Inherited from generax.flows.base.BijectiveTransform.data_dependent_init . Source code in generax/flows/conv.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ): \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" return self inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Inherited from generax.flows.base.BijectiveTransform.inverse . Source code in generax/flows/conv.py def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs ) __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. key : A jax.random.PRNGKey for initialization Source code in generax/flows/conv.py def __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) H , W , C = input_shape w = random . normal ( key , shape = ( C , C )) self . w = util . whiten ( w ) __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation Returns : (z, log_det) Source code in generax/flows/conv.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\" **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' H , W , C = x . shape # Using lax.conv instead of matrix multiplication over the channel dimension # is faster and also more numerically stable for some reason. # Run the flow if inverse == False : z = util . conv ( self . w [ None , None ,:,:], x ) else : w_inv = jnp . linalg . inv ( self . w ) z = util . conv ( w_inv [ None , None ,:,:], x ) log_det = jnp . linalg . slogdet ( self . w )[ 1 ] * H * W if inverse : log_det = - log_det return z , log_det generax.flows.conv.HaarWavelet ( BijectiveTransform ) \u00a4 Wavelet flow https://arxiv.org/pdf/2010.13821.pdf Source code in generax/flows/conv.py class HaarWavelet ( BijectiveTransform ): \"\"\"Wavelet flow https://arxiv.org/pdf/2010.13821.pdf\"\"\" W : Array = eqx . field ( static = True ) output_shape : Tuple [ int ] = eqx . field ( static = True ) def __init__ ( self , input_shape : Tuple [ int ], ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. \"\"\" H , W , C = input_shape if H % 2 != 0 : raise ValueError ( 'Height must be even' ) if W % 2 != 0 : raise ValueError ( 'Width must be even' ) super () . __init__ ( input_shape = input_shape , ** kwargs ) self . output_shape = ( H // 2 , W // 2 , 4 * C ) # Construct the filter p , n = 0.5 , - 0.5 W = np . array ([[[ p , p ], [ p , p ]], [[ p , n ], [ p , n ]], [[ p , p ], [ n , n ]], [[ p , n ], [ n , p ]]]) W = W . transpose (( 1 , 2 , 0 )) # (H, W, O) W = W [:,:, None ,:] # (H, W, I, O). We'll be applying this channelwise self . W = W def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\" **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" def haar_conv ( x ): \"\"\"(H, W) -> (H/2, W/2, 4))\"\"\" return util . conv ( self . W , x [:,:, None ], stride = 2 ) if inverse == False : H , W , C = x . shape z = jax . vmap ( haar_conv , in_axes =- 1 , out_axes =- 1 )( x ) # Rescale the lowpass to have same mean z = z . at [:,:, 0 ] . mul ( 0.5 ) z = einops . rearrange ( z , 'H W D C -> H W (C D)' , D = 4 ) else : h = einops . rearrange ( x , 'H W (C D) -> H W C D' , D = 4 ) # Rescale h = h . at [:,:,:, 0 ] . mul ( 2.0 ) h = einops . rearrange ( h , 'H W C (M N) -> (H M) (W N) C' , M = 2 , N = 2 ) h = jax . vmap ( haar_conv , in_axes =- 1 , out_axes =- 1 )( h ) z = einops . rearrange ( h , 'H W (M N) C -> (H M) (W N) C' , M = 2 , N = 2 ) total_dim = util . list_prod ( x . shape ) log_det = jnp . log ( 0.5 ) * total_dim / 4 if inverse : log_det = - log_det return z , log_det data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) \u00a4 Inherited from generax.flows.base.BijectiveTransform.data_dependent_init . Source code in generax/flows/conv.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ): \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" return self inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Inherited from generax.flows.base.BijectiveTransform.inverse . Source code in generax/flows/conv.py def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs ) __init__ ( self , input_shape : Tuple [ int ], ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. Source code in generax/flows/conv.py def __init__ ( self , input_shape : Tuple [ int ], ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. \"\"\" H , W , C = input_shape if H % 2 != 0 : raise ValueError ( 'Height must be even' ) if W % 2 != 0 : raise ValueError ( 'Width must be even' ) super () . __init__ ( input_shape = input_shape , ** kwargs ) self . output_shape = ( H // 2 , W // 2 , 4 * C ) # Construct the filter p , n = 0.5 , - 0.5 W = np . array ([[[ p , p ], [ p , p ]], [[ p , n ], [ p , n ]], [[ p , p ], [ n , n ]], [[ p , n ], [ n , p ]]]) W = W . transpose (( 1 , 2 , 0 )) # (H, W, O) W = W [:,:, None ,:] # (H, W, I, O). We'll be applying this channelwise self . W = W __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation Returns : (z, log_det) Source code in generax/flows/conv.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\" **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" def haar_conv ( x ): \"\"\"(H, W) -> (H/2, W/2, 4))\"\"\" return util . conv ( self . W , x [:,:, None ], stride = 2 ) if inverse == False : H , W , C = x . shape z = jax . vmap ( haar_conv , in_axes =- 1 , out_axes =- 1 )( x ) # Rescale the lowpass to have same mean z = z . at [:,:, 0 ] . mul ( 0.5 ) z = einops . rearrange ( z , 'H W D C -> H W (C D)' , D = 4 ) else : h = einops . rearrange ( x , 'H W (C D) -> H W C D' , D = 4 ) # Rescale h = h . at [:,:,:, 0 ] . mul ( 2.0 ) h = einops . rearrange ( h , 'H W C (M N) -> (H M) (W N) C' , M = 2 , N = 2 ) h = jax . vmap ( haar_conv , in_axes =- 1 , out_axes =- 1 )( h ) z = einops . rearrange ( h , 'H W (M N) C -> (H M) (W N) C' , M = 2 , N = 2 ) total_dim = util . list_prod ( x . shape ) log_det = jnp . log ( 0.5 ) * total_dim / 4 if inverse : log_det = - log_det return z , log_det generax.flows.pac_flow.EmergingConv ( PACFlow ) \u00a4 Emerging convolutions https://arxiv.org/pdf/1901.11137.pdf This is a special case of PAC flows Source code in generax/flows/pac_flow.py class EmergingConv ( PACFlow ): \"\"\"Emerging convolutions https://arxiv.org/pdf/1901.11137.pdf This is a special case of PAC flows \"\"\" def __init__ ( self , input_shape : Tuple [ int ], kernel_size : int = 5 , order_type : str = \"s_curve\" , zero_init : bool = True , * , key : PRNGKeyArray , ** kwargs ): super () . __init__ ( input_shape = input_shape , feature_dim = None , kernel_size = kernel_size , order_type = order_type , pixel_adaptive = False , zero_init = zero_init , key = key , ** kwargs ) data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) \u00a4 Initialize the parameters of the layer based on the data. Arguments : x : The data to initialize the parameters with. y : The conditioning information key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/flows/pac_flow.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ): \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" return self inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Apply the inverse transformation. Arguments : x : The input to the transformation y : The conditioning information Returns : (z, log_det) Source code in generax/flows/pac_flow.py def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs ) __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Inherited from generax.flows.pac_flow.PACFlow.__call__ . Source code in generax/flows/pac_flow.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\" **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' H , W , C = x . shape # Apply the linear function z , diag_jacobian = pac_ldu_mvp ( x , self . theta , self . w , self . order , inverse = inverse , ** self . im2col_kwargs ) # Get the log det if self . theta is not None : flat_diag = diag_jacobian . reshape ( self . theta . shape [: - 3 ] + ( - 1 ,)) else : flat_diag = diag_jacobian . reshape ( x . shape [: 1 ] + ( - 1 ,)) log_det = jnp . log ( jnp . abs ( flat_diag )) . sum () if inverse : log_det = - log_det assert z . shape == self . input_shape assert log_det . shape == () return z , log_det __init__ ( self , input_shape : Tuple [ int ], kernel_size : int = 5 , order_type : str = 's_curve' , zero_init : bool = True , * , key : PRNGKeyArray , ** kwargs ) \u00a4 Source code in generax/flows/pac_flow.py def __init__ ( self , input_shape : Tuple [ int ], kernel_size : int = 5 , order_type : str = \"s_curve\" , zero_init : bool = True , * , key : PRNGKeyArray , ** kwargs ): super () . __init__ ( input_shape = input_shape , feature_dim = None , kernel_size = kernel_size , order_type = order_type , pixel_adaptive = False , zero_init = zero_init , key = key , ** kwargs ) generax.flows.pac_flow.PACFlow ( BijectiveTransform ) \u00a4 Pixel adaptive convolutions. Gets too numerically unstable to use in practice... https://eddiecunningham.github.io/pdfs/PAC_Flow.pdf Source code in generax/flows/pac_flow.py class PACFlow ( BijectiveTransform ): \"\"\"Pixel adaptive convolutions. Gets too numerically unstable to use in practice... https://eddiecunningham.github.io/pdfs/PAC_Flow.pdf \"\"\" kernel_shape : Tuple [ int ] = eqx . field ( static = True ) feature_dim : int = eqx . field ( static = True ) order_type : str = eqx . field ( static = True ) pixel_adaptive : bool = eqx . field ( static = True ) im2col_kwargs : Any = eqx . field ( static = True ) order : Array = eqx . field ( static = True ) w : Array theta : Union [ Array , None ] def __init__ ( self , input_shape : Tuple [ int ], feature_dim : int = 8 , kernel_size : int = 5 , order_type : str = \"s_curve\" , pixel_adaptive : bool = True , zero_init : bool = True , * , key : PRNGKeyArray , ** kwargs ): \"\"\" **Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `feature_dim`: The dimension of the features - `kernel_size`: Height and width for the convolutional filter, (Kx, Ky). The full kernel will have shape (Kx, Ky, C, C) - `order_type`: The order to convolve in. Either \"raster\" or \"s_curve\" - `pixel_adaptive`: Whether to use pixel adaptive convolutions - `zero_init`: Whether to initialize the weights to zero \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) assert kernel_size % 2 == 1 self . kernel_shape = ( kernel_size , kernel_size ) self . feature_dim = feature_dim self . order_type = order_type self . pixel_adaptive = pixel_adaptive H , W , C = input_shape # Extract the im2col kwargs Kx , Ky = self . kernel_shape pad = Kx // 2 self . im2col_kwargs = dict ( filter_shape = self . kernel_shape , stride = ( 1 , 1 ), padding = (( pad , Kx - pad - 1 ), ( pad , Ky - pad - 1 )), lhs_dilation = ( 1 , 1 ), rhs_dilation = ( 1 , 1 ), dimension_numbers = ( \"NHWC\" , \"HWIO\" , \"NHWC\" )) # Determine the order to convolve order_shape = H , W , 1 if self . order_type == \"raster\" : order = np . arange ( 1 , 1 + util . list_prod ( order_shape )) . reshape ( order_shape ) elif self . order_type == \"s_curve\" : order = np . arange ( 1 , 1 + util . list_prod ( order_shape )) . reshape ( order_shape ) order [:: 2 , :, :] = order [:: 2 , :, :][:, :: - 1 ] order = order * 1.0 # Turn into a float self . order = order # Initialize the weights k1 , k2 = random . split ( key , 2 ) w = random . normal ( k1 , shape = self . kernel_shape + ( C , C )) if zero_init : pad = Kx // 2 w = w . at [ pad , pad , jnp . arange ( C ), jnp . arange ( C )] . set ( 1.0 ) self . w = w if self . pixel_adaptive == True : self . theta = random . normal ( k2 , shape = ( H , W , 2 * C + self . feature_dim )) else : self . theta = None def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\" **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' H , W , C = x . shape # Apply the linear function z , diag_jacobian = pac_ldu_mvp ( x , self . theta , self . w , self . order , inverse = inverse , ** self . im2col_kwargs ) # Get the log det if self . theta is not None : flat_diag = diag_jacobian . reshape ( self . theta . shape [: - 3 ] + ( - 1 ,)) else : flat_diag = diag_jacobian . reshape ( x . shape [: 1 ] + ( - 1 ,)) log_det = jnp . log ( jnp . abs ( flat_diag )) . sum () if inverse : log_det = - log_det assert z . shape == self . input_shape assert log_det . shape == () return z , log_det data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) \u00a4 Inherited from generax.flows.base.BijectiveTransform.data_dependent_init . Source code in generax/flows/pac_flow.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ): \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" return self inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Inherited from generax.flows.base.BijectiveTransform.inverse . Source code in generax/flows/pac_flow.py def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs ) __init__ ( self , input_shape : Tuple [ int ], feature_dim : int = 8 , kernel_size : int = 5 , order_type : str = 's_curve' , pixel_adaptive : bool = True , zero_init : bool = True , * , key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. feature_dim : The dimension of the features kernel_size : Height and width for the convolutional filter, (Kx, Ky). The full kernel will have shape (Kx, Ky, C, C) order_type : The order to convolve in. Either \"raster\" or \"s_curve\" pixel_adaptive : Whether to use pixel adaptive convolutions zero_init : Whether to initialize the weights to zero Source code in generax/flows/pac_flow.py def __init__ ( self , input_shape : Tuple [ int ], feature_dim : int = 8 , kernel_size : int = 5 , order_type : str = \"s_curve\" , pixel_adaptive : bool = True , zero_init : bool = True , * , key : PRNGKeyArray , ** kwargs ): \"\"\" **Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `feature_dim`: The dimension of the features - `kernel_size`: Height and width for the convolutional filter, (Kx, Ky). The full kernel will have shape (Kx, Ky, C, C) - `order_type`: The order to convolve in. Either \"raster\" or \"s_curve\" - `pixel_adaptive`: Whether to use pixel adaptive convolutions - `zero_init`: Whether to initialize the weights to zero \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) assert kernel_size % 2 == 1 self . kernel_shape = ( kernel_size , kernel_size ) self . feature_dim = feature_dim self . order_type = order_type self . pixel_adaptive = pixel_adaptive H , W , C = input_shape # Extract the im2col kwargs Kx , Ky = self . kernel_shape pad = Kx // 2 self . im2col_kwargs = dict ( filter_shape = self . kernel_shape , stride = ( 1 , 1 ), padding = (( pad , Kx - pad - 1 ), ( pad , Ky - pad - 1 )), lhs_dilation = ( 1 , 1 ), rhs_dilation = ( 1 , 1 ), dimension_numbers = ( \"NHWC\" , \"HWIO\" , \"NHWC\" )) # Determine the order to convolve order_shape = H , W , 1 if self . order_type == \"raster\" : order = np . arange ( 1 , 1 + util . list_prod ( order_shape )) . reshape ( order_shape ) elif self . order_type == \"s_curve\" : order = np . arange ( 1 , 1 + util . list_prod ( order_shape )) . reshape ( order_shape ) order [:: 2 , :, :] = order [:: 2 , :, :][:, :: - 1 ] order = order * 1.0 # Turn into a float self . order = order # Initialize the weights k1 , k2 = random . split ( key , 2 ) w = random . normal ( k1 , shape = self . kernel_shape + ( C , C )) if zero_init : pad = Kx // 2 w = w . at [ pad , pad , jnp . arange ( C ), jnp . arange ( C )] . set ( 1.0 ) self . w = w if self . pixel_adaptive == True : self . theta = random . normal ( k2 , shape = ( H , W , 2 * C + self . feature_dim )) else : self . theta = None __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation Returns : (z, log_det) Source code in generax/flows/pac_flow.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\" **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' H , W , C = x . shape # Apply the linear function z , diag_jacobian = pac_ldu_mvp ( x , self . theta , self . w , self . order , inverse = inverse , ** self . im2col_kwargs ) # Get the log det if self . theta is not None : flat_diag = diag_jacobian . reshape ( self . theta . shape [: - 3 ] + ( - 1 ,)) else : flat_diag = diag_jacobian . reshape ( x . shape [: 1 ] + ( - 1 ,)) log_det = jnp . log ( jnp . abs ( flat_diag )) . sum () if inverse : log_det = - log_det assert z . shape == self . input_shape assert log_det . shape == () return z , log_det","title":"Convolution"},{"location":"api/flows/conv/#convolution","text":"","title":"Convolution"},{"location":"api/flows/conv/#generax.flows.conv.CircularConv","text":"Circular convolution. Equivalent to a regular convolution with circular padding. https://papers.nips.cc/paper/2019/file/b1f62fa99de9f27a048344d55c5ef7a6-Paper.pdf Source code in generax/flows/conv.py class CircularConv ( BijectiveTransform ): \"\"\"Circular convolution. Equivalent to a regular convolution with circular padding. https://papers.nips.cc/paper/2019/file/b1f62fa99de9f27a048344d55c5ef7a6-Paper.pdf \"\"\" filter_shape : Tuple [ int ] = eqx . field ( static = True ) w : Array def __init__ ( self , input_shape : Tuple [ int ], filter_shape : Tuple [ int ] = ( 3 , 3 ), * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization - `filter_shape`: Height and width for the convolutional filter, (Kx, Ky). The full kernel will have shape (Kx, Ky, C, C) \"\"\" assert len ( filter_shape ) == 2 self . filter_shape = filter_shape super () . __init__ ( input_shape = input_shape , ** kwargs ) H , W , C = input_shape w = random . normal ( key , shape = self . filter_shape + ( C , C )) self . w = jax . vmap ( jax . vmap ( util . whiten ))( w ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\" See http://developer.download.nvidia.com/compute/cuda/2_2/sdk/website/projects/convolutionFFT2D/doc/convolutionFFT2D.pdf **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' H , W , C = x . shape Kx , Ky , _ , _ = self . w . shape # See how much we need to roll the filter W_x = ( Kx - 1 ) // 2 W_y = ( Ky - 1 ) // 2 # Pad the filter to match the fft size and roll it so that its center is at (0,0) W_padded = jnp . pad ( self . w [:: - 1 ,:: - 1 ,:,:], (( 0 , H - Kx ), ( 0 , W - Ky ), ( 0 , 0 ), ( 0 , 0 ))) W_padded = jnp . roll ( W_padded , ( - W_x , - W_y ), axis = ( 0 , 1 )) # Apply the FFT to get the convolution if inverse == False : image_fft = fft_channel_vmap ( x ) else : image_fft = fft_channel_vmap ( x ) W_fft = fft_double_channel_vmap ( W_padded ) if inverse == True : z_fft = jnp . einsum ( \"abij,abj->abi\" , W_fft , image_fft ) z = ifft_channel_vmap ( z_fft ) . real else : # For deconv, we need to invert the W over the channel dims W_fft_inv = inv_height_width_vmap ( W_fft ) x_fft = jnp . einsum ( \"abij,abj->abi\" , W_fft_inv , image_fft ) z = ifft_channel_vmap ( x_fft ) . real # The log determinant is the log det of the frequencies over the channel dims log_det = - slogdet_height_width_vmap ( W_fft ) . sum () if inverse : log_det = - log_det return z , log_det","title":"CircularConv"},{"location":"api/flows/conv/#generax.flows.conv.CaleyOrthogonalConv","text":"Caley parametrization of an orthogonal convolution. https://arxiv.org/pdf/2104.07167.pdf Source code in generax/flows/conv.py class CaleyOrthogonalConv ( BijectiveTransform ): \"\"\"Caley parametrization of an orthogonal convolution. https://arxiv.org/pdf/2104.07167.pdf \"\"\" filter_shape : Tuple [ int ] = eqx . field ( static = True ) v : Array g : Array def __init__ ( self , input_shape : Tuple [ int ], filter_shape : Tuple [ int ] = ( 3 , 3 ), * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization - `filter_shape`: Height and width for the convolutional filter, (Kx, Ky). The full kernel will have shape (Kx, Ky, C, C) \"\"\" assert len ( filter_shape ) == 2 self . filter_shape = filter_shape super () . __init__ ( input_shape = input_shape , ** kwargs ) H , W , C = input_shape k1 , k2 = random . split ( key , 2 ) self . v = random . normal ( k1 , shape = self . filter_shape + ( C , C )) self . g = random . normal ( k2 , shape = ()) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\" **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' H , W , C = x . shape w = self . g * self . v / jnp . linalg . norm ( self . v ) Kx , Ky , _ , _ = w . shape # See how much we need to roll the filter W_x = ( Kx - 1 ) // 2 W_y = ( Ky - 1 ) // 2 # Pad the filter to match the fft size and roll it so that its center is at (0,0) W_padded = jnp . pad ( w [:: - 1 ,:: - 1 ,:,:], (( 0 , H - Kx ), ( 0 , W - Ky ), ( 0 , 0 ), ( 0 , 0 ))) W_padded = jnp . roll ( W_padded , ( - W_x , - W_y ), axis = ( 0 , 1 )) # Apply the FFT to get the convolution if inverse == False : image_fft = fft_channel_vmap ( x ) else : image_fft = fft_channel_vmap ( x ) W_fft = fft_double_channel_vmap ( W_padded ) A_fft = W_fft - W_fft . conj () . transpose (( 0 , 1 , 3 , 2 )) I = jnp . eye ( W_fft . shape [ - 1 ]) if inverse == True : IpA_inv = inv_height_width_vmap ( I [ None , None ] + A_fft ) y_fft = jnp . einsum ( \"abij,abj->abi\" , IpA_inv , image_fft ) z_fft = y_fft - jnp . einsum ( \"abij,abj->abi\" , A_fft , y_fft ) z = ifft_channel_vmap ( z_fft ) . real else : ImA_inv = inv_height_width_vmap ( I [ None , None ] - A_fft ) y_fft = jnp . einsum ( \"abij,abj->abi\" , ImA_inv , image_fft ) z_fft = y_fft + jnp . einsum ( \"abij,abj->abi\" , A_fft , y_fft ) z = ifft_channel_vmap ( z_fft ) . real log_det = jnp . array ( 0.0 ) return z , log_det","title":"CaleyOrthogonalConv"},{"location":"api/flows/conv/#generax.flows.conv.OneByOneConv","text":"1x1 convolution. Uses a dense parametrization because the channel dimension will probably never be that big. Costs O(C^3). Used in GLOW https://arxiv.org/pdf/1807.03039.pdf Source code in generax/flows/conv.py class OneByOneConv ( BijectiveTransform ): \"\"\" 1x1 convolution. Uses a dense parametrization because the channel dimension will probably never be that big. Costs O(C^3). Used in GLOW https://arxiv.org/pdf/1807.03039.pdf \"\"\" w : Array def __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) H , W , C = input_shape w = random . normal ( key , shape = ( C , C )) self . w = util . whiten ( w ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\" **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' H , W , C = x . shape # Using lax.conv instead of matrix multiplication over the channel dimension # is faster and also more numerically stable for some reason. # Run the flow if inverse == False : z = util . conv ( self . w [ None , None ,:,:], x ) else : w_inv = jnp . linalg . inv ( self . w ) z = util . conv ( w_inv [ None , None ,:,:], x ) log_det = jnp . linalg . slogdet ( self . w )[ 1 ] * H * W if inverse : log_det = - log_det return z , log_det","title":"OneByOneConv"},{"location":"api/flows/conv/#generax.flows.conv.HaarWavelet","text":"Wavelet flow https://arxiv.org/pdf/2010.13821.pdf Source code in generax/flows/conv.py class HaarWavelet ( BijectiveTransform ): \"\"\"Wavelet flow https://arxiv.org/pdf/2010.13821.pdf\"\"\" W : Array = eqx . field ( static = True ) output_shape : Tuple [ int ] = eqx . field ( static = True ) def __init__ ( self , input_shape : Tuple [ int ], ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. \"\"\" H , W , C = input_shape if H % 2 != 0 : raise ValueError ( 'Height must be even' ) if W % 2 != 0 : raise ValueError ( 'Width must be even' ) super () . __init__ ( input_shape = input_shape , ** kwargs ) self . output_shape = ( H // 2 , W // 2 , 4 * C ) # Construct the filter p , n = 0.5 , - 0.5 W = np . array ([[[ p , p ], [ p , p ]], [[ p , n ], [ p , n ]], [[ p , p ], [ n , n ]], [[ p , n ], [ n , p ]]]) W = W . transpose (( 1 , 2 , 0 )) # (H, W, O) W = W [:,:, None ,:] # (H, W, I, O). We'll be applying this channelwise self . W = W def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\" **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" def haar_conv ( x ): \"\"\"(H, W) -> (H/2, W/2, 4))\"\"\" return util . conv ( self . W , x [:,:, None ], stride = 2 ) if inverse == False : H , W , C = x . shape z = jax . vmap ( haar_conv , in_axes =- 1 , out_axes =- 1 )( x ) # Rescale the lowpass to have same mean z = z . at [:,:, 0 ] . mul ( 0.5 ) z = einops . rearrange ( z , 'H W D C -> H W (C D)' , D = 4 ) else : h = einops . rearrange ( x , 'H W (C D) -> H W C D' , D = 4 ) # Rescale h = h . at [:,:,:, 0 ] . mul ( 2.0 ) h = einops . rearrange ( h , 'H W C (M N) -> (H M) (W N) C' , M = 2 , N = 2 ) h = jax . vmap ( haar_conv , in_axes =- 1 , out_axes =- 1 )( h ) z = einops . rearrange ( h , 'H W (M N) C -> (H M) (W N) C' , M = 2 , N = 2 ) total_dim = util . list_prod ( x . shape ) log_det = jnp . log ( 0.5 ) * total_dim / 4 if inverse : log_det = - log_det return z , log_det","title":"HaarWavelet"},{"location":"api/flows/conv/#generax.flows.pac_flow.EmergingConv","text":"Emerging convolutions https://arxiv.org/pdf/1901.11137.pdf This is a special case of PAC flows Source code in generax/flows/pac_flow.py class EmergingConv ( PACFlow ): \"\"\"Emerging convolutions https://arxiv.org/pdf/1901.11137.pdf This is a special case of PAC flows \"\"\" def __init__ ( self , input_shape : Tuple [ int ], kernel_size : int = 5 , order_type : str = \"s_curve\" , zero_init : bool = True , * , key : PRNGKeyArray , ** kwargs ): super () . __init__ ( input_shape = input_shape , feature_dim = None , kernel_size = kernel_size , order_type = order_type , pixel_adaptive = False , zero_init = zero_init , key = key , ** kwargs )","title":"EmergingConv"},{"location":"api/flows/conv/#generax.flows.pac_flow.PACFlow","text":"Pixel adaptive convolutions. Gets too numerically unstable to use in practice... https://eddiecunningham.github.io/pdfs/PAC_Flow.pdf Source code in generax/flows/pac_flow.py class PACFlow ( BijectiveTransform ): \"\"\"Pixel adaptive convolutions. Gets too numerically unstable to use in practice... https://eddiecunningham.github.io/pdfs/PAC_Flow.pdf \"\"\" kernel_shape : Tuple [ int ] = eqx . field ( static = True ) feature_dim : int = eqx . field ( static = True ) order_type : str = eqx . field ( static = True ) pixel_adaptive : bool = eqx . field ( static = True ) im2col_kwargs : Any = eqx . field ( static = True ) order : Array = eqx . field ( static = True ) w : Array theta : Union [ Array , None ] def __init__ ( self , input_shape : Tuple [ int ], feature_dim : int = 8 , kernel_size : int = 5 , order_type : str = \"s_curve\" , pixel_adaptive : bool = True , zero_init : bool = True , * , key : PRNGKeyArray , ** kwargs ): \"\"\" **Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `feature_dim`: The dimension of the features - `kernel_size`: Height and width for the convolutional filter, (Kx, Ky). The full kernel will have shape (Kx, Ky, C, C) - `order_type`: The order to convolve in. Either \"raster\" or \"s_curve\" - `pixel_adaptive`: Whether to use pixel adaptive convolutions - `zero_init`: Whether to initialize the weights to zero \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) assert kernel_size % 2 == 1 self . kernel_shape = ( kernel_size , kernel_size ) self . feature_dim = feature_dim self . order_type = order_type self . pixel_adaptive = pixel_adaptive H , W , C = input_shape # Extract the im2col kwargs Kx , Ky = self . kernel_shape pad = Kx // 2 self . im2col_kwargs = dict ( filter_shape = self . kernel_shape , stride = ( 1 , 1 ), padding = (( pad , Kx - pad - 1 ), ( pad , Ky - pad - 1 )), lhs_dilation = ( 1 , 1 ), rhs_dilation = ( 1 , 1 ), dimension_numbers = ( \"NHWC\" , \"HWIO\" , \"NHWC\" )) # Determine the order to convolve order_shape = H , W , 1 if self . order_type == \"raster\" : order = np . arange ( 1 , 1 + util . list_prod ( order_shape )) . reshape ( order_shape ) elif self . order_type == \"s_curve\" : order = np . arange ( 1 , 1 + util . list_prod ( order_shape )) . reshape ( order_shape ) order [:: 2 , :, :] = order [:: 2 , :, :][:, :: - 1 ] order = order * 1.0 # Turn into a float self . order = order # Initialize the weights k1 , k2 = random . split ( key , 2 ) w = random . normal ( k1 , shape = self . kernel_shape + ( C , C )) if zero_init : pad = Kx // 2 w = w . at [ pad , pad , jnp . arange ( C ), jnp . arange ( C )] . set ( 1.0 ) self . w = w if self . pixel_adaptive == True : self . theta = random . normal ( k2 , shape = ( H , W , 2 * C + self . feature_dim )) else : self . theta = None def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\" **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' H , W , C = x . shape # Apply the linear function z , diag_jacobian = pac_ldu_mvp ( x , self . theta , self . w , self . order , inverse = inverse , ** self . im2col_kwargs ) # Get the log det if self . theta is not None : flat_diag = diag_jacobian . reshape ( self . theta . shape [: - 3 ] + ( - 1 ,)) else : flat_diag = diag_jacobian . reshape ( x . shape [: 1 ] + ( - 1 ,)) log_det = jnp . log ( jnp . abs ( flat_diag )) . sum () if inverse : log_det = - log_det assert z . shape == self . input_shape assert log_det . shape == () return z , log_det","title":"PACFlow"},{"location":"api/flows/coupling/","text":"Coupling \u00a4 generax.flows.coupling.RavelParameters \u00a4 Flatten and concatenate the parameters of a eqx.Module Source code in generax/flows/coupling.py class RavelParameters ( eqx . Module ): \"\"\"Flatten and concatenate the parameters of a eqx.Module \"\"\" shapes_and_sizes : Sequence [ Tuple [ Tuple [ int ], int ]] = eqx . field ( static = True ) flat_params_size : Tuple [ int ] = eqx . field ( static = True ) static : Any = eqx . field ( static = True ) treedef : Any = eqx . field ( static = True ) indices : np . ndarray = eqx . field ( static = True ) def __init__ ( self , module ): # Split the parameters into dynamic and static params , self . static = eqx . partition ( module , eqx . is_array ) # Flatten the parameters so that we can extract its sizes leaves , self . treedef = jax . tree_util . tree_flatten ( params ) # Get the shape and size of each leaf self . shapes_and_sizes = [( leaf . shape , leaf . size ) for leaf in leaves ] # Flatten the parameters flat_params = jnp . concatenate ([ leaf . ravel () for leaf in leaves ]) # Keep track of the size of the flattened parameters self . flat_params_size = flat_params . size # Keep track of the split points for each paramter in the flattened array self . indices = np . cumsum ( np . array ([ 0 ] + [ size for _ , size in self . shapes_and_sizes ])) def flatten_params ( self , module : eqx . Module ) -> Array : # Split the parameters into dynamic and static params , _ = eqx . partition ( module , eqx . is_array ) # Flatten the parameters so that we can extract its sizes leaves , _ = jax . tree_util . tree_flatten ( params ) # Flatten the parameters flat_params = jnp . concatenate ([ leaf . ravel () for leaf in leaves ]) return flat_params def __call__ ( self , flat_params : Array ) -> eqx . Module : flat_params = flat_params . ravel () # Flatten the parameters completely leaves = [] for i , ( shape , size ) in enumerate ( self . shapes_and_sizes ): # Extract each leaf from the flattened parameters and reshape it buffer = flat_params [ self . indices [ i ]: self . indices [ i + 1 ]] if buffer . size != misc . list_prod ( shape ): raise ValueError ( f 'Expected total size of { misc . list_prod ( shape ) } but got { buffer . size } ' ) leaf = buffer . reshape ( shape ) leaves . append ( leaf ) # Turn the leaves back into a tree params = jax . tree_util . tree_unflatten ( self . treedef , leaves ) return eqx . combine ( params , self . static ) __init__ ( self , module ) \u00a4 Source code in generax/flows/coupling.py def __init__ ( self , module ): # Split the parameters into dynamic and static params , self . static = eqx . partition ( module , eqx . is_array ) # Flatten the parameters so that we can extract its sizes leaves , self . treedef = jax . tree_util . tree_flatten ( params ) # Get the shape and size of each leaf self . shapes_and_sizes = [( leaf . shape , leaf . size ) for leaf in leaves ] # Flatten the parameters flat_params = jnp . concatenate ([ leaf . ravel () for leaf in leaves ]) # Keep track of the size of the flattened parameters self . flat_params_size = flat_params . size # Keep track of the split points for each paramter in the flattened array self . indices = np . cumsum ( np . array ([ 0 ] + [ size for _ , size in self . shapes_and_sizes ])) __call__ ( self , flat_params : Array ) -> Module \u00a4 Call self as a function. Source code in generax/flows/coupling.py def __call__ ( self , flat_params : Array ) -> eqx . Module : flat_params = flat_params . ravel () # Flatten the parameters completely leaves = [] for i , ( shape , size ) in enumerate ( self . shapes_and_sizes ): # Extract each leaf from the flattened parameters and reshape it buffer = flat_params [ self . indices [ i ]: self . indices [ i + 1 ]] if buffer . size != misc . list_prod ( shape ): raise ValueError ( f 'Expected total size of { misc . list_prod ( shape ) } but got { buffer . size } ' ) leaf = buffer . reshape ( shape ) leaves . append ( leaf ) # Turn the leaves back into a tree params = jax . tree_util . tree_unflatten ( self . treedef , leaves ) return eqx . combine ( params , self . static ) generax.flows.coupling.Coupling ( BijectiveTransform ) \u00a4 Parametrize a flow over half of the inputs using the other half. The conditioning network will be fixed # Example of intended usage: def initialize_scale ( transform_input_shape , key ): return ShiftScale ( input_shape = transform_input_shape , key = key , ** kwargs ) def initialize_network ( net_input_shape , net_output_size , key ): return ResNet ( input_shape = net_input_shape , out_size = net_output_size , key = key , ** kwargs ) layer = Coupling ( transform_init = initialize_scale , net_init = initialize_network , input_shape = input_shape , cond_shape = cond_shape , key = key , reverse_conditioning = True , split_dim = 1 ) z , log_det = layer ( x , y ) Attributes : - params_to_transform : A module that turns an array of parameters into an eqx.Module. - scale : A scalar that we'll use to start with small parameter values - net : The neural network to use. Source code in generax/flows/coupling.py class Coupling ( BijectiveTransform ): \"\"\"Parametrize a flow over half of the inputs using the other half. The conditioning network will be fixed ```python # Example of intended usage: def initialize_scale(transform_input_shape, key): return ShiftScale(input_shape=transform_input_shape, key=key, **kwargs) def initialize_network(net_input_shape, net_output_size, key): return ResNet(input_shape=net_input_shape, out_size=net_output_size, key=key, **kwargs) layer = Coupling(transform_init=initialize_scale, net_init=initialize_network, input_shape=input_shape, cond_shape=cond_shape, key=key, reverse_conditioning=True, split_dim=1) z, log_det = layer(x, y) ``` **Attributes**: - `params_to_transform`: A module that turns an array of parameters into an eqx.Module. - `scale`: A scalar that we'll use to start with small parameter values - `net`: The neural network to use. \"\"\" net : eqx . Module scale : Array params_to_transform : RavelParameters split_dim : Optional [ int ] = eqx . field ( static = True ) reverse_conditioning : bool = eqx . field ( static = True ) def __init__ ( self , transform_init : Callable [[ Tuple [ int ]], BijectiveTransform ], net_init : Callable [[ Tuple [ int ], int ], eqx . Module ], input_shape : Tuple [ int ], cond_shape : Optional [ Tuple [ int ]] = None , split_dim : Optional [ int ] = None , reverse_conditioning : Optional [ bool ] = False , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `transform`: The bijective transformation to use. - `net`: The neural network to generate the transform parameters. - `input_shape`: The shape of the input - `cond_shape`: The shape of the conditioning information - `split_dim`: The number of dimension to split the last axis on. If `None`, defaults to `dim//2`. - `reverse_conditioning`: If `True`, condition on the first part of the input instead of the second part. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , cond_shape = cond_shape , ** kwargs ) k1 , k2 = random . split ( key , 2 ) self . split_dim = split_dim if split_dim is not None else input_shape [ - 1 ] // 2 self . reverse_conditioning = reverse_conditioning # Get the shapes of the input to the transform and the network transform_input_shape , net_input_shape = self . get_split_shapes ( input_shape ) transform = transform_init ( transform_input_shape , key = k1 ) net_output_size = self . get_net_output_shapes ( input_shape , transform ) net = net_init ( net_input_shape , net_output_size , key = k2 ) self . net = net # Use this to turn an eqx module into an array and vice-versa self . params_to_transform = RavelParameters ( transform ) # Also initialize the parameters to be close to 0 self . scale = random . normal ( key , ( 1 ,)) * 0.01 def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' x1 , x2 = self . split ( x ) net = self . net . data_dependent_init ( x2 , y = y , key = key ) # Turn the new parameters into a new module get_net = lambda tree : tree . net updated_layer = eqx . tree_at ( get_net , self , net ) return updated_layer def split ( self , x : Array ) -> Tuple [ Array , Array ]: \"\"\"Split the input into two halves.\"\"\" x1 , x2 = x [ ... , : self . split_dim ], x [ ... , self . split_dim :] if self . reverse_conditioning : return x2 , x1 return x1 , x2 def combine ( self , x1 : Array , x2 : Array ) -> Array : \"\"\"Combine the two halves of the input.\"\"\" if self . reverse_conditioning : return jnp . concatenate ([ x2 , x1 ], axis =- 1 ) return jnp . concatenate ([ x1 , x2 ], axis =- 1 ) def get_split_shapes ( self , input_shape : Tuple [ int ]) -> Tuple [ Tuple [ int ]]: x1_dim , x2_dim = self . split_dim , input_shape [ - 1 ] - self . split_dim x1_shape = input_shape [: - 1 ] + ( x1_dim ,) x2_shape = input_shape [: - 1 ] + ( x2_dim ,) if self . reverse_conditioning : return x2_shape , x1_shape return x1_shape , x2_shape def get_net_output_shapes ( self , input_shape : Tuple [ int ], transform : BijectiveTransform ) -> Tuple [ Tuple [ int ], int ]: \"\"\" **Arguments**: - `input_shape`: The shape of the input - `transform`: The bijective transformation to use. **Returns**: - `net_output_size`: The size of the output of the neural network. This is a single integer because the network is expected to produce a single vector. \"\"\" x1_shape , x2_shape = self . get_split_shapes ( input_shape ) if x1_shape != transform . input_shape : raise ValueError ( f 'The transform { transform } needs to have an input shape equal to { x1_shape } . Use `get_input_shapes` to get this shape.' ) params_to_transform = RavelParameters ( transform ) net_output_size = params_to_transform . flat_params_size return net_output_size def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' # Split the input into two halves x1 , x2 = self . split ( x ) params = self . net ( x2 , y = y , ** kwargs ) params *= self . scale assert params . size == self . params_to_transform . flat_params_size # Apply the transformation to x1 given x2 transform = self . params_to_transform ( params ) z1 , log_det = transform ( x1 , y = y , inverse = inverse , ** kwargs ) z = self . combine ( z1 , x2 ) return z , log_det __init__ ( self , transform_init : Callable [[ Tuple [ int ]], BijectiveTransform ], net_init : Callable [[ Tuple [ int ], int ], equinox . _module . Module ], input_shape : Tuple [ int ], cond_shape : Optional [ Tuple [ int ]] = None , split_dim : Optional [ int ] = None , reverse_conditioning : Optional [ bool ] = False , * , key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : transform : The bijective transformation to use. net : The neural network to generate the transform parameters. input_shape : The shape of the input cond_shape : The shape of the conditioning information split_dim : The number of dimension to split the last axis on. If None , defaults to dim//2 . reverse_conditioning : If True , condition on the first part of the input instead of the second part. key : A jax.random.PRNGKey for initialization Source code in generax/flows/coupling.py def __init__ ( self , transform_init : Callable [[ Tuple [ int ]], BijectiveTransform ], net_init : Callable [[ Tuple [ int ], int ], eqx . Module ], input_shape : Tuple [ int ], cond_shape : Optional [ Tuple [ int ]] = None , split_dim : Optional [ int ] = None , reverse_conditioning : Optional [ bool ] = False , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `transform`: The bijective transformation to use. - `net`: The neural network to generate the transform parameters. - `input_shape`: The shape of the input - `cond_shape`: The shape of the conditioning information - `split_dim`: The number of dimension to split the last axis on. If `None`, defaults to `dim//2`. - `reverse_conditioning`: If `True`, condition on the first part of the input instead of the second part. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , cond_shape = cond_shape , ** kwargs ) k1 , k2 = random . split ( key , 2 ) self . split_dim = split_dim if split_dim is not None else input_shape [ - 1 ] // 2 self . reverse_conditioning = reverse_conditioning # Get the shapes of the input to the transform and the network transform_input_shape , net_input_shape = self . get_split_shapes ( input_shape ) transform = transform_init ( transform_input_shape , key = k1 ) net_output_size = self . get_net_output_shapes ( input_shape , transform ) net = net_init ( net_input_shape , net_output_size , key = k2 ) self . net = net # Use this to turn an eqx module into an array and vice-versa self . params_to_transform = RavelParameters ( transform ) # Also initialize the parameters to be close to 0 self . scale = random . normal ( key , ( 1 ,)) * 0.01 inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Inherited from generax.flows.base.BijectiveTransform.inverse . Source code in generax/flows/coupling.py def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs ) data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform \u00a4 Initialize the parameters of the layer based on the data. Arguments : x : The data to initialize the parameters with. y : The conditioning information key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/flows/coupling.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' x1 , x2 = self . split ( x ) net = self . net . data_dependent_init ( x2 , y = y , key = key ) # Turn the new parameters into a new module get_net = lambda tree : tree . net updated_layer = eqx . tree_at ( get_net , self , net ) return updated_layer split ( self , x : Array ) -> Tuple [ Array , Array ] \u00a4 Split the input into two halves. Source code in generax/flows/coupling.py def split ( self , x : Array ) -> Tuple [ Array , Array ]: \"\"\"Split the input into two halves.\"\"\" x1 , x2 = x [ ... , : self . split_dim ], x [ ... , self . split_dim :] if self . reverse_conditioning : return x2 , x1 return x1 , x2 get_split_shapes ( self , input_shape : Tuple [ int ]) -> Tuple [ Tuple [ int ]] \u00a4 Source code in generax/flows/coupling.py def get_split_shapes ( self , input_shape : Tuple [ int ]) -> Tuple [ Tuple [ int ]]: x1_dim , x2_dim = self . split_dim , input_shape [ - 1 ] - self . split_dim x1_shape = input_shape [: - 1 ] + ( x1_dim ,) x2_shape = input_shape [: - 1 ] + ( x2_dim ,) if self . reverse_conditioning : return x2_shape , x1_shape return x1_shape , x2_shape get_net_output_shapes ( self , input_shape : Tuple [ int ], transform : BijectiveTransform ) -> Tuple [ Tuple [ int ], int ] \u00a4 Arguments : - input_shape : The shape of the input - transform : The bijective transformation to use. Returns : - net_output_size : The size of the output of the neural network. This is a single integer because the network is expected to produce a single vector. Source code in generax/flows/coupling.py def get_net_output_shapes ( self , input_shape : Tuple [ int ], transform : BijectiveTransform ) -> Tuple [ Tuple [ int ], int ]: \"\"\" **Arguments**: - `input_shape`: The shape of the input - `transform`: The bijective transformation to use. **Returns**: - `net_output_size`: The size of the output of the neural network. This is a single integer because the network is expected to produce a single vector. \"\"\" x1_shape , x2_shape = self . get_split_shapes ( input_shape ) if x1_shape != transform . input_shape : raise ValueError ( f 'The transform { transform } needs to have an input shape equal to { x1_shape } . Use `get_input_shapes` to get this shape.' ) params_to_transform = RavelParameters ( transform ) net_output_size = params_to_transform . flat_params_size return net_output_size __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Implements generax.flows.base.BijectiveTransform.__call__ . Source code in generax/flows/coupling.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' # Split the input into two halves x1 , x2 = self . split ( x ) params = self . net ( x2 , y = y , ** kwargs ) params *= self . scale assert params . size == self . params_to_transform . flat_params_size # Apply the transformation to x1 given x2 transform = self . params_to_transform ( params ) z1 , log_det = transform ( x1 , y = y , inverse = inverse , ** kwargs ) z = self . combine ( z1 , x2 ) return z , log_det generax.flows.coupling.TimeDependentCoupling ( Coupling , TimeDependentBijectiveTransform ) \u00a4 Time dependent coupling transform. At t=0, this will pass parameters of 0s to the transform. ``` Attributes : - params_to_transform : A module that turns an array of parameters into an eqx.Module. - scale : A scalar that we'll use to start with small parameter values - net : The neural network to use. Source code in generax/flows/coupling.py class TimeDependentCoupling ( Coupling , TimeDependentBijectiveTransform ): \"\"\"Time dependent coupling transform. At t=0, this will pass parameters of 0s to the transform. ``` **Attributes**: - `params_to_transform`: A module that turns an array of parameters into an eqx.Module. - `scale`: A scalar that we'll use to start with small parameter values - `net`: The neural network to use. \"\"\" def data_dependent_init ( self , t : Array , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `t`: The time to initialize the parameters with. - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' x1 , x2 = self . split ( x ) net = self . net . data_dependent_init ( t , x2 , y = y , key = key ) # Turn the new parameters into a new module def get_net ( tree ): return tree . net updated_layer = eqx . tree_at ( get_net , self , net ) return updated_layer def __call__ ( self , t : Array , xt : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `xt`: The input to the transformation. If inverse=True, then should be x0 - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (x0, log_det) \"\"\" assert xt . shape == self . input_shape , 'Only works on unbatched data' # Split the input into two halves x1 , x2 = self . split ( xt ) params = self . net ( t , x2 , y = y , ** kwargs ) params *= self . scale * t assert params . size == self . params_to_transform . flat_params_size # Apply the transformation to x1 given x2 transform = self . params_to_transform ( params ) z1 , log_det = transform ( x1 , y = y , inverse = inverse , ** kwargs ) z = self . combine ( z1 , x2 ) return z , log_det __init__ ( self , transform_init : Callable [[ Tuple [ int ]], BijectiveTransform ], net_init : Callable [[ Tuple [ int ], int ], equinox . _module . Module ], input_shape : Tuple [ int ], cond_shape : Optional [ Tuple [ int ]] = None , split_dim : Optional [ int ] = None , reverse_conditioning : Optional [ bool ] = False , * , key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : transform : The bijective transformation to use. net : The neural network to generate the transform parameters. input_shape : The shape of the input cond_shape : The shape of the conditioning information split_dim : The number of dimension to split the last axis on. If None , defaults to dim//2 . reverse_conditioning : If True , condition on the first part of the input instead of the second part. key : A jax.random.PRNGKey for initialization Source code in generax/flows/coupling.py def __init__ ( self , transform_init : Callable [[ Tuple [ int ]], BijectiveTransform ], net_init : Callable [[ Tuple [ int ], int ], eqx . Module ], input_shape : Tuple [ int ], cond_shape : Optional [ Tuple [ int ]] = None , split_dim : Optional [ int ] = None , reverse_conditioning : Optional [ bool ] = False , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `transform`: The bijective transformation to use. - `net`: The neural network to generate the transform parameters. - `input_shape`: The shape of the input - `cond_shape`: The shape of the conditioning information - `split_dim`: The number of dimension to split the last axis on. If `None`, defaults to `dim//2`. - `reverse_conditioning`: If `True`, condition on the first part of the input instead of the second part. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , cond_shape = cond_shape , ** kwargs ) k1 , k2 = random . split ( key , 2 ) self . split_dim = split_dim if split_dim is not None else input_shape [ - 1 ] // 2 self . reverse_conditioning = reverse_conditioning # Get the shapes of the input to the transform and the network transform_input_shape , net_input_shape = self . get_split_shapes ( input_shape ) transform = transform_init ( transform_input_shape , key = k1 ) net_output_size = self . get_net_output_shapes ( input_shape , transform ) net = net_init ( net_input_shape , net_output_size , key = k2 ) self . net = net # Use this to turn an eqx module into an array and vice-versa self . params_to_transform = RavelParameters ( transform ) # Also initialize the parameters to be close to 0 self . scale = random . normal ( key , ( 1 ,)) * 0.01 split ( self , x : Array ) -> Tuple [ Array , Array ] \u00a4 Inherited from generax.flows.coupling.Coupling.split . Source code in generax/flows/coupling.py def split ( self , x : Array ) -> Tuple [ Array , Array ]: \"\"\"Split the input into two halves.\"\"\" x1 , x2 = x [ ... , : self . split_dim ], x [ ... , self . split_dim :] if self . reverse_conditioning : return x2 , x1 return x1 , x2 get_split_shapes ( self , input_shape : Tuple [ int ]) -> Tuple [ Tuple [ int ]] \u00a4 Inherited from generax.flows.coupling.Coupling.get_split_shapes . Source code in generax/flows/coupling.py def get_split_shapes ( self , input_shape : Tuple [ int ]) -> Tuple [ Tuple [ int ]]: x1_dim , x2_dim = self . split_dim , input_shape [ - 1 ] - self . split_dim x1_shape = input_shape [: - 1 ] + ( x1_dim ,) x2_shape = input_shape [: - 1 ] + ( x2_dim ,) if self . reverse_conditioning : return x2_shape , x1_shape return x1_shape , x2_shape get_net_output_shapes ( self , input_shape : Tuple [ int ], transform : BijectiveTransform ) -> Tuple [ Tuple [ int ], int ] \u00a4 Inherited from generax.flows.coupling.Coupling.get_net_output_shapes . Source code in generax/flows/coupling.py def get_net_output_shapes ( self , input_shape : Tuple [ int ], transform : BijectiveTransform ) -> Tuple [ Tuple [ int ], int ]: \"\"\" **Arguments**: - `input_shape`: The shape of the input - `transform`: The bijective transformation to use. **Returns**: - `net_output_size`: The size of the output of the neural network. This is a single integer because the network is expected to produce a single vector. \"\"\" x1_shape , x2_shape = self . get_split_shapes ( input_shape ) if x1_shape != transform . input_shape : raise ValueError ( f 'The transform { transform } needs to have an input shape equal to { x1_shape } . Use `get_input_shapes` to get this shape.' ) params_to_transform = RavelParameters ( transform ) net_output_size = params_to_transform . flat_params_size return net_output_size data_dependent_init ( self , t : Array , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform \u00a4 Initialize the parameters of the layer based on the data. Arguments : t : The time to initialize the parameters with. x : The data to initialize the parameters with. y : The conditioning information key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/flows/coupling.py def data_dependent_init ( self , t : Array , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `t`: The time to initialize the parameters with. - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' x1 , x2 = self . split ( x ) net = self . net . data_dependent_init ( t , x2 , y = y , key = key ) # Turn the new parameters into a new module def get_net ( tree ): return tree . net updated_layer = eqx . tree_at ( get_net , self , net ) return updated_layer __call__ ( self , t : Array , xt : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Implements generax.flows.base.TimeDependentBijectiveTransform.__call__ . Source code in generax/flows/coupling.py def __call__ ( self , t : Array , xt : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `xt`: The input to the transformation. If inverse=True, then should be x0 - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (x0, log_det) \"\"\" assert xt . shape == self . input_shape , 'Only works on unbatched data' # Split the input into two halves x1 , x2 = self . split ( xt ) params = self . net ( t , x2 , y = y , ** kwargs ) params *= self . scale * t assert params . size == self . params_to_transform . flat_params_size # Apply the transformation to x1 given x2 transform = self . params_to_transform ( params ) z1 , log_det = transform ( x1 , y = y , inverse = inverse , ** kwargs ) z = self . combine ( z1 , x2 ) return z , log_det inverse ( self , t : Array , x0 : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Inherited from generax.flows.base.TimeDependentBijectiveTransform.inverse . Source code in generax/flows/coupling.py def inverse ( self , t : Array , x0 : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (xt, log_det) \"\"\" return self ( t , x0 , y = y , inverse = True , ** kwargs ) vector_field ( self , t : Array , xt : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Inherited from generax.flows.base.TimeDependentBijectiveTransform.vector_field . Source code in generax/flows/coupling.py def vector_field ( self , t : Array , xt : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"The vector field that samples evolve on as t changes **Arguments**: - `t`: Time. - `xt`: A point in the data space. - `y`: The conditioning information. **Returns**: `return vt` \"\"\" x0 = self . to_base_space ( t , xt , y = y , ** kwargs ) def ft ( t ): return self . to_data_space ( t , x0 , y = y , ** kwargs ) return jax . jvp ( ft , ( t ,), ( jnp . ones_like ( t ),))[ 1 ]","title":"Coupling"},{"location":"api/flows/coupling/#coupling","text":"","title":"Coupling"},{"location":"api/flows/coupling/#generax.flows.coupling.RavelParameters","text":"Flatten and concatenate the parameters of a eqx.Module Source code in generax/flows/coupling.py class RavelParameters ( eqx . Module ): \"\"\"Flatten and concatenate the parameters of a eqx.Module \"\"\" shapes_and_sizes : Sequence [ Tuple [ Tuple [ int ], int ]] = eqx . field ( static = True ) flat_params_size : Tuple [ int ] = eqx . field ( static = True ) static : Any = eqx . field ( static = True ) treedef : Any = eqx . field ( static = True ) indices : np . ndarray = eqx . field ( static = True ) def __init__ ( self , module ): # Split the parameters into dynamic and static params , self . static = eqx . partition ( module , eqx . is_array ) # Flatten the parameters so that we can extract its sizes leaves , self . treedef = jax . tree_util . tree_flatten ( params ) # Get the shape and size of each leaf self . shapes_and_sizes = [( leaf . shape , leaf . size ) for leaf in leaves ] # Flatten the parameters flat_params = jnp . concatenate ([ leaf . ravel () for leaf in leaves ]) # Keep track of the size of the flattened parameters self . flat_params_size = flat_params . size # Keep track of the split points for each paramter in the flattened array self . indices = np . cumsum ( np . array ([ 0 ] + [ size for _ , size in self . shapes_and_sizes ])) def flatten_params ( self , module : eqx . Module ) -> Array : # Split the parameters into dynamic and static params , _ = eqx . partition ( module , eqx . is_array ) # Flatten the parameters so that we can extract its sizes leaves , _ = jax . tree_util . tree_flatten ( params ) # Flatten the parameters flat_params = jnp . concatenate ([ leaf . ravel () for leaf in leaves ]) return flat_params def __call__ ( self , flat_params : Array ) -> eqx . Module : flat_params = flat_params . ravel () # Flatten the parameters completely leaves = [] for i , ( shape , size ) in enumerate ( self . shapes_and_sizes ): # Extract each leaf from the flattened parameters and reshape it buffer = flat_params [ self . indices [ i ]: self . indices [ i + 1 ]] if buffer . size != misc . list_prod ( shape ): raise ValueError ( f 'Expected total size of { misc . list_prod ( shape ) } but got { buffer . size } ' ) leaf = buffer . reshape ( shape ) leaves . append ( leaf ) # Turn the leaves back into a tree params = jax . tree_util . tree_unflatten ( self . treedef , leaves ) return eqx . combine ( params , self . static )","title":"RavelParameters"},{"location":"api/flows/coupling/#generax.flows.coupling.Coupling","text":"Parametrize a flow over half of the inputs using the other half. The conditioning network will be fixed # Example of intended usage: def initialize_scale ( transform_input_shape , key ): return ShiftScale ( input_shape = transform_input_shape , key = key , ** kwargs ) def initialize_network ( net_input_shape , net_output_size , key ): return ResNet ( input_shape = net_input_shape , out_size = net_output_size , key = key , ** kwargs ) layer = Coupling ( transform_init = initialize_scale , net_init = initialize_network , input_shape = input_shape , cond_shape = cond_shape , key = key , reverse_conditioning = True , split_dim = 1 ) z , log_det = layer ( x , y ) Attributes : - params_to_transform : A module that turns an array of parameters into an eqx.Module. - scale : A scalar that we'll use to start with small parameter values - net : The neural network to use. Source code in generax/flows/coupling.py class Coupling ( BijectiveTransform ): \"\"\"Parametrize a flow over half of the inputs using the other half. The conditioning network will be fixed ```python # Example of intended usage: def initialize_scale(transform_input_shape, key): return ShiftScale(input_shape=transform_input_shape, key=key, **kwargs) def initialize_network(net_input_shape, net_output_size, key): return ResNet(input_shape=net_input_shape, out_size=net_output_size, key=key, **kwargs) layer = Coupling(transform_init=initialize_scale, net_init=initialize_network, input_shape=input_shape, cond_shape=cond_shape, key=key, reverse_conditioning=True, split_dim=1) z, log_det = layer(x, y) ``` **Attributes**: - `params_to_transform`: A module that turns an array of parameters into an eqx.Module. - `scale`: A scalar that we'll use to start with small parameter values - `net`: The neural network to use. \"\"\" net : eqx . Module scale : Array params_to_transform : RavelParameters split_dim : Optional [ int ] = eqx . field ( static = True ) reverse_conditioning : bool = eqx . field ( static = True ) def __init__ ( self , transform_init : Callable [[ Tuple [ int ]], BijectiveTransform ], net_init : Callable [[ Tuple [ int ], int ], eqx . Module ], input_shape : Tuple [ int ], cond_shape : Optional [ Tuple [ int ]] = None , split_dim : Optional [ int ] = None , reverse_conditioning : Optional [ bool ] = False , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `transform`: The bijective transformation to use. - `net`: The neural network to generate the transform parameters. - `input_shape`: The shape of the input - `cond_shape`: The shape of the conditioning information - `split_dim`: The number of dimension to split the last axis on. If `None`, defaults to `dim//2`. - `reverse_conditioning`: If `True`, condition on the first part of the input instead of the second part. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , cond_shape = cond_shape , ** kwargs ) k1 , k2 = random . split ( key , 2 ) self . split_dim = split_dim if split_dim is not None else input_shape [ - 1 ] // 2 self . reverse_conditioning = reverse_conditioning # Get the shapes of the input to the transform and the network transform_input_shape , net_input_shape = self . get_split_shapes ( input_shape ) transform = transform_init ( transform_input_shape , key = k1 ) net_output_size = self . get_net_output_shapes ( input_shape , transform ) net = net_init ( net_input_shape , net_output_size , key = k2 ) self . net = net # Use this to turn an eqx module into an array and vice-versa self . params_to_transform = RavelParameters ( transform ) # Also initialize the parameters to be close to 0 self . scale = random . normal ( key , ( 1 ,)) * 0.01 def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' x1 , x2 = self . split ( x ) net = self . net . data_dependent_init ( x2 , y = y , key = key ) # Turn the new parameters into a new module get_net = lambda tree : tree . net updated_layer = eqx . tree_at ( get_net , self , net ) return updated_layer def split ( self , x : Array ) -> Tuple [ Array , Array ]: \"\"\"Split the input into two halves.\"\"\" x1 , x2 = x [ ... , : self . split_dim ], x [ ... , self . split_dim :] if self . reverse_conditioning : return x2 , x1 return x1 , x2 def combine ( self , x1 : Array , x2 : Array ) -> Array : \"\"\"Combine the two halves of the input.\"\"\" if self . reverse_conditioning : return jnp . concatenate ([ x2 , x1 ], axis =- 1 ) return jnp . concatenate ([ x1 , x2 ], axis =- 1 ) def get_split_shapes ( self , input_shape : Tuple [ int ]) -> Tuple [ Tuple [ int ]]: x1_dim , x2_dim = self . split_dim , input_shape [ - 1 ] - self . split_dim x1_shape = input_shape [: - 1 ] + ( x1_dim ,) x2_shape = input_shape [: - 1 ] + ( x2_dim ,) if self . reverse_conditioning : return x2_shape , x1_shape return x1_shape , x2_shape def get_net_output_shapes ( self , input_shape : Tuple [ int ], transform : BijectiveTransform ) -> Tuple [ Tuple [ int ], int ]: \"\"\" **Arguments**: - `input_shape`: The shape of the input - `transform`: The bijective transformation to use. **Returns**: - `net_output_size`: The size of the output of the neural network. This is a single integer because the network is expected to produce a single vector. \"\"\" x1_shape , x2_shape = self . get_split_shapes ( input_shape ) if x1_shape != transform . input_shape : raise ValueError ( f 'The transform { transform } needs to have an input shape equal to { x1_shape } . Use `get_input_shapes` to get this shape.' ) params_to_transform = RavelParameters ( transform ) net_output_size = params_to_transform . flat_params_size return net_output_size def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' # Split the input into two halves x1 , x2 = self . split ( x ) params = self . net ( x2 , y = y , ** kwargs ) params *= self . scale assert params . size == self . params_to_transform . flat_params_size # Apply the transformation to x1 given x2 transform = self . params_to_transform ( params ) z1 , log_det = transform ( x1 , y = y , inverse = inverse , ** kwargs ) z = self . combine ( z1 , x2 ) return z , log_det","title":"Coupling"},{"location":"api/flows/coupling/#generax.flows.coupling.TimeDependentCoupling","text":"Time dependent coupling transform. At t=0, this will pass parameters of 0s to the transform. ``` Attributes : - params_to_transform : A module that turns an array of parameters into an eqx.Module. - scale : A scalar that we'll use to start with small parameter values - net : The neural network to use. Source code in generax/flows/coupling.py class TimeDependentCoupling ( Coupling , TimeDependentBijectiveTransform ): \"\"\"Time dependent coupling transform. At t=0, this will pass parameters of 0s to the transform. ``` **Attributes**: - `params_to_transform`: A module that turns an array of parameters into an eqx.Module. - `scale`: A scalar that we'll use to start with small parameter values - `net`: The neural network to use. \"\"\" def data_dependent_init ( self , t : Array , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `t`: The time to initialize the parameters with. - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' x1 , x2 = self . split ( x ) net = self . net . data_dependent_init ( t , x2 , y = y , key = key ) # Turn the new parameters into a new module def get_net ( tree ): return tree . net updated_layer = eqx . tree_at ( get_net , self , net ) return updated_layer def __call__ ( self , t : Array , xt : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `xt`: The input to the transformation. If inverse=True, then should be x0 - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (x0, log_det) \"\"\" assert xt . shape == self . input_shape , 'Only works on unbatched data' # Split the input into two halves x1 , x2 = self . split ( xt ) params = self . net ( t , x2 , y = y , ** kwargs ) params *= self . scale * t assert params . size == self . params_to_transform . flat_params_size # Apply the transformation to x1 given x2 transform = self . params_to_transform ( params ) z1 , log_det = transform ( x1 , y = y , inverse = inverse , ** kwargs ) z = self . combine ( z1 , x2 ) return z , log_det","title":"TimeDependentCoupling"},{"location":"api/flows/ffjord/","text":"FFJORD \u00a4 generax.flows.ffjord.FFJORDTransform ( BijectiveTransform ) \u00a4 Flow parametrized by a neural ODE https://arxiv.org/pdf/1810.01367.pdf Attributes : input_shape : The input shape. Output shape will have the same dimensionality as the input. cond_shape : The shape of the conditioning information. If there is no conditioning information, this is None. neural_ode : The neural ODE adjoint : The adjoint method to use. Can be one of the following: key : The random key to use for initialization Source code in generax/flows/ffjord.py class FFJORDTransform ( BijectiveTransform ): \"\"\"Flow parametrized by a neural ODE https://arxiv.org/pdf/1810.01367.pdf **Attributes**: - `input_shape`: The input shape. Output shape will have the same dimensionality as the input. - `cond_shape`: The shape of the conditioning information. If there is no conditioning information, this is None. - `neural_ode`: The neural ODE - `adjoint`: The adjoint method to use. Can be one of the following: - `key`: The random key to use for initialization \"\"\" neural_ode : NeuralODE def __init__ ( self , input_shape : Tuple [ int ], net : eqx . Module = None , working_size : int = 16 , hidden_size : int = 32 , n_blocks : int = 4 , time_embedding_size = 16 , n_time_features = 8 , cond_shape : Optional [ Tuple [ int ]] = None , * , controller_rtol : Optional [ float ] = 1e-8 , controller_atol : Optional [ float ] = 1e-8 , adjoint = 'recursive_checkpoint' , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The shape of the input to the transformation - `net`: The neural network to use for the vector field. If None, a default network will be used. `net` should accept `net(t, x, y=y)` - `controller_rtol`: The relative tolerance of the stepsize controller. - `controller_atol`: The absolute tolerance of the stepsize controller. - `trace_estimate_likelihood`: Whether to use a trace estimate for the likelihood. - `adjoint`: The adjoint method to use. Can be one of the following: - `\"recursive_checkpoint\"`: Use the recursive checkpoint method. Doesn't support jvp. - `\"direct\"`: Use the direct method. Supports jvps. - `\"seminorm\"`: Use the seminorm method. Does fast backprop through the solver. - `key`: The random key to use for initialization \"\"\" if net is None : net = TimeDependentResNet ( input_shape = input_shape , working_size = working_size , hidden_size = hidden_size , out_size = input_shape [ - 1 ], n_blocks = n_blocks , cond_shape = cond_shape , embedding_size = time_embedding_size , out_features = n_time_features , key = key ) self . neural_ode = NeuralODE ( vf = net , adjoint = adjoint , controller_rtol = controller_rtol , controller_atol = controller_atol ) super () . __init__ ( input_shape = input_shape , ** kwargs ) @property def vector_field ( self ): return self . neural_ode . vector_field def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , log_likelihood : bool = True , trace_estimate_likelihood : Optional [ bool ] = False , save_at : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation - `log_likelihood`: Whether to compute the log likelihood of the transformation - `trace_estimate_likelihood`: Whether to compute a trace estimate of the likelihood of the neural ODE. - `save_at`: The times to save the neural ODE at. - `key`: The random key to use for initialization **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if log_likelihood and ( trace_estimate_likelihood and ( key is None )): raise TypeError ( f 'When using trace estimation, must pass random key' ) if log_likelihood == False : trace_estimate_likelihood = False solution = self . neural_ode ( x , y = y , inverse = inverse , log_likelihood = log_likelihood , trace_estimate_likelihood = trace_estimate_likelihood , save_at = save_at , key = key , ** kwargs ) return solution . ys , solution . log_det neural_ode : NeuralODE dataclass-field \u00a4 vector_field property readonly \u00a4 __init__ ( self , input_shape : Tuple [ int ], net : Module = None , working_size : int = 16 , hidden_size : int = 32 , n_blocks : int = 4 , time_embedding_size = 16 , n_time_features = 8 , cond_shape : Optional [ Tuple [ int ]] = None , * , controller_rtol : Optional [ float ] = 1e-08 , controller_atol : Optional [ float ] = 1e-08 , adjoint = 'recursive_checkpoint' , key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : input_shape : The shape of the input to the transformation net : The neural network to use for the vector field. If None, a default network will be used. net should accept net(t, x, y=y) controller_rtol : The relative tolerance of the stepsize controller. controller_atol : The absolute tolerance of the stepsize controller. trace_estimate_likelihood : Whether to use a trace estimate for the likelihood. adjoint : The adjoint method to use. Can be one of the following: \"recursive_checkpoint\" : Use the recursive checkpoint method. Doesn't support jvp. \"direct\" : Use the direct method. Supports jvps. \"seminorm\" : Use the seminorm method. Does fast backprop through the solver. key : The random key to use for initialization Source code in generax/flows/ffjord.py def __init__ ( self , input_shape : Tuple [ int ], net : eqx . Module = None , working_size : int = 16 , hidden_size : int = 32 , n_blocks : int = 4 , time_embedding_size = 16 , n_time_features = 8 , cond_shape : Optional [ Tuple [ int ]] = None , * , controller_rtol : Optional [ float ] = 1e-8 , controller_atol : Optional [ float ] = 1e-8 , adjoint = 'recursive_checkpoint' , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The shape of the input to the transformation - `net`: The neural network to use for the vector field. If None, a default network will be used. `net` should accept `net(t, x, y=y)` - `controller_rtol`: The relative tolerance of the stepsize controller. - `controller_atol`: The absolute tolerance of the stepsize controller. - `trace_estimate_likelihood`: Whether to use a trace estimate for the likelihood. - `adjoint`: The adjoint method to use. Can be one of the following: - `\"recursive_checkpoint\"`: Use the recursive checkpoint method. Doesn't support jvp. - `\"direct\"`: Use the direct method. Supports jvps. - `\"seminorm\"`: Use the seminorm method. Does fast backprop through the solver. - `key`: The random key to use for initialization \"\"\" if net is None : net = TimeDependentResNet ( input_shape = input_shape , working_size = working_size , hidden_size = hidden_size , out_size = input_shape [ - 1 ], n_blocks = n_blocks , cond_shape = cond_shape , embedding_size = time_embedding_size , out_features = n_time_features , key = key ) self . neural_ode = NeuralODE ( vf = net , adjoint = adjoint , controller_rtol = controller_rtol , controller_atol = controller_atol ) super () . __init__ ( input_shape = input_shape , ** kwargs ) data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) \u00a4 Inherited from generax.flows.base.BijectiveTransform.data_dependent_init . Source code in generax/flows/ffjord.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ): \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" return self __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , log_likelihood : bool = True , trace_estimate_likelihood : Optional [ bool ] = False , save_at : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None , ** kwargs ) -> Array \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation log_likelihood : Whether to compute the log likelihood of the transformation trace_estimate_likelihood : Whether to compute a trace estimate of the likelihood of the neural ODE. save_at : The times to save the neural ODE at. key : The random key to use for initialization Returns : (z, log_det) Source code in generax/flows/ffjord.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , log_likelihood : bool = True , trace_estimate_likelihood : Optional [ bool ] = False , save_at : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation - `log_likelihood`: Whether to compute the log likelihood of the transformation - `trace_estimate_likelihood`: Whether to compute a trace estimate of the likelihood of the neural ODE. - `save_at`: The times to save the neural ODE at. - `key`: The random key to use for initialization **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if log_likelihood and ( trace_estimate_likelihood and ( key is None )): raise TypeError ( f 'When using trace estimation, must pass random key' ) if log_likelihood == False : trace_estimate_likelihood = False solution = self . neural_ode ( x , y = y , inverse = inverse , log_likelihood = log_likelihood , trace_estimate_likelihood = trace_estimate_likelihood , save_at = save_at , key = key , ** kwargs ) return solution . ys , solution . log_det inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Inherited from generax.flows.base.BijectiveTransform.inverse . Source code in generax/flows/ffjord.py def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs ) generax.nn.neural_ode.NeuralODE \u00a4 Neural ODE Source code in generax/nn/neural_ode.py class NeuralODE ( eqx . Module ): \"\"\"Neural ODE\"\"\" vector_field : eqx . Module adjoint : diffrax . AbstractAdjoint stepsize_controller : diffrax . AbstractAdaptiveStepSizeController def __init__ ( self , vf : eqx . Module , adjoint : Optional [ str ] = 'recursive_checkpoint' , controller_rtol : Optional [ float ] = 1e-3 , controller_atol : Optional [ float ] = 1e-5 , ): \"\"\"**Arguments**: - `vf`: A function that computes the vector field. It must output a vector of the same shape as its input. - `adjoint`: The adjoint method to use. Can be one of the following: - `\"recursive_checkpoint\"`: Use the recursive checkpoint method. Doesn't support jvp. - `\"direct\"`: Use the direct method. Supports jvps. - `\"seminorm\"`: Use the seminorm method. Does fast backprop through the solver. - `controller_rtol`: The relative tolerance of the stepsize controller. - `controller_atol`: The absolute tolerance of the stepsize controller. \"\"\" self . vector_field = vf if adjoint == 'recursive_checkpoint' : self . adjoint = diffrax . RecursiveCheckpointAdjoint () elif adjoint == 'direct' : self . adjoint = diffrax . DirectAdjoint () elif adjoint == 'seminorm' : adjoint_controller = diffrax . PIDController ( rtol = 1e-3 , atol = 1e-6 , norm = diffrax . adjoint_rms_seminorm ) self . adjoint = diffrax . BacksolveAdjoint ( stepsize_controller = adjoint_controller ) self . stepsize_controller = diffrax . PIDController ( rtol = controller_rtol , atol = controller_atol ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , * , inverse : Optional [ bool ] = False , log_likelihood : Optional [ bool ] = False , trace_estimate_likelihood : Optional [ bool ] = False , save_at : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None , t0 : Optional [ float ] = 0.0 , t1 : Optional [ float ] = 1.0 ) -> Array : \"\"\"**Arguemnts**: - `x`: The input to the neural ODE. Must be a rank 1 array. - `key`: The random number generator key. - `inverse`: Whether to compute the inverse of the neural ODE. `inverse=True` corresponds to going from the base space to the data space. - `log_likelihood`: Whether to compute the log likelihood of the neural ODE. - `trace_estimate_likelihood`: Whether to compute a trace estimate of the likelihood of the neural ODE. - `save_at`: The times to save the neural ODE at. - `key`: The random key to use for initialization - `t0`: The initial time. - `t1`: The final time. **Returns**: - `z`: The output of the neural ODE. - `log_likelihood`: The log likelihood of the neural ODE if `log_likelihood=True`. \"\"\" assert x . shape == self . vector_field . input_shape if trace_estimate_likelihood : # Get a random vector for hutchinsons trace estimator k1 , _ = random . split ( key , 2 ) v = random . normal ( k1 , x . shape ) # Split the model into its static and dynamic parts so that backprop # through the ode solver can be faster. params , static = eqx . partition ( self . vector_field , eqx . is_array ) def f ( t , carry , params ): x , log_det , total_vf_norm , total_jac_frob_norm = carry if inverse == False : # If we're inverting the flow, we need to adjust the time t = t1 - t # Recombine the model model = eqx . combine ( params , static ) # Fill the model with the current time def apply_vf ( x ): return model ( t , x , y = y ) if log_likelihood : if trace_estimate_likelihood : # Hutchinsons trace estimator. See ContinuousNormalizingFlow https://arxiv.org/pdf/1810.01367.pdf dxdt , dudxv = jax . jvp ( apply_vf , ( x ,), ( v ,)) dlogpxdt = - jnp . sum ( dudxv * v ) dtjfndt = jnp . sum ( dudxv ** 2 ) else : # Brute force dlogpx/dt. See NeuralODE https://arxiv.org/pdf/1806.07366.pdf x_flat = x . ravel () eye = jnp . eye ( x_flat . shape [ - 1 ]) x_shape = x . shape def jvp_flat ( x_flat , dx_flat ): x = x_flat . reshape ( x_shape ) dx = dx_flat . reshape ( x_shape ) dxdt , d2dx_dtdx = jax . jvp ( apply_vf , ( x ,), ( dx ,)) return dxdt , d2dx_dtdx . ravel () dxdt , d2dx_dtdx_flat = jax . vmap ( jvp_flat , in_axes = ( None , 0 ))( x_flat , eye ) dxdt = dxdt [ 0 ] dlogpxdt = - jnp . trace ( d2dx_dtdx_flat ) dtjfndt = jnp . sum ( d2dx_dtdx_flat ** 2 ) else : # Don't worry about the log likelihood dxdt = apply_vf ( x ) dlogpxdt = jnp . zeros_like ( log_det ) dtjfndt = jnp . zeros_like ( total_jac_frob_norm ) if inverse == False : # If we're inverting the flow, we need to flip the sign of dxdt dxdt = - dxdt # Accumulate the norm of the vector field dvfnormdt = jnp . sum ( dxdt ** 2 ) if inverse : dlogpxdt = - dlogpxdt return dxdt , dlogpxdt , dvfnormdt , dtjfndt term = diffrax . ODETerm ( f ) solver = diffrax . Dopri5 () # Determine which times we want to save the neural ODE at. if save_at is None : saveat = diffrax . SaveAt ( ts = [ t1 ]) else : saveat = diffrax . SaveAt ( ts = save_at ) log_det = jnp . array ( 0.0 ) total_vf_norm = jnp . array ( 0.0 ) total_jac_frob_norm = jnp . array ( 0.0 ) # Run the ODE solver solution = diffrax . diffeqsolve ( term , solver , saveat = saveat , t0 = t0 , t1 = t1 , dt0 = 0.001 , y0 = ( x , log_det , total_vf_norm , total_jac_frob_norm ), args = params , adjoint = self . adjoint , stepsize_controller = self . stepsize_controller , throw = True ) outs = solution . ys if save_at is None : # Only take the first time outs = jax . tree_util . tree_map ( lambda x : x [ 0 ], outs ) z , log_det , total_vf_norm , total_jac_frob_norm = outs # Construct the new solution kwargs = { f . name : getattr ( solution , f . name ) for f in fields ( solution )} kwargs [ 'ys' ] = z new_solution = NeuralODESolution ( log_det = log_det , total_vf_norm = total_vf_norm , total_jac_frob_norm = total_jac_frob_norm , ** kwargs ) return new_solution __init__ ( self , vf : Module , adjoint : Optional [ str ] = 'recursive_checkpoint' , controller_rtol : Optional [ float ] = 0.001 , controller_atol : Optional [ float ] = 1e-05 ) \u00a4 Arguments : vf : A function that computes the vector field. It must output a vector of the same shape as its input. adjoint : The adjoint method to use. Can be one of the following: \"recursive_checkpoint\" : Use the recursive checkpoint method. Doesn't support jvp. \"direct\" : Use the direct method. Supports jvps. \"seminorm\" : Use the seminorm method. Does fast backprop through the solver. controller_rtol : The relative tolerance of the stepsize controller. controller_atol : The absolute tolerance of the stepsize controller. Source code in generax/nn/neural_ode.py def __init__ ( self , vf : eqx . Module , adjoint : Optional [ str ] = 'recursive_checkpoint' , controller_rtol : Optional [ float ] = 1e-3 , controller_atol : Optional [ float ] = 1e-5 , ): \"\"\"**Arguments**: - `vf`: A function that computes the vector field. It must output a vector of the same shape as its input. - `adjoint`: The adjoint method to use. Can be one of the following: - `\"recursive_checkpoint\"`: Use the recursive checkpoint method. Doesn't support jvp. - `\"direct\"`: Use the direct method. Supports jvps. - `\"seminorm\"`: Use the seminorm method. Does fast backprop through the solver. - `controller_rtol`: The relative tolerance of the stepsize controller. - `controller_atol`: The absolute tolerance of the stepsize controller. \"\"\" self . vector_field = vf if adjoint == 'recursive_checkpoint' : self . adjoint = diffrax . RecursiveCheckpointAdjoint () elif adjoint == 'direct' : self . adjoint = diffrax . DirectAdjoint () elif adjoint == 'seminorm' : adjoint_controller = diffrax . PIDController ( rtol = 1e-3 , atol = 1e-6 , norm = diffrax . adjoint_rms_seminorm ) self . adjoint = diffrax . BacksolveAdjoint ( stepsize_controller = adjoint_controller ) self . stepsize_controller = diffrax . PIDController ( rtol = controller_rtol , atol = controller_atol ) __call__ ( self , x : Array , y : Optional [ Array ] = None , * , inverse : Optional [ bool ] = False , log_likelihood : Optional [ bool ] = False , trace_estimate_likelihood : Optional [ bool ] = False , save_at : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None , t0 : Optional [ float ] = 0.0 , t1 : Optional [ float ] = 1.0 ) -> Array \u00a4 Arguemnts : x : The input to the neural ODE. Must be a rank 1 array. key : The random number generator key. inverse : Whether to compute the inverse of the neural ODE. inverse=True corresponds to going from the base space to the data space. log_likelihood : Whether to compute the log likelihood of the neural ODE. trace_estimate_likelihood : Whether to compute a trace estimate of the likelihood of the neural ODE. save_at : The times to save the neural ODE at. key : The random key to use for initialization t0 : The initial time. t1 : The final time. Returns : - z : The output of the neural ODE. - log_likelihood : The log likelihood of the neural ODE if log_likelihood=True . Source code in generax/nn/neural_ode.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , * , inverse : Optional [ bool ] = False , log_likelihood : Optional [ bool ] = False , trace_estimate_likelihood : Optional [ bool ] = False , save_at : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None , t0 : Optional [ float ] = 0.0 , t1 : Optional [ float ] = 1.0 ) -> Array : \"\"\"**Arguemnts**: - `x`: The input to the neural ODE. Must be a rank 1 array. - `key`: The random number generator key. - `inverse`: Whether to compute the inverse of the neural ODE. `inverse=True` corresponds to going from the base space to the data space. - `log_likelihood`: Whether to compute the log likelihood of the neural ODE. - `trace_estimate_likelihood`: Whether to compute a trace estimate of the likelihood of the neural ODE. - `save_at`: The times to save the neural ODE at. - `key`: The random key to use for initialization - `t0`: The initial time. - `t1`: The final time. **Returns**: - `z`: The output of the neural ODE. - `log_likelihood`: The log likelihood of the neural ODE if `log_likelihood=True`. \"\"\" assert x . shape == self . vector_field . input_shape if trace_estimate_likelihood : # Get a random vector for hutchinsons trace estimator k1 , _ = random . split ( key , 2 ) v = random . normal ( k1 , x . shape ) # Split the model into its static and dynamic parts so that backprop # through the ode solver can be faster. params , static = eqx . partition ( self . vector_field , eqx . is_array ) def f ( t , carry , params ): x , log_det , total_vf_norm , total_jac_frob_norm = carry if inverse == False : # If we're inverting the flow, we need to adjust the time t = t1 - t # Recombine the model model = eqx . combine ( params , static ) # Fill the model with the current time def apply_vf ( x ): return model ( t , x , y = y ) if log_likelihood : if trace_estimate_likelihood : # Hutchinsons trace estimator. See ContinuousNormalizingFlow https://arxiv.org/pdf/1810.01367.pdf dxdt , dudxv = jax . jvp ( apply_vf , ( x ,), ( v ,)) dlogpxdt = - jnp . sum ( dudxv * v ) dtjfndt = jnp . sum ( dudxv ** 2 ) else : # Brute force dlogpx/dt. See NeuralODE https://arxiv.org/pdf/1806.07366.pdf x_flat = x . ravel () eye = jnp . eye ( x_flat . shape [ - 1 ]) x_shape = x . shape def jvp_flat ( x_flat , dx_flat ): x = x_flat . reshape ( x_shape ) dx = dx_flat . reshape ( x_shape ) dxdt , d2dx_dtdx = jax . jvp ( apply_vf , ( x ,), ( dx ,)) return dxdt , d2dx_dtdx . ravel () dxdt , d2dx_dtdx_flat = jax . vmap ( jvp_flat , in_axes = ( None , 0 ))( x_flat , eye ) dxdt = dxdt [ 0 ] dlogpxdt = - jnp . trace ( d2dx_dtdx_flat ) dtjfndt = jnp . sum ( d2dx_dtdx_flat ** 2 ) else : # Don't worry about the log likelihood dxdt = apply_vf ( x ) dlogpxdt = jnp . zeros_like ( log_det ) dtjfndt = jnp . zeros_like ( total_jac_frob_norm ) if inverse == False : # If we're inverting the flow, we need to flip the sign of dxdt dxdt = - dxdt # Accumulate the norm of the vector field dvfnormdt = jnp . sum ( dxdt ** 2 ) if inverse : dlogpxdt = - dlogpxdt return dxdt , dlogpxdt , dvfnormdt , dtjfndt term = diffrax . ODETerm ( f ) solver = diffrax . Dopri5 () # Determine which times we want to save the neural ODE at. if save_at is None : saveat = diffrax . SaveAt ( ts = [ t1 ]) else : saveat = diffrax . SaveAt ( ts = save_at ) log_det = jnp . array ( 0.0 ) total_vf_norm = jnp . array ( 0.0 ) total_jac_frob_norm = jnp . array ( 0.0 ) # Run the ODE solver solution = diffrax . diffeqsolve ( term , solver , saveat = saveat , t0 = t0 , t1 = t1 , dt0 = 0.001 , y0 = ( x , log_det , total_vf_norm , total_jac_frob_norm ), args = params , adjoint = self . adjoint , stepsize_controller = self . stepsize_controller , throw = True ) outs = solution . ys if save_at is None : # Only take the first time outs = jax . tree_util . tree_map ( lambda x : x [ 0 ], outs ) z , log_det , total_vf_norm , total_jac_frob_norm = outs # Construct the new solution kwargs = { f . name : getattr ( solution , f . name ) for f in fields ( solution )} kwargs [ 'ys' ] = z new_solution = NeuralODESolution ( log_det = log_det , total_vf_norm = total_vf_norm , total_jac_frob_norm = total_jac_frob_norm , ** kwargs ) return new_solution generax.nn.neural_ode.NeuralODESolution \u00a4 The solution to a neural ODE. This wraps the diffrax solution class and adds the log determinant of the transformation and some other items from http://proceedings.mlr.press/v119/finlay20a/finlay20a.pdf Attributes : log_det : The log determinant of the transformation. total_vf_norm : The total norm of the vector on the path. This can help determine how straight the path is. total_jac_frob_norm : The total norm of the jacobian of the vector field. This Source code in generax/nn/neural_ode.py class NeuralODESolution ( Solution ): \"\"\"The solution to a neural ODE. This wraps the diffrax solution class and adds the log determinant of the transformation and some other items from http://proceedings.mlr.press/v119/finlay20a/finlay20a.pdf **Attributes**: - `log_det`: The log determinant of the transformation. - `total_vf_norm`: The total norm of the vector on the path. This can help determine how straight the path is. - `total_jac_frob_norm`: The total norm of the jacobian of the vector field. This \"\"\" log_det : Array total_vf_norm : Array total_jac_frob_norm : Array __init__ ( self , t0 : Scalar , t1 : Scalar , ts : Optional [ Array [ 'times' ]], ys : Optional [ PyTree [ 'times' , ... ]], interpolation : Optional [ diffrax . global_interpolation . DenseInterpolation ], stats : Dict [ str , Any ], result : RESULTS , solver_state : Optional [ PyTree ], controller_state : Optional [ PyTree ], made_jump : Optional [ bool ], log_det : Array , total_vf_norm : Array , total_jac_frob_norm : Array ) \u00a4","title":"FFJORD"},{"location":"api/flows/ffjord/#ffjord","text":"","title":"FFJORD"},{"location":"api/flows/ffjord/#generax.flows.ffjord.FFJORDTransform","text":"Flow parametrized by a neural ODE https://arxiv.org/pdf/1810.01367.pdf Attributes : input_shape : The input shape. Output shape will have the same dimensionality as the input. cond_shape : The shape of the conditioning information. If there is no conditioning information, this is None. neural_ode : The neural ODE adjoint : The adjoint method to use. Can be one of the following: key : The random key to use for initialization Source code in generax/flows/ffjord.py class FFJORDTransform ( BijectiveTransform ): \"\"\"Flow parametrized by a neural ODE https://arxiv.org/pdf/1810.01367.pdf **Attributes**: - `input_shape`: The input shape. Output shape will have the same dimensionality as the input. - `cond_shape`: The shape of the conditioning information. If there is no conditioning information, this is None. - `neural_ode`: The neural ODE - `adjoint`: The adjoint method to use. Can be one of the following: - `key`: The random key to use for initialization \"\"\" neural_ode : NeuralODE def __init__ ( self , input_shape : Tuple [ int ], net : eqx . Module = None , working_size : int = 16 , hidden_size : int = 32 , n_blocks : int = 4 , time_embedding_size = 16 , n_time_features = 8 , cond_shape : Optional [ Tuple [ int ]] = None , * , controller_rtol : Optional [ float ] = 1e-8 , controller_atol : Optional [ float ] = 1e-8 , adjoint = 'recursive_checkpoint' , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The shape of the input to the transformation - `net`: The neural network to use for the vector field. If None, a default network will be used. `net` should accept `net(t, x, y=y)` - `controller_rtol`: The relative tolerance of the stepsize controller. - `controller_atol`: The absolute tolerance of the stepsize controller. - `trace_estimate_likelihood`: Whether to use a trace estimate for the likelihood. - `adjoint`: The adjoint method to use. Can be one of the following: - `\"recursive_checkpoint\"`: Use the recursive checkpoint method. Doesn't support jvp. - `\"direct\"`: Use the direct method. Supports jvps. - `\"seminorm\"`: Use the seminorm method. Does fast backprop through the solver. - `key`: The random key to use for initialization \"\"\" if net is None : net = TimeDependentResNet ( input_shape = input_shape , working_size = working_size , hidden_size = hidden_size , out_size = input_shape [ - 1 ], n_blocks = n_blocks , cond_shape = cond_shape , embedding_size = time_embedding_size , out_features = n_time_features , key = key ) self . neural_ode = NeuralODE ( vf = net , adjoint = adjoint , controller_rtol = controller_rtol , controller_atol = controller_atol ) super () . __init__ ( input_shape = input_shape , ** kwargs ) @property def vector_field ( self ): return self . neural_ode . vector_field def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , log_likelihood : bool = True , trace_estimate_likelihood : Optional [ bool ] = False , save_at : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation - `log_likelihood`: Whether to compute the log likelihood of the transformation - `trace_estimate_likelihood`: Whether to compute a trace estimate of the likelihood of the neural ODE. - `save_at`: The times to save the neural ODE at. - `key`: The random key to use for initialization **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if log_likelihood and ( trace_estimate_likelihood and ( key is None )): raise TypeError ( f 'When using trace estimation, must pass random key' ) if log_likelihood == False : trace_estimate_likelihood = False solution = self . neural_ode ( x , y = y , inverse = inverse , log_likelihood = log_likelihood , trace_estimate_likelihood = trace_estimate_likelihood , save_at = save_at , key = key , ** kwargs ) return solution . ys , solution . log_det","title":"FFJORDTransform"},{"location":"api/flows/ffjord/#generax.nn.neural_ode.NeuralODE","text":"Neural ODE Source code in generax/nn/neural_ode.py class NeuralODE ( eqx . Module ): \"\"\"Neural ODE\"\"\" vector_field : eqx . Module adjoint : diffrax . AbstractAdjoint stepsize_controller : diffrax . AbstractAdaptiveStepSizeController def __init__ ( self , vf : eqx . Module , adjoint : Optional [ str ] = 'recursive_checkpoint' , controller_rtol : Optional [ float ] = 1e-3 , controller_atol : Optional [ float ] = 1e-5 , ): \"\"\"**Arguments**: - `vf`: A function that computes the vector field. It must output a vector of the same shape as its input. - `adjoint`: The adjoint method to use. Can be one of the following: - `\"recursive_checkpoint\"`: Use the recursive checkpoint method. Doesn't support jvp. - `\"direct\"`: Use the direct method. Supports jvps. - `\"seminorm\"`: Use the seminorm method. Does fast backprop through the solver. - `controller_rtol`: The relative tolerance of the stepsize controller. - `controller_atol`: The absolute tolerance of the stepsize controller. \"\"\" self . vector_field = vf if adjoint == 'recursive_checkpoint' : self . adjoint = diffrax . RecursiveCheckpointAdjoint () elif adjoint == 'direct' : self . adjoint = diffrax . DirectAdjoint () elif adjoint == 'seminorm' : adjoint_controller = diffrax . PIDController ( rtol = 1e-3 , atol = 1e-6 , norm = diffrax . adjoint_rms_seminorm ) self . adjoint = diffrax . BacksolveAdjoint ( stepsize_controller = adjoint_controller ) self . stepsize_controller = diffrax . PIDController ( rtol = controller_rtol , atol = controller_atol ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , * , inverse : Optional [ bool ] = False , log_likelihood : Optional [ bool ] = False , trace_estimate_likelihood : Optional [ bool ] = False , save_at : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None , t0 : Optional [ float ] = 0.0 , t1 : Optional [ float ] = 1.0 ) -> Array : \"\"\"**Arguemnts**: - `x`: The input to the neural ODE. Must be a rank 1 array. - `key`: The random number generator key. - `inverse`: Whether to compute the inverse of the neural ODE. `inverse=True` corresponds to going from the base space to the data space. - `log_likelihood`: Whether to compute the log likelihood of the neural ODE. - `trace_estimate_likelihood`: Whether to compute a trace estimate of the likelihood of the neural ODE. - `save_at`: The times to save the neural ODE at. - `key`: The random key to use for initialization - `t0`: The initial time. - `t1`: The final time. **Returns**: - `z`: The output of the neural ODE. - `log_likelihood`: The log likelihood of the neural ODE if `log_likelihood=True`. \"\"\" assert x . shape == self . vector_field . input_shape if trace_estimate_likelihood : # Get a random vector for hutchinsons trace estimator k1 , _ = random . split ( key , 2 ) v = random . normal ( k1 , x . shape ) # Split the model into its static and dynamic parts so that backprop # through the ode solver can be faster. params , static = eqx . partition ( self . vector_field , eqx . is_array ) def f ( t , carry , params ): x , log_det , total_vf_norm , total_jac_frob_norm = carry if inverse == False : # If we're inverting the flow, we need to adjust the time t = t1 - t # Recombine the model model = eqx . combine ( params , static ) # Fill the model with the current time def apply_vf ( x ): return model ( t , x , y = y ) if log_likelihood : if trace_estimate_likelihood : # Hutchinsons trace estimator. See ContinuousNormalizingFlow https://arxiv.org/pdf/1810.01367.pdf dxdt , dudxv = jax . jvp ( apply_vf , ( x ,), ( v ,)) dlogpxdt = - jnp . sum ( dudxv * v ) dtjfndt = jnp . sum ( dudxv ** 2 ) else : # Brute force dlogpx/dt. See NeuralODE https://arxiv.org/pdf/1806.07366.pdf x_flat = x . ravel () eye = jnp . eye ( x_flat . shape [ - 1 ]) x_shape = x . shape def jvp_flat ( x_flat , dx_flat ): x = x_flat . reshape ( x_shape ) dx = dx_flat . reshape ( x_shape ) dxdt , d2dx_dtdx = jax . jvp ( apply_vf , ( x ,), ( dx ,)) return dxdt , d2dx_dtdx . ravel () dxdt , d2dx_dtdx_flat = jax . vmap ( jvp_flat , in_axes = ( None , 0 ))( x_flat , eye ) dxdt = dxdt [ 0 ] dlogpxdt = - jnp . trace ( d2dx_dtdx_flat ) dtjfndt = jnp . sum ( d2dx_dtdx_flat ** 2 ) else : # Don't worry about the log likelihood dxdt = apply_vf ( x ) dlogpxdt = jnp . zeros_like ( log_det ) dtjfndt = jnp . zeros_like ( total_jac_frob_norm ) if inverse == False : # If we're inverting the flow, we need to flip the sign of dxdt dxdt = - dxdt # Accumulate the norm of the vector field dvfnormdt = jnp . sum ( dxdt ** 2 ) if inverse : dlogpxdt = - dlogpxdt return dxdt , dlogpxdt , dvfnormdt , dtjfndt term = diffrax . ODETerm ( f ) solver = diffrax . Dopri5 () # Determine which times we want to save the neural ODE at. if save_at is None : saveat = diffrax . SaveAt ( ts = [ t1 ]) else : saveat = diffrax . SaveAt ( ts = save_at ) log_det = jnp . array ( 0.0 ) total_vf_norm = jnp . array ( 0.0 ) total_jac_frob_norm = jnp . array ( 0.0 ) # Run the ODE solver solution = diffrax . diffeqsolve ( term , solver , saveat = saveat , t0 = t0 , t1 = t1 , dt0 = 0.001 , y0 = ( x , log_det , total_vf_norm , total_jac_frob_norm ), args = params , adjoint = self . adjoint , stepsize_controller = self . stepsize_controller , throw = True ) outs = solution . ys if save_at is None : # Only take the first time outs = jax . tree_util . tree_map ( lambda x : x [ 0 ], outs ) z , log_det , total_vf_norm , total_jac_frob_norm = outs # Construct the new solution kwargs = { f . name : getattr ( solution , f . name ) for f in fields ( solution )} kwargs [ 'ys' ] = z new_solution = NeuralODESolution ( log_det = log_det , total_vf_norm = total_vf_norm , total_jac_frob_norm = total_jac_frob_norm , ** kwargs ) return new_solution","title":"NeuralODE"},{"location":"api/flows/ffjord/#generax.nn.neural_ode.NeuralODESolution","text":"The solution to a neural ODE. This wraps the diffrax solution class and adds the log determinant of the transformation and some other items from http://proceedings.mlr.press/v119/finlay20a/finlay20a.pdf Attributes : log_det : The log determinant of the transformation. total_vf_norm : The total norm of the vector on the path. This can help determine how straight the path is. total_jac_frob_norm : The total norm of the jacobian of the vector field. This Source code in generax/nn/neural_ode.py class NeuralODESolution ( Solution ): \"\"\"The solution to a neural ODE. This wraps the diffrax solution class and adds the log determinant of the transformation and some other items from http://proceedings.mlr.press/v119/finlay20a/finlay20a.pdf **Attributes**: - `log_det`: The log determinant of the transformation. - `total_vf_norm`: The total norm of the vector on the path. This can help determine how straight the path is. - `total_jac_frob_norm`: The total norm of the jacobian of the vector field. This \"\"\" log_det : Array total_vf_norm : Array total_jac_frob_norm : Array","title":"NeuralODESolution"},{"location":"api/flows/nonlinearities/","text":"Nonlinearities \u00a4 generax.flows.nonlinearities.Softplus ( BijectiveTransform ) \u00a4 Softplus( args, *kwargs) Source code in generax/flows/nonlinearities.py class Softplus ( BijectiveTransform ): def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray = None , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == True : x = jnp . where ( x < 0.0 , 1e-5 , x ) dx = jnp . log1p ( - jnp . exp ( - x )) z = x + dx log_det = dx . sum () else : z = jax . nn . softplus ( x ) log_det = jnp . log1p ( - jnp . exp ( - z )) . sum () if inverse : log_det = - log_det return z , log_det __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray = None , ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. key : A jax.random.PRNGKey for initialization Source code in generax/flows/nonlinearities.py def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray = None , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation Returns : (z, log_det) Source code in generax/flows/nonlinearities.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == True : x = jnp . where ( x < 0.0 , 1e-5 , x ) dx = jnp . log1p ( - jnp . exp ( - x )) z = x + dx log_det = dx . sum () else : z = jax . nn . softplus ( x ) log_det = jnp . log1p ( - jnp . exp ( - z )) . sum () if inverse : log_det = - log_det return z , log_det generax.flows.nonlinearities.GaussianCDF ( BijectiveTransform ) \u00a4 GaussianCDF( args, *kwargs) Source code in generax/flows/nonlinearities.py class GaussianCDF ( BijectiveTransform ): def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray = None , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : z = jax . scipy . stats . norm . cdf ( x ) log_det = jax . scipy . stats . norm . logpdf ( x ) . sum () else : z = jax . scipy . stats . norm . ppf ( x ) log_det = jax . scipy . stats . norm . logpdf ( z ) . sum () if inverse : log_det = - log_det return z , log_det __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray = None , ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. key : A jax.random.PRNGKey for initialization Source code in generax/flows/nonlinearities.py def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray = None , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation Returns : (z, log_det) Source code in generax/flows/nonlinearities.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : z = jax . scipy . stats . norm . cdf ( x ) log_det = jax . scipy . stats . norm . logpdf ( x ) . sum () else : z = jax . scipy . stats . norm . ppf ( x ) log_det = jax . scipy . stats . norm . logpdf ( z ) . sum () if inverse : log_det = - log_det return z , log_det generax.flows.nonlinearities.LogisticCDF ( BijectiveTransform ) \u00a4 LogisticCDF( args, *kwargs) Source code in generax/flows/nonlinearities.py class LogisticCDF ( BijectiveTransform ): def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray = None , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : z = jax . scipy . stats . logistic . cdf ( x ) log_det = jax . scipy . stats . logistic . logpdf ( x ) . sum () else : z = jax . scipy . stats . logistic . ppf ( x ) log_det = jax . scipy . stats . logistic . logpdf ( z ) . sum () if inverse : log_det = - log_det return z , log_det __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray = None , ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. key : A jax.random.PRNGKey for initialization Source code in generax/flows/nonlinearities.py def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray = None , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation Returns : (z, log_det) Source code in generax/flows/nonlinearities.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : z = jax . scipy . stats . logistic . cdf ( x ) log_det = jax . scipy . stats . logistic . logpdf ( x ) . sum () else : z = jax . scipy . stats . logistic . ppf ( x ) log_det = jax . scipy . stats . logistic . logpdf ( z ) . sum () if inverse : log_det = - log_det return z , log_det generax.flows.nonlinearities.LeakyReLU ( BijectiveTransform ) \u00a4 LeakyReLU( args, *kwargs) Source code in generax/flows/nonlinearities.py class LeakyReLU ( BijectiveTransform ): alpha : float def __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray = None , alpha : Optional [ float ] = 0.01 , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) self . alpha = alpha def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : z = jnp . where ( x > 0 , x , self . alpha * x ) else : z = jnp . where ( x > 0 , x , x / self . alpha ) log_dx_dz = jnp . where ( x > 0 , 0 , jnp . log ( self . alpha )) log_det = log_dx_dz . sum () if inverse : log_det = - log_det return z , log_det __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray = None , alpha : Optional [ float ] = 0.01 , ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. key : A jax.random.PRNGKey for initialization Source code in generax/flows/nonlinearities.py def __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray = None , alpha : Optional [ float ] = 0.01 , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) self . alpha = alpha __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation Returns : (z, log_det) Source code in generax/flows/nonlinearities.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : z = jnp . where ( x > 0 , x , self . alpha * x ) else : z = jnp . where ( x > 0 , x , x / self . alpha ) log_dx_dz = jnp . where ( x > 0 , 0 , jnp . log ( self . alpha )) log_det = log_dx_dz . sum () if inverse : log_det = - log_det return z , log_det generax.flows.nonlinearities.SneakyReLU ( BijectiveTransform ) \u00a4 Originally from https://invertibleworkshop.github.io/INNF_2019/accepted_papers/pdfs/INNF_2019_paper_26.pdf Source code in generax/flows/nonlinearities.py class SneakyReLU ( BijectiveTransform ): \"\"\" Originally from https://invertibleworkshop.github.io/INNF_2019/accepted_papers/pdfs/INNF_2019_paper_26.pdf \"\"\" alpha : float def __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray = None , alpha : Optional [ float ] = 0.01 , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) # Sneaky ReLU uses a different convention self . alpha = ( 1.0 - alpha ) / ( 1.0 + alpha ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : sqrt_1px2 = jnp . sqrt ( 1 + x ** 2 ) z = ( x + self . alpha * ( sqrt_1px2 - 1 )) / ( 1 + self . alpha ) log_det = jnp . log ( 1 + self . alpha * x / sqrt_1px2 ) - jnp . log ( 1 + self . alpha ) else : alpha_sq = self . alpha ** 2 b = ( 1 + self . alpha ) * x + self . alpha z = ( jnp . sqrt ( alpha_sq * ( 1 + b ** 2 - alpha_sq )) - b ) / ( alpha_sq - 1 ) sqrt_1px2 = jnp . sqrt ( 1 + z ** 2 ) log_det = jnp . log ( 1 + self . alpha * z / sqrt_1px2 ) - jnp . log ( 1 + self . alpha ) log_det = log_det . sum () if inverse : log_det = - log_det return z , log_det __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray = None , alpha : Optional [ float ] = 0.01 , ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. key : A jax.random.PRNGKey for initialization Source code in generax/flows/nonlinearities.py def __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray = None , alpha : Optional [ float ] = 0.01 , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) # Sneaky ReLU uses a different convention self . alpha = ( 1.0 - alpha ) / ( 1.0 + alpha ) __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation Returns : (z, log_det) Source code in generax/flows/nonlinearities.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : sqrt_1px2 = jnp . sqrt ( 1 + x ** 2 ) z = ( x + self . alpha * ( sqrt_1px2 - 1 )) / ( 1 + self . alpha ) log_det = jnp . log ( 1 + self . alpha * x / sqrt_1px2 ) - jnp . log ( 1 + self . alpha ) else : alpha_sq = self . alpha ** 2 b = ( 1 + self . alpha ) * x + self . alpha z = ( jnp . sqrt ( alpha_sq * ( 1 + b ** 2 - alpha_sq )) - b ) / ( alpha_sq - 1 ) sqrt_1px2 = jnp . sqrt ( 1 + z ** 2 ) log_det = jnp . log ( 1 + self . alpha * z / sqrt_1px2 ) - jnp . log ( 1 + self . alpha ) log_det = log_det . sum () if inverse : log_det = - log_det return z , log_det generax.flows.nonlinearities.SquarePlus ( BijectiveTransform ) \u00a4 SquarePlus( args, *kwargs) Source code in generax/flows/nonlinearities.py class SquarePlus ( BijectiveTransform ): gamma : float def __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray = None , gamma : Optional [ float ] = 0.5 , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) self . gamma = gamma def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : sqrt_arg = x ** 2 + 4 * self . gamma z = 0.5 * ( x + jnp . sqrt ( sqrt_arg )) z = jnp . maximum ( z , 0.0 ) dzdx = 0.5 * ( 1 + x * jax . lax . rsqrt ( sqrt_arg )) # Always positive dzdx = jnp . maximum ( dzdx , 1e-5 ) else : z = x - self . gamma / x dzdx = 0.5 * ( 1 + z * jax . lax . rsqrt ( z ** 2 + 4 * self . gamma )) log_det = jnp . log ( dzdx ) . sum () if inverse : log_det = - log_det return z , log_det __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray = None , gamma : Optional [ float ] = 0.5 , ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. key : A jax.random.PRNGKey for initialization Source code in generax/flows/nonlinearities.py def __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray = None , gamma : Optional [ float ] = 0.5 , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) self . gamma = gamma __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation Returns : (z, log_det) Source code in generax/flows/nonlinearities.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : sqrt_arg = x ** 2 + 4 * self . gamma z = 0.5 * ( x + jnp . sqrt ( sqrt_arg )) z = jnp . maximum ( z , 0.0 ) dzdx = 0.5 * ( 1 + x * jax . lax . rsqrt ( sqrt_arg )) # Always positive dzdx = jnp . maximum ( dzdx , 1e-5 ) else : z = x - self . gamma / x dzdx = 0.5 * ( 1 + z * jax . lax . rsqrt ( z ** 2 + 4 * self . gamma )) log_det = jnp . log ( dzdx ) . sum () if inverse : log_det = - log_det return z , log_det generax.flows.nonlinearities.SquareSigmoid ( BijectiveTransform ) \u00a4 SquareSigmoid( args, *kwargs) Source code in generax/flows/nonlinearities.py class SquareSigmoid ( BijectiveTransform ): gamma : float def __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray = None , gamma : Optional [ float ] = 0.5 , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) self . gamma = gamma def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : rsqrt = jax . lax . rsqrt ( x ** 2 + 4 * self . gamma ) z = 0.5 * ( 1 + x * rsqrt ) else : arg = 2 * x - 1 z = 2 * jnp . sqrt ( self . gamma ) * arg * jax . lax . rsqrt ( 1 - arg ** 2 ) rsqrt = jax . lax . rsqrt ( z ** 2 + 4 * self . gamma ) dzdx = 2 * self . gamma * rsqrt ** 3 log_det = jnp . log ( dzdx ) . sum () if inverse : log_det = - log_det return z , log_det __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray = None , gamma : Optional [ float ] = 0.5 , ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. key : A jax.random.PRNGKey for initialization Source code in generax/flows/nonlinearities.py def __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray = None , gamma : Optional [ float ] = 0.5 , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) self . gamma = gamma __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation Returns : (z, log_det) Source code in generax/flows/nonlinearities.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : rsqrt = jax . lax . rsqrt ( x ** 2 + 4 * self . gamma ) z = 0.5 * ( 1 + x * rsqrt ) else : arg = 2 * x - 1 z = 2 * jnp . sqrt ( self . gamma ) * arg * jax . lax . rsqrt ( 1 - arg ** 2 ) rsqrt = jax . lax . rsqrt ( z ** 2 + 4 * self . gamma ) dzdx = 2 * self . gamma * rsqrt ** 3 log_det = jnp . log ( dzdx ) . sum () if inverse : log_det = - log_det return z , log_det generax.flows.nonlinearities.SLog ( BijectiveTransform ) \u00a4 https://papers.nips.cc/paper/2019/file/b1f62fa99de9f27a048344d55c5ef7a6-Paper.pdf Source code in generax/flows/nonlinearities.py class SLog ( BijectiveTransform ): \"\"\" https://papers.nips.cc/paper/2019/file/b1f62fa99de9f27a048344d55c5ef7a6-Paper.pdf \"\"\" alpha : Union [ float , None ] def __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray = None , alpha : Optional [ float ] = 0.0 , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) self . alpha = alpha def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' # Bound alpha to be positive alpha = misc . square_plus ( self . alpha ) + 1e-4 if inverse == False : log_det = jnp . log1p ( alpha * jnp . abs ( x )) z = jnp . sign ( x ) / alpha * log_det else : z = jnp . sign ( x ) / alpha * ( jnp . exp ( alpha * jnp . abs ( x )) - 1 ) log_det = jnp . log1p ( alpha * jnp . abs ( z )) log_det = - log_det . sum () if inverse : log_det = - log_det return z , log_det __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray = None , alpha : Optional [ float ] = 0.0 , ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. key : A jax.random.PRNGKey for initialization Source code in generax/flows/nonlinearities.py def __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray = None , alpha : Optional [ float ] = 0.0 , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) self . alpha = alpha __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation Returns : (z, log_det) Source code in generax/flows/nonlinearities.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' # Bound alpha to be positive alpha = misc . square_plus ( self . alpha ) + 1e-4 if inverse == False : log_det = jnp . log1p ( alpha * jnp . abs ( x )) z = jnp . sign ( x ) / alpha * log_det else : z = jnp . sign ( x ) / alpha * ( jnp . exp ( alpha * jnp . abs ( x )) - 1 ) log_det = jnp . log1p ( alpha * jnp . abs ( z )) log_det = - log_det . sum () if inverse : log_det = - log_det return z , log_det generax.flows.nonlinearities.CartesianToSpherical ( BijectiveTransform ) \u00a4 CartesianToSpherical( args, *kwargs) Source code in generax/flows/nonlinearities.py class CartesianToSpherical ( BijectiveTransform ): def __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray = None , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) def forward_fun ( self , x , eps = 1e-5 ): r = jnp . linalg . norm ( x ) denominators = jnp . sqrt ( jnp . cumsum ( x [:: - 1 ] ** 2 )[:: - 1 ])[: - 1 ] cos_phi = x [: - 1 ] / denominators cos_phi = jnp . maximum ( - 1.0 + eps , cos_phi ) cos_phi = jnp . minimum ( 1.0 - eps , cos_phi ) phi = jnp . arccos ( cos_phi ) last_value = jnp . where ( x [ - 1 ] >= 0 , phi [ - 1 ], 2 * jnp . pi - phi [ - 1 ]) phi = phi . at [ - 1 ] . set ( last_value ) return jnp . concatenate ([ r [ None ], phi ]) def inverse_fun ( self , x ): r = x [: 1 ] phi = x [ 1 :] sin_prod = jnp . cumprod ( jnp . sin ( phi )) first_part = jnp . concatenate ([ jnp . ones ( r . shape ), sin_prod ]) second_part = jnp . concatenate ([ jnp . cos ( phi ), jnp . ones ( r . shape )]) return r * first_part * second_part def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' @partial ( jnp . vectorize , signature = '(n)->(n),()' ) def unbatched_apply ( x ): def _forward ( x ): z = self . forward_fun ( x ) r , phi = z [ 0 ], z [ 1 :] return z , r , phi def _inverse ( x ): z = self . inverse_fun ( x ) r , phi = x [ 0 ], x [ 1 :] return z , r , phi if inverse == False : z , r , phi = _forward ( x ) else : z , r , phi = _inverse ( x ) n = x . shape [ - 1 ] n_range = jnp . arange ( n - 2 , - 1 , - 1 ) log_abs_sin_phi = jnp . log ( jnp . abs ( jnp . sin ( phi ))) log_det = - ( n - 1 ) * jnp . log ( r ) - jnp . sum ( n_range * log_abs_sin_phi , axis =- 1 ) log_det = log_det . sum () if inverse : log_det = - log_det return z , log_det z , log_det = unbatched_apply ( x ) return z , log_det . sum () __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray = None , ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. key : A jax.random.PRNGKey for initialization Source code in generax/flows/nonlinearities.py def __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray = None , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation Returns : (z, log_det) Source code in generax/flows/nonlinearities.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' @partial ( jnp . vectorize , signature = '(n)->(n),()' ) def unbatched_apply ( x ): def _forward ( x ): z = self . forward_fun ( x ) r , phi = z [ 0 ], z [ 1 :] return z , r , phi def _inverse ( x ): z = self . inverse_fun ( x ) r , phi = x [ 0 ], x [ 1 :] return z , r , phi if inverse == False : z , r , phi = _forward ( x ) else : z , r , phi = _inverse ( x ) n = x . shape [ - 1 ] n_range = jnp . arange ( n - 2 , - 1 , - 1 ) log_abs_sin_phi = jnp . log ( jnp . abs ( jnp . sin ( phi ))) log_det = - ( n - 1 ) * jnp . log ( r ) - jnp . sum ( n_range * log_abs_sin_phi , axis =- 1 ) log_det = log_det . sum () if inverse : log_det = - log_det return z , log_det z , log_det = unbatched_apply ( x ) return z , log_det . sum () generax.flows.logistic_cdf_mixture_logit.LogisticCDFMixtureLogit ( BijectiveTransform ) \u00a4 Used in Flow++ https://arxiv.org/pdf/1902.00275.pdf This is a logistic CDF mixture model followed by a logit. Attributes : - theta : The parameters of the transformation. Source code in generax/flows/logistic_cdf_mixture_logit.py class LogisticCDFMixtureLogit ( BijectiveTransform ): \"\"\"Used in Flow++ https://arxiv.org/pdf/1902.00275.pdf This is a logistic CDF mixture model followed by a logit. **Attributes**: - `theta`: The parameters of the transformation. \"\"\" theta : Array K : int = eqx . field ( static = True ) def __init__ ( self , input_shape : Tuple [ int ], K : int = 8 , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization - `K`: The number of knots to use. \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) self . K = K x_dim = util . list_prod ( input_shape ) self . theta = random . normal ( key , shape = ( x_dim * ( 3 * self . K ),)) * 0.1 def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' # Flatten x x = x . ravel () theta = self . theta . reshape ( x . shape + ( 3 * self . K ,)) # Split the parameters weight_logits , means , scales = theta [ ... ,: self . K ], theta [ ... , self . K : 2 * self . K ], theta [ ... , 2 * self . K :] scales = misc . square_plus ( scales , gamma = 1.0 ) + 1e-4 # Create the jvp function that we'll need def f_and_df ( x , * args ): primals = weight_logits , means , scales , x tangents = jax . tree_util . tree_map ( jnp . zeros_like , primals [: - 1 ]) + ( jnp . ones_like ( x ),) return jax . jvp ( logistic_cdf_mixture_logit , primals , tangents ) if inverse == False : # Only need a single pass z , dzdx = f_and_df ( x ) else : # Invert with bisection method. f = lambda x , * args : f_and_df ( x , * args )[ 0 ] lower , upper = - 1000.0 , 1000.0 lower , upper = jnp . broadcast_to ( lower , x . shape ), jnp . broadcast_to ( upper , x . shape ) z = util . bisection ( f , lower , upper , x ) reconstr , dzdx = f_and_df ( z ) ew_log_det = jnp . log ( dzdx ) log_det = ew_log_det . sum () if inverse : log_det *= - 1 # Unflatten the output z = z . reshape ( self . input_shape ) return z , log_det __init__ ( self , input_shape : Tuple [ int ], K : int = 8 , * , key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. key : A jax.random.PRNGKey for initialization K : The number of knots to use. Source code in generax/flows/logistic_cdf_mixture_logit.py def __init__ ( self , input_shape : Tuple [ int ], K : int = 8 , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization - `K`: The number of knots to use. \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) self . K = K x_dim = util . list_prod ( input_shape ) self . theta = random . normal ( key , shape = ( x_dim * ( 3 * self . K ),)) * 0.1 __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation Returns : (z, log_det) Source code in generax/flows/logistic_cdf_mixture_logit.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' # Flatten x x = x . ravel () theta = self . theta . reshape ( x . shape + ( 3 * self . K ,)) # Split the parameters weight_logits , means , scales = theta [ ... ,: self . K ], theta [ ... , self . K : 2 * self . K ], theta [ ... , 2 * self . K :] scales = misc . square_plus ( scales , gamma = 1.0 ) + 1e-4 # Create the jvp function that we'll need def f_and_df ( x , * args ): primals = weight_logits , means , scales , x tangents = jax . tree_util . tree_map ( jnp . zeros_like , primals [: - 1 ]) + ( jnp . ones_like ( x ),) return jax . jvp ( logistic_cdf_mixture_logit , primals , tangents ) if inverse == False : # Only need a single pass z , dzdx = f_and_df ( x ) else : # Invert with bisection method. f = lambda x , * args : f_and_df ( x , * args )[ 0 ] lower , upper = - 1000.0 , 1000.0 lower , upper = jnp . broadcast_to ( lower , x . shape ), jnp . broadcast_to ( upper , x . shape ) z = util . bisection ( f , lower , upper , x ) reconstr , dzdx = f_and_df ( z ) ew_log_det = jnp . log ( dzdx ) log_det = ew_log_det . sum () if inverse : log_det *= - 1 # Unflatten the output z = z . reshape ( self . input_shape ) return z , log_det generax.flows.spline.RationalQuadraticSpline ( BijectiveTransform ) \u00a4 Splines from https://arxiv.org/pdf/1906.04032.pdf. This is the best overall choice to use in flows. Attributes : - theta : The parameters of the spline. Source code in generax/flows/spline.py class RationalQuadraticSpline ( BijectiveTransform ): \"\"\"Splines from https://arxiv.org/pdf/1906.04032.pdf. This is the best overall choice to use in flows. **Attributes**: - `theta`: The parameters of the spline. \"\"\" theta : Array K : int = eqx . field ( static = True ) min_width : float = eqx . field ( static = True ) min_height : float = eqx . field ( static = True ) min_derivative : float = eqx . field ( static = True ) bounds : Sequence [ float ] = eqx . field ( static = True ) def __init__ ( self , input_shape : Tuple [ int ], K : int = 8 , min_width : Optional [ float ] = 1e-3 , min_height : Optional [ float ] = 1e-3 , min_derivative : Optional [ float ] = 1e-8 , bounds : Sequence [ float ] = (( - 10.0 , 10.0 ), ( - 10.0 , 10.0 )), * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization - `K`: The number of knots to use. - `min_width`: The minimum width of the knots. - `min_height`: The minimum height of the knots. - `min_derivative`: The minimum derivative of the knots. - `bounds`: The bounds of the splines. \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) self . K = K self . min_width = min_width self . min_height = min_height self . min_derivative = min_derivative self . bounds = bounds x_dim = util . list_prod ( input_shape ) self . theta = random . normal ( key , shape = ( x_dim * ( 3 * self . K - 1 ),)) * 0.1 def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' # Flatten x x = x . ravel () # Get the parameters settings = self . K , self . min_width , self . min_height , self . min_derivative , self . bounds theta = jnp . broadcast_to ( self . theta , x . shape + self . theta . shape ) knot_x , knot_y , knot_derivs = get_knot_params ( settings , theta ) # The relevant knot depends on if we are inverting or not if inverse == False : mask = ( x > self . bounds [ 0 ][ 0 ] + 1e-5 ) & ( x < self . bounds [ 0 ][ 1 ] - 1e-5 ) apply_fun = forward_spline else : mask = ( x > self . bounds [ 1 ][ 0 ] + 1e-5 ) & ( x < self . bounds [ 1 ][ 1 ] - 1e-5 ) apply_fun = inverse_spline args = find_knots ( x , knot_x , knot_y , knot_derivs , inverse ) z , dzdx = apply_fun ( x , mask , * args ) elementwise_log_det = jnp . log ( dzdx ) log_det = elementwise_log_det . sum () if inverse : log_det = - log_det # Unflatten the output z = z . reshape ( self . input_shape ) return z , log_det __init__ ( self , input_shape : Tuple [ int ], K : int = 8 , min_width : Optional [ float ] = 0.001 , min_height : Optional [ float ] = 0.001 , min_derivative : Optional [ float ] = 1e-08 , bounds : Sequence [ float ] = (( - 10.0 , 10.0 ), ( - 10.0 , 10.0 )), * , key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. key : A jax.random.PRNGKey for initialization K : The number of knots to use. min_width : The minimum width of the knots. min_height : The minimum height of the knots. min_derivative : The minimum derivative of the knots. bounds : The bounds of the splines. Source code in generax/flows/spline.py def __init__ ( self , input_shape : Tuple [ int ], K : int = 8 , min_width : Optional [ float ] = 1e-3 , min_height : Optional [ float ] = 1e-3 , min_derivative : Optional [ float ] = 1e-8 , bounds : Sequence [ float ] = (( - 10.0 , 10.0 ), ( - 10.0 , 10.0 )), * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization - `K`: The number of knots to use. - `min_width`: The minimum width of the knots. - `min_height`: The minimum height of the knots. - `min_derivative`: The minimum derivative of the knots. - `bounds`: The bounds of the splines. \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) self . K = K self . min_width = min_width self . min_height = min_height self . min_derivative = min_derivative self . bounds = bounds x_dim = util . list_prod ( input_shape ) self . theta = random . normal ( key , shape = ( x_dim * ( 3 * self . K - 1 ),)) * 0.1 __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation Returns : (z, log_det) Source code in generax/flows/spline.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' # Flatten x x = x . ravel () # Get the parameters settings = self . K , self . min_width , self . min_height , self . min_derivative , self . bounds theta = jnp . broadcast_to ( self . theta , x . shape + self . theta . shape ) knot_x , knot_y , knot_derivs = get_knot_params ( settings , theta ) # The relevant knot depends on if we are inverting or not if inverse == False : mask = ( x > self . bounds [ 0 ][ 0 ] + 1e-5 ) & ( x < self . bounds [ 0 ][ 1 ] - 1e-5 ) apply_fun = forward_spline else : mask = ( x > self . bounds [ 1 ][ 0 ] + 1e-5 ) & ( x < self . bounds [ 1 ][ 1 ] - 1e-5 ) apply_fun = inverse_spline args = find_knots ( x , knot_x , knot_y , knot_derivs , inverse ) z , dzdx = apply_fun ( x , mask , * args ) elementwise_log_det = jnp . log ( dzdx ) log_det = elementwise_log_det . sum () if inverse : log_det = - log_det # Unflatten the output z = z . reshape ( self . input_shape ) return z , log_det","title":"Nonlinearities"},{"location":"api/flows/nonlinearities/#nonlinearities","text":"","title":"Nonlinearities"},{"location":"api/flows/nonlinearities/#generax.flows.nonlinearities.Softplus","text":"Softplus( args, *kwargs) Source code in generax/flows/nonlinearities.py class Softplus ( BijectiveTransform ): def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray = None , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == True : x = jnp . where ( x < 0.0 , 1e-5 , x ) dx = jnp . log1p ( - jnp . exp ( - x )) z = x + dx log_det = dx . sum () else : z = jax . nn . softplus ( x ) log_det = jnp . log1p ( - jnp . exp ( - z )) . sum () if inverse : log_det = - log_det return z , log_det","title":"Softplus"},{"location":"api/flows/nonlinearities/#generax.flows.nonlinearities.GaussianCDF","text":"GaussianCDF( args, *kwargs) Source code in generax/flows/nonlinearities.py class GaussianCDF ( BijectiveTransform ): def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray = None , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : z = jax . scipy . stats . norm . cdf ( x ) log_det = jax . scipy . stats . norm . logpdf ( x ) . sum () else : z = jax . scipy . stats . norm . ppf ( x ) log_det = jax . scipy . stats . norm . logpdf ( z ) . sum () if inverse : log_det = - log_det return z , log_det","title":"GaussianCDF"},{"location":"api/flows/nonlinearities/#generax.flows.nonlinearities.LogisticCDF","text":"LogisticCDF( args, *kwargs) Source code in generax/flows/nonlinearities.py class LogisticCDF ( BijectiveTransform ): def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray = None , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : z = jax . scipy . stats . logistic . cdf ( x ) log_det = jax . scipy . stats . logistic . logpdf ( x ) . sum () else : z = jax . scipy . stats . logistic . ppf ( x ) log_det = jax . scipy . stats . logistic . logpdf ( z ) . sum () if inverse : log_det = - log_det return z , log_det","title":"LogisticCDF"},{"location":"api/flows/nonlinearities/#generax.flows.nonlinearities.LeakyReLU","text":"LeakyReLU( args, *kwargs) Source code in generax/flows/nonlinearities.py class LeakyReLU ( BijectiveTransform ): alpha : float def __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray = None , alpha : Optional [ float ] = 0.01 , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) self . alpha = alpha def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : z = jnp . where ( x > 0 , x , self . alpha * x ) else : z = jnp . where ( x > 0 , x , x / self . alpha ) log_dx_dz = jnp . where ( x > 0 , 0 , jnp . log ( self . alpha )) log_det = log_dx_dz . sum () if inverse : log_det = - log_det return z , log_det","title":"LeakyReLU"},{"location":"api/flows/nonlinearities/#generax.flows.nonlinearities.SneakyReLU","text":"Originally from https://invertibleworkshop.github.io/INNF_2019/accepted_papers/pdfs/INNF_2019_paper_26.pdf Source code in generax/flows/nonlinearities.py class SneakyReLU ( BijectiveTransform ): \"\"\" Originally from https://invertibleworkshop.github.io/INNF_2019/accepted_papers/pdfs/INNF_2019_paper_26.pdf \"\"\" alpha : float def __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray = None , alpha : Optional [ float ] = 0.01 , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) # Sneaky ReLU uses a different convention self . alpha = ( 1.0 - alpha ) / ( 1.0 + alpha ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : sqrt_1px2 = jnp . sqrt ( 1 + x ** 2 ) z = ( x + self . alpha * ( sqrt_1px2 - 1 )) / ( 1 + self . alpha ) log_det = jnp . log ( 1 + self . alpha * x / sqrt_1px2 ) - jnp . log ( 1 + self . alpha ) else : alpha_sq = self . alpha ** 2 b = ( 1 + self . alpha ) * x + self . alpha z = ( jnp . sqrt ( alpha_sq * ( 1 + b ** 2 - alpha_sq )) - b ) / ( alpha_sq - 1 ) sqrt_1px2 = jnp . sqrt ( 1 + z ** 2 ) log_det = jnp . log ( 1 + self . alpha * z / sqrt_1px2 ) - jnp . log ( 1 + self . alpha ) log_det = log_det . sum () if inverse : log_det = - log_det return z , log_det","title":"SneakyReLU"},{"location":"api/flows/nonlinearities/#generax.flows.nonlinearities.SquarePlus","text":"SquarePlus( args, *kwargs) Source code in generax/flows/nonlinearities.py class SquarePlus ( BijectiveTransform ): gamma : float def __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray = None , gamma : Optional [ float ] = 0.5 , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) self . gamma = gamma def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : sqrt_arg = x ** 2 + 4 * self . gamma z = 0.5 * ( x + jnp . sqrt ( sqrt_arg )) z = jnp . maximum ( z , 0.0 ) dzdx = 0.5 * ( 1 + x * jax . lax . rsqrt ( sqrt_arg )) # Always positive dzdx = jnp . maximum ( dzdx , 1e-5 ) else : z = x - self . gamma / x dzdx = 0.5 * ( 1 + z * jax . lax . rsqrt ( z ** 2 + 4 * self . gamma )) log_det = jnp . log ( dzdx ) . sum () if inverse : log_det = - log_det return z , log_det","title":"SquarePlus"},{"location":"api/flows/nonlinearities/#generax.flows.nonlinearities.SquareSigmoid","text":"SquareSigmoid( args, *kwargs) Source code in generax/flows/nonlinearities.py class SquareSigmoid ( BijectiveTransform ): gamma : float def __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray = None , gamma : Optional [ float ] = 0.5 , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) self . gamma = gamma def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : rsqrt = jax . lax . rsqrt ( x ** 2 + 4 * self . gamma ) z = 0.5 * ( 1 + x * rsqrt ) else : arg = 2 * x - 1 z = 2 * jnp . sqrt ( self . gamma ) * arg * jax . lax . rsqrt ( 1 - arg ** 2 ) rsqrt = jax . lax . rsqrt ( z ** 2 + 4 * self . gamma ) dzdx = 2 * self . gamma * rsqrt ** 3 log_det = jnp . log ( dzdx ) . sum () if inverse : log_det = - log_det return z , log_det","title":"SquareSigmoid"},{"location":"api/flows/nonlinearities/#generax.flows.nonlinearities.SLog","text":"https://papers.nips.cc/paper/2019/file/b1f62fa99de9f27a048344d55c5ef7a6-Paper.pdf Source code in generax/flows/nonlinearities.py class SLog ( BijectiveTransform ): \"\"\" https://papers.nips.cc/paper/2019/file/b1f62fa99de9f27a048344d55c5ef7a6-Paper.pdf \"\"\" alpha : Union [ float , None ] def __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray = None , alpha : Optional [ float ] = 0.0 , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) self . alpha = alpha def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' # Bound alpha to be positive alpha = misc . square_plus ( self . alpha ) + 1e-4 if inverse == False : log_det = jnp . log1p ( alpha * jnp . abs ( x )) z = jnp . sign ( x ) / alpha * log_det else : z = jnp . sign ( x ) / alpha * ( jnp . exp ( alpha * jnp . abs ( x )) - 1 ) log_det = jnp . log1p ( alpha * jnp . abs ( z )) log_det = - log_det . sum () if inverse : log_det = - log_det return z , log_det","title":"SLog"},{"location":"api/flows/nonlinearities/#generax.flows.nonlinearities.CartesianToSpherical","text":"CartesianToSpherical( args, *kwargs) Source code in generax/flows/nonlinearities.py class CartesianToSpherical ( BijectiveTransform ): def __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray = None , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) def forward_fun ( self , x , eps = 1e-5 ): r = jnp . linalg . norm ( x ) denominators = jnp . sqrt ( jnp . cumsum ( x [:: - 1 ] ** 2 )[:: - 1 ])[: - 1 ] cos_phi = x [: - 1 ] / denominators cos_phi = jnp . maximum ( - 1.0 + eps , cos_phi ) cos_phi = jnp . minimum ( 1.0 - eps , cos_phi ) phi = jnp . arccos ( cos_phi ) last_value = jnp . where ( x [ - 1 ] >= 0 , phi [ - 1 ], 2 * jnp . pi - phi [ - 1 ]) phi = phi . at [ - 1 ] . set ( last_value ) return jnp . concatenate ([ r [ None ], phi ]) def inverse_fun ( self , x ): r = x [: 1 ] phi = x [ 1 :] sin_prod = jnp . cumprod ( jnp . sin ( phi )) first_part = jnp . concatenate ([ jnp . ones ( r . shape ), sin_prod ]) second_part = jnp . concatenate ([ jnp . cos ( phi ), jnp . ones ( r . shape )]) return r * first_part * second_part def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' @partial ( jnp . vectorize , signature = '(n)->(n),()' ) def unbatched_apply ( x ): def _forward ( x ): z = self . forward_fun ( x ) r , phi = z [ 0 ], z [ 1 :] return z , r , phi def _inverse ( x ): z = self . inverse_fun ( x ) r , phi = x [ 0 ], x [ 1 :] return z , r , phi if inverse == False : z , r , phi = _forward ( x ) else : z , r , phi = _inverse ( x ) n = x . shape [ - 1 ] n_range = jnp . arange ( n - 2 , - 1 , - 1 ) log_abs_sin_phi = jnp . log ( jnp . abs ( jnp . sin ( phi ))) log_det = - ( n - 1 ) * jnp . log ( r ) - jnp . sum ( n_range * log_abs_sin_phi , axis =- 1 ) log_det = log_det . sum () if inverse : log_det = - log_det return z , log_det z , log_det = unbatched_apply ( x ) return z , log_det . sum ()","title":"CartesianToSpherical"},{"location":"api/flows/nonlinearities/#generax.flows.logistic_cdf_mixture_logit.LogisticCDFMixtureLogit","text":"Used in Flow++ https://arxiv.org/pdf/1902.00275.pdf This is a logistic CDF mixture model followed by a logit. Attributes : - theta : The parameters of the transformation. Source code in generax/flows/logistic_cdf_mixture_logit.py class LogisticCDFMixtureLogit ( BijectiveTransform ): \"\"\"Used in Flow++ https://arxiv.org/pdf/1902.00275.pdf This is a logistic CDF mixture model followed by a logit. **Attributes**: - `theta`: The parameters of the transformation. \"\"\" theta : Array K : int = eqx . field ( static = True ) def __init__ ( self , input_shape : Tuple [ int ], K : int = 8 , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization - `K`: The number of knots to use. \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) self . K = K x_dim = util . list_prod ( input_shape ) self . theta = random . normal ( key , shape = ( x_dim * ( 3 * self . K ),)) * 0.1 def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' # Flatten x x = x . ravel () theta = self . theta . reshape ( x . shape + ( 3 * self . K ,)) # Split the parameters weight_logits , means , scales = theta [ ... ,: self . K ], theta [ ... , self . K : 2 * self . K ], theta [ ... , 2 * self . K :] scales = misc . square_plus ( scales , gamma = 1.0 ) + 1e-4 # Create the jvp function that we'll need def f_and_df ( x , * args ): primals = weight_logits , means , scales , x tangents = jax . tree_util . tree_map ( jnp . zeros_like , primals [: - 1 ]) + ( jnp . ones_like ( x ),) return jax . jvp ( logistic_cdf_mixture_logit , primals , tangents ) if inverse == False : # Only need a single pass z , dzdx = f_and_df ( x ) else : # Invert with bisection method. f = lambda x , * args : f_and_df ( x , * args )[ 0 ] lower , upper = - 1000.0 , 1000.0 lower , upper = jnp . broadcast_to ( lower , x . shape ), jnp . broadcast_to ( upper , x . shape ) z = util . bisection ( f , lower , upper , x ) reconstr , dzdx = f_and_df ( z ) ew_log_det = jnp . log ( dzdx ) log_det = ew_log_det . sum () if inverse : log_det *= - 1 # Unflatten the output z = z . reshape ( self . input_shape ) return z , log_det","title":"LogisticCDFMixtureLogit"},{"location":"api/flows/nonlinearities/#generax.flows.spline.RationalQuadraticSpline","text":"Splines from https://arxiv.org/pdf/1906.04032.pdf. This is the best overall choice to use in flows. Attributes : - theta : The parameters of the spline. Source code in generax/flows/spline.py class RationalQuadraticSpline ( BijectiveTransform ): \"\"\"Splines from https://arxiv.org/pdf/1906.04032.pdf. This is the best overall choice to use in flows. **Attributes**: - `theta`: The parameters of the spline. \"\"\" theta : Array K : int = eqx . field ( static = True ) min_width : float = eqx . field ( static = True ) min_height : float = eqx . field ( static = True ) min_derivative : float = eqx . field ( static = True ) bounds : Sequence [ float ] = eqx . field ( static = True ) def __init__ ( self , input_shape : Tuple [ int ], K : int = 8 , min_width : Optional [ float ] = 1e-3 , min_height : Optional [ float ] = 1e-3 , min_derivative : Optional [ float ] = 1e-8 , bounds : Sequence [ float ] = (( - 10.0 , 10.0 ), ( - 10.0 , 10.0 )), * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization - `K`: The number of knots to use. - `min_width`: The minimum width of the knots. - `min_height`: The minimum height of the knots. - `min_derivative`: The minimum derivative of the knots. - `bounds`: The bounds of the splines. \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) self . K = K self . min_width = min_width self . min_height = min_height self . min_derivative = min_derivative self . bounds = bounds x_dim = util . list_prod ( input_shape ) self . theta = random . normal ( key , shape = ( x_dim * ( 3 * self . K - 1 ),)) * 0.1 def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' # Flatten x x = x . ravel () # Get the parameters settings = self . K , self . min_width , self . min_height , self . min_derivative , self . bounds theta = jnp . broadcast_to ( self . theta , x . shape + self . theta . shape ) knot_x , knot_y , knot_derivs = get_knot_params ( settings , theta ) # The relevant knot depends on if we are inverting or not if inverse == False : mask = ( x > self . bounds [ 0 ][ 0 ] + 1e-5 ) & ( x < self . bounds [ 0 ][ 1 ] - 1e-5 ) apply_fun = forward_spline else : mask = ( x > self . bounds [ 1 ][ 0 ] + 1e-5 ) & ( x < self . bounds [ 1 ][ 1 ] - 1e-5 ) apply_fun = inverse_spline args = find_knots ( x , knot_x , knot_y , knot_derivs , inverse ) z , dzdx = apply_fun ( x , mask , * args ) elementwise_log_det = jnp . log ( dzdx ) log_det = elementwise_log_det . sum () if inverse : log_det = - log_det # Unflatten the output z = z . reshape ( self . input_shape ) return z , log_det","title":"RationalQuadraticSpline"},{"location":"api/flows/reshape/","text":"Reshape \u00a4 generax.flows.reshape.Flatten ( BijectiveTransform ) \u00a4 Flatten Source code in generax/flows/reshape.py class Flatten ( BijectiveTransform ): \"\"\"Flatten \"\"\" def __init__ ( self , input_shape : Tuple [ int ], ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: The transformed input and 0 \"\"\" log_det = jnp . array ( 0.0 ) if inverse == False : assert x . shape == self . input_shape return x . ravel (), log_det else : return x . reshape ( self . input_shape ), log_det __init__ ( self , input_shape : Tuple [ int ], ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. key : A jax.random.PRNGKey for initialization Source code in generax/flows/reshape.py def __init__ ( self , input_shape : Tuple [ int ], ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation Returns : The transformed input and 0 Source code in generax/flows/reshape.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: The transformed input and 0 \"\"\" log_det = jnp . array ( 0.0 ) if inverse == False : assert x . shape == self . input_shape return x . ravel (), log_det else : return x . reshape ( self . input_shape ), log_det data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) \u00a4 Inherited from generax.flows.base.BijectiveTransform.data_dependent_init . Source code in generax/flows/reshape.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ): \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" return self inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Inherited from generax.flows.base.BijectiveTransform.inverse . Source code in generax/flows/reshape.py def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs ) generax.flows.reshape.Reverse ( BijectiveTransform ) \u00a4 Reverse an input Source code in generax/flows/reshape.py class Reverse ( BijectiveTransform ): \"\"\"Reverse an input \"\"\" def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: The transformed input and 0 \"\"\" assert x . shape == self . input_shape z = x [ ... , :: - 1 ] log_det = jnp . array ( 0.0 ) return z , log_det data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) \u00a4 Inherited from generax.flows.base.BijectiveTransform.data_dependent_init . Source code in generax/flows/reshape.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ): \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" return self __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. key : A jax.random.PRNGKey for initialization Source code in generax/flows/reshape.py def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation Returns : The transformed input and 0 Source code in generax/flows/reshape.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: The transformed input and 0 \"\"\" assert x . shape == self . input_shape z = x [ ... , :: - 1 ] log_det = jnp . array ( 0.0 ) return z , log_det inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Inherited from generax.flows.base.BijectiveTransform.inverse . Source code in generax/flows/reshape.py def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs ) generax.flows.reshape.Checkerboard ( BijectiveTransform ) \u00a4 Checkerboard pattern from https://arxiv.org/pdf/1605.08803.pdf Source code in generax/flows/reshape.py class Checkerboard ( BijectiveTransform ): \"\"\"Checkerboard pattern from https://arxiv.org/pdf/1605.08803.pdf\"\"\" output_shape : Tuple [ int ] = eqx . field ( static = True ) def __init__ ( self , input_shape : Tuple [ int ], ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. \"\"\" H , W , C = input_shape assert W % 2 == 0 , 'Need even width' super () . __init__ ( input_shape = input_shape , ** kwargs ) self . output_shape = ( H , W // 2 , C * 2 ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: The transformed input and 0 \"\"\" if inverse == False : assert x . shape == self . input_shape z = einops . rearrange ( x , 'h (w k) c -> h w (c k)' , k = 2 ) else : assert x . shape == self . output_shape z = einops . rearrange ( x , 'h w (c k) -> h (w k) c' , k = 2 ) log_det = jnp . array ( 0.0 ) return z , log_det data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) \u00a4 Inherited from generax.flows.base.BijectiveTransform.data_dependent_init . Source code in generax/flows/reshape.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ): \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" return self __init__ ( self , input_shape : Tuple [ int ], ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. Source code in generax/flows/reshape.py def __init__ ( self , input_shape : Tuple [ int ], ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. \"\"\" H , W , C = input_shape assert W % 2 == 0 , 'Need even width' super () . __init__ ( input_shape = input_shape , ** kwargs ) self . output_shape = ( H , W // 2 , C * 2 ) __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation Returns : The transformed input and 0 Source code in generax/flows/reshape.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: The transformed input and 0 \"\"\" if inverse == False : assert x . shape == self . input_shape z = einops . rearrange ( x , 'h (w k) c -> h w (c k)' , k = 2 ) else : assert x . shape == self . output_shape z = einops . rearrange ( x , 'h w (c k) -> h (w k) c' , k = 2 ) log_det = jnp . array ( 0.0 ) return z , log_det inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Inherited from generax.flows.base.BijectiveTransform.inverse . Source code in generax/flows/reshape.py def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs ) generax.flows.reshape.Squeeze ( BijectiveTransform ) \u00a4 Space to depth. (H, W, C) -> (H//2, W//2, C*4) Source code in generax/flows/reshape.py class Squeeze ( BijectiveTransform ): \"\"\"Space to depth. (H, W, C) -> (H//2, W//2, C*4)\"\"\" output_shape : Tuple [ int ] = eqx . field ( static = True ) def __init__ ( self , input_shape : Tuple [ int ], ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" H , W , C = input_shape assert H % 2 == 0 , 'Need even height' assert W % 2 == 0 , 'Need even width' super () . __init__ ( input_shape = input_shape , ** kwargs ) self . output_shape = ( H // 2 , W // 2 , C * 4 ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: The transformed input and 0 \"\"\" if inverse == False : assert x . shape == self . input_shape z = einops . rearrange ( x , '(h m) (w n) c -> h w (c m n)' , m = 2 , n = 2 ) else : assert x . shape == self . output_shape z = einops . rearrange ( x , 'h w (c m n) -> (h m) (w n) c' , m = 2 , n = 2 ) log_det = jnp . array ( 0.0 ) return z , log_det data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) \u00a4 Inherited from generax.flows.base.BijectiveTransform.data_dependent_init . Source code in generax/flows/reshape.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ): \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" return self inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Inherited from generax.flows.base.BijectiveTransform.inverse . Source code in generax/flows/reshape.py def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs ) __init__ ( self , input_shape : Tuple [ int ], ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. key : A jax.random.PRNGKey for initialization Source code in generax/flows/reshape.py def __init__ ( self , input_shape : Tuple [ int ], ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" H , W , C = input_shape assert H % 2 == 0 , 'Need even height' assert W % 2 == 0 , 'Need even width' super () . __init__ ( input_shape = input_shape , ** kwargs ) self . output_shape = ( H // 2 , W // 2 , C * 4 ) __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation Returns : The transformed input and 0 Source code in generax/flows/reshape.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: The transformed input and 0 \"\"\" if inverse == False : assert x . shape == self . input_shape z = einops . rearrange ( x , '(h m) (w n) c -> h w (c m n)' , m = 2 , n = 2 ) else : assert x . shape == self . output_shape z = einops . rearrange ( x , 'h w (c m n) -> (h m) (w n) c' , m = 2 , n = 2 ) log_det = jnp . array ( 0.0 ) return z , log_det generax.flows.reshape.Slice ( InjectiveTransform ) \u00a4 Slice an input to reduce the dimension Source code in generax/flows/reshape.py class Slice ( InjectiveTransform ): \"\"\"Slice an input to reduce the dimension \"\"\" def __init__ ( self , input_shape : Tuple [ int ], output_shape : Tuple [ int ], ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" assert input_shape [: - 1 ] == output_shape [: - 1 ], 'Need to keep the same spatial dimensions' super () . __init__ ( input_shape = input_shape , output_shape = output_shape , ** kwargs ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" if inverse == False : assert x . shape == self . input_shape , 'Only works on unbatched data' else : assert x . shape == self . output_shape , 'Only works on unbatched data' if inverse == False : z = x [ ... , : self . output_shape [ - 1 ]] else : pad_shape = self . input_shape [: - 1 ] + ( self . input_shape [ - 1 ] - self . output_shape [ - 1 ],) z = jnp . concatenate ([ x , jnp . zeros ( pad_shape )], axis =- 1 ) log_det = jnp . array ( 0.0 ) return z , log_det def log_determinant ( self , z : Array , ** kwargs ) -> Array : \"\"\"Compute -0.5*log(det(J^TJ)) **Arguments**: - `z`: An element of the base space **Returns**: The log determinant of (J^TJ)^0.5 \"\"\" return jnp . array ( 0.0 ) data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) \u00a4 Initialize the parameters of the layer based on the data. Arguments : x : The data to initialize the parameters with. y : The conditioning information key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/flows/reshape.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ): \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" return self inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Apply the inverse transformation. Arguments : x : The input to the transformation y : The conditioning information Returns : (z, log_det) Source code in generax/flows/reshape.py def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs ) __init__ ( self , input_shape : Tuple [ int ], output_shape : Tuple [ int ], ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. key : A jax.random.PRNGKey for initialization Source code in generax/flows/reshape.py def __init__ ( self , input_shape : Tuple [ int ], output_shape : Tuple [ int ], ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" assert input_shape [: - 1 ] == output_shape [: - 1 ], 'Need to keep the same spatial dimensions' super () . __init__ ( input_shape = input_shape , output_shape = output_shape , ** kwargs ) __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation Returns : (z, log_det) Source code in generax/flows/reshape.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" if inverse == False : assert x . shape == self . input_shape , 'Only works on unbatched data' else : assert x . shape == self . output_shape , 'Only works on unbatched data' if inverse == False : z = x [ ... , : self . output_shape [ - 1 ]] else : pad_shape = self . input_shape [: - 1 ] + ( self . input_shape [ - 1 ] - self . output_shape [ - 1 ],) z = jnp . concatenate ([ x , jnp . zeros ( pad_shape )], axis =- 1 ) log_det = jnp . array ( 0.0 ) return z , log_det project ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Inherited from generax.flows.base.InjectiveTransform.project . Source code in generax/flows/reshape.py def project ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Project a point onto the image of the transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: z \"\"\" z , _ = self ( x , y = y , ** kwargs ) x_proj , _ = self ( z , y = y , inverse = True , ** kwargs ) return x_proj","title":"Reshape"},{"location":"api/flows/reshape/#reshape","text":"","title":"Reshape"},{"location":"api/flows/reshape/#generax.flows.reshape.Flatten","text":"Flatten Source code in generax/flows/reshape.py class Flatten ( BijectiveTransform ): \"\"\"Flatten \"\"\" def __init__ ( self , input_shape : Tuple [ int ], ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: The transformed input and 0 \"\"\" log_det = jnp . array ( 0.0 ) if inverse == False : assert x . shape == self . input_shape return x . ravel (), log_det else : return x . reshape ( self . input_shape ), log_det","title":"Flatten"},{"location":"api/flows/reshape/#generax.flows.reshape.Reverse","text":"Reverse an input Source code in generax/flows/reshape.py class Reverse ( BijectiveTransform ): \"\"\"Reverse an input \"\"\" def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: The transformed input and 0 \"\"\" assert x . shape == self . input_shape z = x [ ... , :: - 1 ] log_det = jnp . array ( 0.0 ) return z , log_det","title":"Reverse"},{"location":"api/flows/reshape/#generax.flows.reshape.Checkerboard","text":"Checkerboard pattern from https://arxiv.org/pdf/1605.08803.pdf Source code in generax/flows/reshape.py class Checkerboard ( BijectiveTransform ): \"\"\"Checkerboard pattern from https://arxiv.org/pdf/1605.08803.pdf\"\"\" output_shape : Tuple [ int ] = eqx . field ( static = True ) def __init__ ( self , input_shape : Tuple [ int ], ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. \"\"\" H , W , C = input_shape assert W % 2 == 0 , 'Need even width' super () . __init__ ( input_shape = input_shape , ** kwargs ) self . output_shape = ( H , W // 2 , C * 2 ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: The transformed input and 0 \"\"\" if inverse == False : assert x . shape == self . input_shape z = einops . rearrange ( x , 'h (w k) c -> h w (c k)' , k = 2 ) else : assert x . shape == self . output_shape z = einops . rearrange ( x , 'h w (c k) -> h (w k) c' , k = 2 ) log_det = jnp . array ( 0.0 ) return z , log_det","title":"Checkerboard"},{"location":"api/flows/reshape/#generax.flows.reshape.Squeeze","text":"Space to depth. (H, W, C) -> (H//2, W//2, C*4) Source code in generax/flows/reshape.py class Squeeze ( BijectiveTransform ): \"\"\"Space to depth. (H, W, C) -> (H//2, W//2, C*4)\"\"\" output_shape : Tuple [ int ] = eqx . field ( static = True ) def __init__ ( self , input_shape : Tuple [ int ], ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" H , W , C = input_shape assert H % 2 == 0 , 'Need even height' assert W % 2 == 0 , 'Need even width' super () . __init__ ( input_shape = input_shape , ** kwargs ) self . output_shape = ( H // 2 , W // 2 , C * 4 ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: The transformed input and 0 \"\"\" if inverse == False : assert x . shape == self . input_shape z = einops . rearrange ( x , '(h m) (w n) c -> h w (c m n)' , m = 2 , n = 2 ) else : assert x . shape == self . output_shape z = einops . rearrange ( x , 'h w (c m n) -> (h m) (w n) c' , m = 2 , n = 2 ) log_det = jnp . array ( 0.0 ) return z , log_det","title":"Squeeze"},{"location":"api/flows/reshape/#generax.flows.reshape.Slice","text":"Slice an input to reduce the dimension Source code in generax/flows/reshape.py class Slice ( InjectiveTransform ): \"\"\"Slice an input to reduce the dimension \"\"\" def __init__ ( self , input_shape : Tuple [ int ], output_shape : Tuple [ int ], ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" assert input_shape [: - 1 ] == output_shape [: - 1 ], 'Need to keep the same spatial dimensions' super () . __init__ ( input_shape = input_shape , output_shape = output_shape , ** kwargs ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" if inverse == False : assert x . shape == self . input_shape , 'Only works on unbatched data' else : assert x . shape == self . output_shape , 'Only works on unbatched data' if inverse == False : z = x [ ... , : self . output_shape [ - 1 ]] else : pad_shape = self . input_shape [: - 1 ] + ( self . input_shape [ - 1 ] - self . output_shape [ - 1 ],) z = jnp . concatenate ([ x , jnp . zeros ( pad_shape )], axis =- 1 ) log_det = jnp . array ( 0.0 ) return z , log_det def log_determinant ( self , z : Array , ** kwargs ) -> Array : \"\"\"Compute -0.5*log(det(J^TJ)) **Arguments**: - `z`: An element of the base space **Returns**: The log determinant of (J^TJ)^0.5 \"\"\" return jnp . array ( 0.0 )","title":"Slice"},{"location":"api/misc/trainer/","text":"Trainer \u00a4 generax.trainer.TrainingState \u00a4 TrainingState( args, *kwargs) Source code in generax/trainer.py class TrainingState ( eqx . Module ): i : float key : PRNGKeyArray model : eqx . Module opt_state : optax . OptState def __init__ ( self , i : float , # float so that it is not treated as static key : PRNGKeyArray , model : eqx . Module , opt_state : optax . OptState ): self . i = i self . key = key self . model = model self . opt_state = opt_state __init__ ( self , i : float , key : PRNGKeyArray , model : Module , opt_state : Union [ jax . Array , numpy . ndarray , numpy . bool_ , numpy . number , Iterable [ ArrayTree ], Mapping [ Any , ArrayTree ]]) \u00a4 Source code in generax/trainer.py def __init__ ( self , i : float , # float so that it is not treated as static key : PRNGKeyArray , model : eqx . Module , opt_state : optax . OptState ): self . i = i self . key = key self . model = model self . opt_state = opt_state generax.trainer.Checkpointer \u00a4 Checkpointer( args, *kwargs) Source code in generax/trainer.py class Checkpointer ( eqx . Module ): save_path : str model_folder : str def __init__ ( self , save_path : str ): self . save_path = save_path self . model_folder = os . path . join ( save_path , 'models' ) misc . ensure_path_exists ( self . model_folder ) @property def saved_model_path ( self ): return os . path . join ( self . model_folder , 'saved_model.pickle' ) def save_eqx_module ( self , model : eqx . Module ): eqx . tree_serialise_leaves ( self . saved_model_path , model ) def load_eqx_module ( self , model_example : eqx . Module ): return eqx . tree_deserialise_leaves ( self . saved_model_path , model_example ) __init__ ( self , save_path : str ) \u00a4 Source code in generax/trainer.py def __init__ ( self , save_path : str ): self . save_path = save_path self . model_folder = os . path . join ( save_path , 'models' ) misc . ensure_path_exists ( self . model_folder ) save_eqx_module ( self , model : Module ) \u00a4 Source code in generax/trainer.py def save_eqx_module ( self , model : eqx . Module ): eqx . tree_serialise_leaves ( self . saved_model_path , model ) load_eqx_module ( self , model_example : Module ) \u00a4 Source code in generax/trainer.py def load_eqx_module ( self , model_example : eqx . Module ): return eqx . tree_deserialise_leaves ( self . saved_model_path , model_example ) generax.trainer.Trainer \u00a4 Class that will monitor training and handle checkpointing. Attributes : checkpointer : Object that saves checkpoints of the model Source code in generax/trainer.py class Trainer ( eqx . Module ): \"\"\"Class that will monitor training and handle checkpointing. **Attributes**: - `checkpointer`: Object that saves checkpoints of the model \"\"\" checkpointer : Checkpointer _aux_history : list def __init__ ( self , checkpoint_path : str ): self . checkpointer = Checkpointer ( checkpoint_path ) self . _aux_history = [] @property def aux_history ( self ): return jax . tree_util . tree_map ( lambda * xs : jnp . array ( xs ), * self . _aux_history ) def train_step ( self , objective : Callable , optimizer : optax . GradientTransformation , train_state : TrainingState , data : Dict [ str , Array ]) -> Tuple [ TrainingState , Mapping [ str , Any ]]: i , model , opt_state = train_state . i , train_state . model , train_state . opt_state train_key , next_key = random . split ( train_state . key ) # Compute the gradients of the objective ( obj , aux ), grads = eqx . filter_value_and_grad ( objective , has_aux = True )( model , data , train_key ) aux [ 'objective' ] = obj # Update the model updates , new_opt_state = optimizer . update ( grads , opt_state , model ) new_model = eqx . apply_updates ( model , updates ) # Package the updated training state updated_train_state = TrainingState ( i = i + 1 , key = next_key , model = new_model , opt_state = new_opt_state ) return updated_train_state , aux def train ( self , model : eqx . Module , objective : Callable , evaluate_model : Callable , optimizer : optax . GradientTransformation , num_steps : int , data_iterator : Iterator , double_batch : int = - 1 , checkpoint_every : int = 1000 , test_every : int = 1000 , retrain : bool = False , just_load : bool = False ): \"\"\"Train the model. This will load the model if the most recent checkpoint exists has completed training. **Arguments**: - `model`: The model to train - `objective`: The objective function to optimize - `evaluate_model`: A function that takes in the model and evaluates it on a test set - `optimizer`: The optimizer to use - `num_steps`: The number of training steps to take - `data_iterator`: An iterator that yields batches of data - `double_batch`: If `double_batch > 0`, then we will take `double_batch` batches of data at a time and train over them in a fast `jax.lax.scan` loop. - `checkpoint_every`: How often to checkpoint the model - `test_every`: How often to evaluate the model - `retrain`: Whether to force retraining from scratch - `just_load`: Whether to just load the most recent checkpoint and return \"\"\" key0 = random . PRNGKey ( 0 ) # Load the most recent checkpoint opt_state = optimizer . init ( eqx . filter ( model , eqx . is_inexact_array )) train_state = TrainingState ( jnp . array ( 0.0 ), key0 , model , opt_state ) # Load the most recent checkpoint if retrain == False : train_state = self . restore ( train_state ) if just_load : return train_state . model # Fill in the training step with the objective and optimizer train_step = eqx . Partial ( self . train_step , objective , optimizer ) if double_batch == - 1 : # JIT the training update here train_step = eqx . filter_jit ( train_step ) else : # We can only pass in parameters dynamically to the scan loop, so we # need to extract the static values here (because they won't change) # and combine later inside the scan loop _ , static = eqx . partition ( train_state , eqx . is_array ) # Construct the scan loop that we'll use to process batches of data def step ( params , data ): train_state = eqx . combine ( params , static ) new_train_state , aux = train_step ( train_state , data ) new_params , _ = eqx . partition ( new_train_state , eqx . is_array ) return new_params , aux scan_step = partial ( jax . lax . scan , step ) scan_step = jax . jit ( scan_step ) # Construct the progress bar start = int ( train_state . i ) if retrain == False else 0 if double_batch <= 0 : pbar = tqdm . tqdm ( jnp . arange ( start , num_steps ), total = num_steps - start ) else : pbar = tqdm . tqdm ( jnp . arange ( start , num_steps , double_batch ), total = num_steps - start ) # Training loop for i in pbar : # Take a training step if double_batch == - 1 : data = next ( data_iterator ) train_state , aux = train_step ( train_state , data ) pbar . update ( 1 ) else : data = misc . extract_multiple_batches_from_iterator ( data_iterator , double_batch ) params , static = eqx . partition ( train_state , eqx . is_array ) params , aux = scan_step ( params , data ) train_state = eqx . combine ( params , static ) pbar . update ( double_batch ) self . _aux_history . append ( aux ) # Update the progress bar description = ', ' . join ([ f ' { k } = { float ( v . mean ()) : .4f } ' for k , v in aux . items ()]) pbar . set_description ( description ) # Checkpoint the model if ( i and ( i % checkpoint_every == 0 )): self . checkpoint ( train_state ) print ( 'Checkpointed model' ) # Evaluate the model if ( i % test_every == 0 ) or ( i == num_steps - 1 ): evaluate_model ( train_state . model ) # Final checkpoint self . checkpoint ( train_state ) print ( 'Checkpointed model' ) return train_state . model def checkpoint ( self , train_state : TrainingState ): self . checkpointer . save_eqx_module ( train_state ) def restore ( self , train_state : TrainingState ) -> TrainingState : train_state = self . checkpointer . load_eqx_module ( train_state ) print ( f 'Restored train_state { self . checkpointer . saved_model_path } ' ) return train_state __init__ ( self , checkpoint_path : str ) \u00a4 Source code in generax/trainer.py def __init__ ( self , checkpoint_path : str ): self . checkpointer = Checkpointer ( checkpoint_path ) self . _aux_history = [] train_step ( self , objective : Callable , optimizer : GradientTransformation , train_state : TrainingState , data : Dict [ str , Array ]) -> Tuple [ TrainingState , Mapping [ str , Any ]] \u00a4 Source code in generax/trainer.py def train_step ( self , objective : Callable , optimizer : optax . GradientTransformation , train_state : TrainingState , data : Dict [ str , Array ]) -> Tuple [ TrainingState , Mapping [ str , Any ]]: i , model , opt_state = train_state . i , train_state . model , train_state . opt_state train_key , next_key = random . split ( train_state . key ) # Compute the gradients of the objective ( obj , aux ), grads = eqx . filter_value_and_grad ( objective , has_aux = True )( model , data , train_key ) aux [ 'objective' ] = obj # Update the model updates , new_opt_state = optimizer . update ( grads , opt_state , model ) new_model = eqx . apply_updates ( model , updates ) # Package the updated training state updated_train_state = TrainingState ( i = i + 1 , key = next_key , model = new_model , opt_state = new_opt_state ) return updated_train_state , aux train ( self , model : Module , objective : Callable , evaluate_model : Callable , optimizer : GradientTransformation , num_steps : int , data_iterator : Iterator , double_batch : int = - 1 , checkpoint_every : int = 1000 , test_every : int = 1000 , retrain : bool = False , just_load : bool = False ) \u00a4 Train the model. This will load the model if the most recent checkpoint exists has completed training. Arguments : model : The model to train objective : The objective function to optimize evaluate_model : A function that takes in the model and evaluates it on a test set optimizer : The optimizer to use num_steps : The number of training steps to take data_iterator : An iterator that yields batches of data double_batch : If double_batch > 0 , then we will take double_batch batches of data at a time and train over them in a fast jax.lax.scan loop. checkpoint_every : How often to checkpoint the model test_every : How often to evaluate the model retrain : Whether to force retraining from scratch just_load : Whether to just load the most recent checkpoint and return Source code in generax/trainer.py def train ( self , model : eqx . Module , objective : Callable , evaluate_model : Callable , optimizer : optax . GradientTransformation , num_steps : int , data_iterator : Iterator , double_batch : int = - 1 , checkpoint_every : int = 1000 , test_every : int = 1000 , retrain : bool = False , just_load : bool = False ): \"\"\"Train the model. This will load the model if the most recent checkpoint exists has completed training. **Arguments**: - `model`: The model to train - `objective`: The objective function to optimize - `evaluate_model`: A function that takes in the model and evaluates it on a test set - `optimizer`: The optimizer to use - `num_steps`: The number of training steps to take - `data_iterator`: An iterator that yields batches of data - `double_batch`: If `double_batch > 0`, then we will take `double_batch` batches of data at a time and train over them in a fast `jax.lax.scan` loop. - `checkpoint_every`: How often to checkpoint the model - `test_every`: How often to evaluate the model - `retrain`: Whether to force retraining from scratch - `just_load`: Whether to just load the most recent checkpoint and return \"\"\" key0 = random . PRNGKey ( 0 ) # Load the most recent checkpoint opt_state = optimizer . init ( eqx . filter ( model , eqx . is_inexact_array )) train_state = TrainingState ( jnp . array ( 0.0 ), key0 , model , opt_state ) # Load the most recent checkpoint if retrain == False : train_state = self . restore ( train_state ) if just_load : return train_state . model # Fill in the training step with the objective and optimizer train_step = eqx . Partial ( self . train_step , objective , optimizer ) if double_batch == - 1 : # JIT the training update here train_step = eqx . filter_jit ( train_step ) else : # We can only pass in parameters dynamically to the scan loop, so we # need to extract the static values here (because they won't change) # and combine later inside the scan loop _ , static = eqx . partition ( train_state , eqx . is_array ) # Construct the scan loop that we'll use to process batches of data def step ( params , data ): train_state = eqx . combine ( params , static ) new_train_state , aux = train_step ( train_state , data ) new_params , _ = eqx . partition ( new_train_state , eqx . is_array ) return new_params , aux scan_step = partial ( jax . lax . scan , step ) scan_step = jax . jit ( scan_step ) # Construct the progress bar start = int ( train_state . i ) if retrain == False else 0 if double_batch <= 0 : pbar = tqdm . tqdm ( jnp . arange ( start , num_steps ), total = num_steps - start ) else : pbar = tqdm . tqdm ( jnp . arange ( start , num_steps , double_batch ), total = num_steps - start ) # Training loop for i in pbar : # Take a training step if double_batch == - 1 : data = next ( data_iterator ) train_state , aux = train_step ( train_state , data ) pbar . update ( 1 ) else : data = misc . extract_multiple_batches_from_iterator ( data_iterator , double_batch ) params , static = eqx . partition ( train_state , eqx . is_array ) params , aux = scan_step ( params , data ) train_state = eqx . combine ( params , static ) pbar . update ( double_batch ) self . _aux_history . append ( aux ) # Update the progress bar description = ', ' . join ([ f ' { k } = { float ( v . mean ()) : .4f } ' for k , v in aux . items ()]) pbar . set_description ( description ) # Checkpoint the model if ( i and ( i % checkpoint_every == 0 )): self . checkpoint ( train_state ) print ( 'Checkpointed model' ) # Evaluate the model if ( i % test_every == 0 ) or ( i == num_steps - 1 ): evaluate_model ( train_state . model ) # Final checkpoint self . checkpoint ( train_state ) print ( 'Checkpointed model' ) return train_state . model checkpoint ( self , train_state : TrainingState ) \u00a4 Source code in generax/trainer.py def checkpoint ( self , train_state : TrainingState ): self . checkpointer . save_eqx_module ( train_state ) restore ( self , train_state : TrainingState ) -> TrainingState \u00a4 Source code in generax/trainer.py def restore ( self , train_state : TrainingState ) -> TrainingState : train_state = self . checkpointer . load_eqx_module ( train_state ) print ( f 'Restored train_state { self . checkpointer . saved_model_path } ' ) return train_state","title":"Trainer"},{"location":"api/misc/trainer/#trainer","text":"","title":"Trainer"},{"location":"api/misc/trainer/#generax.trainer.TrainingState","text":"TrainingState( args, *kwargs) Source code in generax/trainer.py class TrainingState ( eqx . Module ): i : float key : PRNGKeyArray model : eqx . Module opt_state : optax . OptState def __init__ ( self , i : float , # float so that it is not treated as static key : PRNGKeyArray , model : eqx . Module , opt_state : optax . OptState ): self . i = i self . key = key self . model = model self . opt_state = opt_state","title":"TrainingState"},{"location":"api/misc/trainer/#generax.trainer.Checkpointer","text":"Checkpointer( args, *kwargs) Source code in generax/trainer.py class Checkpointer ( eqx . Module ): save_path : str model_folder : str def __init__ ( self , save_path : str ): self . save_path = save_path self . model_folder = os . path . join ( save_path , 'models' ) misc . ensure_path_exists ( self . model_folder ) @property def saved_model_path ( self ): return os . path . join ( self . model_folder , 'saved_model.pickle' ) def save_eqx_module ( self , model : eqx . Module ): eqx . tree_serialise_leaves ( self . saved_model_path , model ) def load_eqx_module ( self , model_example : eqx . Module ): return eqx . tree_deserialise_leaves ( self . saved_model_path , model_example )","title":"Checkpointer"},{"location":"api/misc/trainer/#generax.trainer.Trainer","text":"Class that will monitor training and handle checkpointing. Attributes : checkpointer : Object that saves checkpoints of the model Source code in generax/trainer.py class Trainer ( eqx . Module ): \"\"\"Class that will monitor training and handle checkpointing. **Attributes**: - `checkpointer`: Object that saves checkpoints of the model \"\"\" checkpointer : Checkpointer _aux_history : list def __init__ ( self , checkpoint_path : str ): self . checkpointer = Checkpointer ( checkpoint_path ) self . _aux_history = [] @property def aux_history ( self ): return jax . tree_util . tree_map ( lambda * xs : jnp . array ( xs ), * self . _aux_history ) def train_step ( self , objective : Callable , optimizer : optax . GradientTransformation , train_state : TrainingState , data : Dict [ str , Array ]) -> Tuple [ TrainingState , Mapping [ str , Any ]]: i , model , opt_state = train_state . i , train_state . model , train_state . opt_state train_key , next_key = random . split ( train_state . key ) # Compute the gradients of the objective ( obj , aux ), grads = eqx . filter_value_and_grad ( objective , has_aux = True )( model , data , train_key ) aux [ 'objective' ] = obj # Update the model updates , new_opt_state = optimizer . update ( grads , opt_state , model ) new_model = eqx . apply_updates ( model , updates ) # Package the updated training state updated_train_state = TrainingState ( i = i + 1 , key = next_key , model = new_model , opt_state = new_opt_state ) return updated_train_state , aux def train ( self , model : eqx . Module , objective : Callable , evaluate_model : Callable , optimizer : optax . GradientTransformation , num_steps : int , data_iterator : Iterator , double_batch : int = - 1 , checkpoint_every : int = 1000 , test_every : int = 1000 , retrain : bool = False , just_load : bool = False ): \"\"\"Train the model. This will load the model if the most recent checkpoint exists has completed training. **Arguments**: - `model`: The model to train - `objective`: The objective function to optimize - `evaluate_model`: A function that takes in the model and evaluates it on a test set - `optimizer`: The optimizer to use - `num_steps`: The number of training steps to take - `data_iterator`: An iterator that yields batches of data - `double_batch`: If `double_batch > 0`, then we will take `double_batch` batches of data at a time and train over them in a fast `jax.lax.scan` loop. - `checkpoint_every`: How often to checkpoint the model - `test_every`: How often to evaluate the model - `retrain`: Whether to force retraining from scratch - `just_load`: Whether to just load the most recent checkpoint and return \"\"\" key0 = random . PRNGKey ( 0 ) # Load the most recent checkpoint opt_state = optimizer . init ( eqx . filter ( model , eqx . is_inexact_array )) train_state = TrainingState ( jnp . array ( 0.0 ), key0 , model , opt_state ) # Load the most recent checkpoint if retrain == False : train_state = self . restore ( train_state ) if just_load : return train_state . model # Fill in the training step with the objective and optimizer train_step = eqx . Partial ( self . train_step , objective , optimizer ) if double_batch == - 1 : # JIT the training update here train_step = eqx . filter_jit ( train_step ) else : # We can only pass in parameters dynamically to the scan loop, so we # need to extract the static values here (because they won't change) # and combine later inside the scan loop _ , static = eqx . partition ( train_state , eqx . is_array ) # Construct the scan loop that we'll use to process batches of data def step ( params , data ): train_state = eqx . combine ( params , static ) new_train_state , aux = train_step ( train_state , data ) new_params , _ = eqx . partition ( new_train_state , eqx . is_array ) return new_params , aux scan_step = partial ( jax . lax . scan , step ) scan_step = jax . jit ( scan_step ) # Construct the progress bar start = int ( train_state . i ) if retrain == False else 0 if double_batch <= 0 : pbar = tqdm . tqdm ( jnp . arange ( start , num_steps ), total = num_steps - start ) else : pbar = tqdm . tqdm ( jnp . arange ( start , num_steps , double_batch ), total = num_steps - start ) # Training loop for i in pbar : # Take a training step if double_batch == - 1 : data = next ( data_iterator ) train_state , aux = train_step ( train_state , data ) pbar . update ( 1 ) else : data = misc . extract_multiple_batches_from_iterator ( data_iterator , double_batch ) params , static = eqx . partition ( train_state , eqx . is_array ) params , aux = scan_step ( params , data ) train_state = eqx . combine ( params , static ) pbar . update ( double_batch ) self . _aux_history . append ( aux ) # Update the progress bar description = ', ' . join ([ f ' { k } = { float ( v . mean ()) : .4f } ' for k , v in aux . items ()]) pbar . set_description ( description ) # Checkpoint the model if ( i and ( i % checkpoint_every == 0 )): self . checkpoint ( train_state ) print ( 'Checkpointed model' ) # Evaluate the model if ( i % test_every == 0 ) or ( i == num_steps - 1 ): evaluate_model ( train_state . model ) # Final checkpoint self . checkpoint ( train_state ) print ( 'Checkpointed model' ) return train_state . model def checkpoint ( self , train_state : TrainingState ): self . checkpointer . save_eqx_module ( train_state ) def restore ( self , train_state : TrainingState ) -> TrainingState : train_state = self . checkpointer . load_eqx_module ( train_state ) print ( f 'Restored train_state { self . checkpointer . saved_model_path } ' ) return train_state","title":"Trainer"},{"location":"api/nn/layers/","text":"Neural network layers \u00a4 generax.nn.layers.WeightNormDense \u00a4 Weight normalization parametrized linear layer https://arxiv.org/pdf/1602.07868.pdf Source code in generax/nn/layers.py class WeightNormDense ( eqx . Module ): \"\"\"Weight normalization parametrized linear layer https://arxiv.org/pdf/1602.07868.pdf \"\"\" in_size : int = eqx . field ( static = True ) out_size : int = eqx . field ( static = True ) W : Array b : Array g : Array def __init__ ( self , in_size : int , out_size : int , key : PRNGKeyArray , ** kwargs ): super () . __init__ ( ** kwargs ) self . in_size = in_size self . out_size = out_size w_init = jax . nn . initializers . he_uniform ( in_axis =- 2 , out_axis =- 1 ) self . W = w_init ( key , shape = ( out_size , in_size )) self . g = jnp . array ( 1.0 ) self . b = jnp . zeros ( out_size ) def data_dependent_init ( self , x : Array , key : PRNGKeyArray = None , before_square_plus : Optional [ bool ] = False ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `key`: A `jax.random.PRNGKey` for initialization - `before_square_plus`: In case we want the activations after square plus to be gaussian **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ - 1 ] == self . in_size , 'Only works on batched data' # Initialize g and b. W = self . W * jax . lax . rsqrt (( self . W ** 2 ) . sum ( axis = 1 ))[:, None ] x = jnp . einsum ( 'ij,bj->bi' , W , x ) std = jnp . std ( x . reshape (( - 1 , x . shape [ - 1 ])), axis = 0 ) + 1e-5 if before_square_plus : std = std - 1 / std g = 1 / std x *= g mean = jnp . mean ( x . reshape (( - 1 , x . shape [ - 1 ])), axis = 0 ) b = - mean # Turn the new parameters into a new module get_g = lambda tree : tree . g get_b = lambda tree : tree . b updated_layer = eqx . tree_at ( get_g , self , g ) updated_layer = eqx . tree_at ( get_b , updated_layer , b ) return updated_layer def __call__ ( self , x : Array , y : Array = None ) -> Array : W = self . W * jax . lax . rsqrt (( self . W ** 2 ) . sum ( axis = 1 ))[:, None ] x = self . g * ( W @x ) + self . b return x __init__ ( self , in_size : int , out_size : int , key : PRNGKeyArray , ** kwargs ) \u00a4 Source code in generax/nn/layers.py def __init__ ( self , in_size : int , out_size : int , key : PRNGKeyArray , ** kwargs ): super () . __init__ ( ** kwargs ) self . in_size = in_size self . out_size = out_size w_init = jax . nn . initializers . he_uniform ( in_axis =- 2 , out_axis =- 1 ) self . W = w_init ( key , shape = ( out_size , in_size )) self . g = jnp . array ( 1.0 ) self . b = jnp . zeros ( out_size ) data_dependent_init ( self , x : Array , key : PRNGKeyArray = None , before_square_plus : Optional [ bool ] = False ) -> Module \u00a4 Initialize the parameters of the layer based on the data. Arguments : x : The data to initialize the parameters with. key : A jax.random.PRNGKey for initialization before_square_plus : In case we want the activations after square plus to be gaussian Returns : A new layer with the parameters initialized. Source code in generax/nn/layers.py def data_dependent_init ( self , x : Array , key : PRNGKeyArray = None , before_square_plus : Optional [ bool ] = False ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `key`: A `jax.random.PRNGKey` for initialization - `before_square_plus`: In case we want the activations after square plus to be gaussian **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ - 1 ] == self . in_size , 'Only works on batched data' # Initialize g and b. W = self . W * jax . lax . rsqrt (( self . W ** 2 ) . sum ( axis = 1 ))[:, None ] x = jnp . einsum ( 'ij,bj->bi' , W , x ) std = jnp . std ( x . reshape (( - 1 , x . shape [ - 1 ])), axis = 0 ) + 1e-5 if before_square_plus : std = std - 1 / std g = 1 / std x *= g mean = jnp . mean ( x . reshape (( - 1 , x . shape [ - 1 ])), axis = 0 ) b = - mean # Turn the new parameters into a new module get_g = lambda tree : tree . g get_b = lambda tree : tree . b updated_layer = eqx . tree_at ( get_g , self , g ) updated_layer = eqx . tree_at ( get_b , updated_layer , b ) return updated_layer __call__ ( self , x : Array , y : Array = None ) -> Array \u00a4 Call self as a function. Source code in generax/nn/layers.py def __call__ ( self , x : Array , y : Array = None ) -> Array : W = self . W * jax . lax . rsqrt (( self . W ** 2 ) . sum ( axis = 1 ))[:, None ] x = self . g * ( W @x ) + self . b return x generax.nn.layers.WeightNormConv \u00a4 Weight normalization parametrized convolutional layer https://arxiv.org/pdf/1602.07868.pdf Source code in generax/nn/layers.py class WeightNormConv ( eqx . Module ): \"\"\"Weight normalization parametrized convolutional layer https://arxiv.org/pdf/1602.07868.pdf \"\"\" input_shape : int = eqx . field ( static = True ) out_size : int = eqx . field ( static = True ) filter_shape : Tuple [ int ] = eqx . field ( static = True ) padding : Union [ int , str ] = eqx . field ( static = True ) stride : int = eqx . field ( static = True ) W : Array b : Array g : Array def __init__ ( self , input_shape : Tuple [ int ], # in_channels filter_shape : Tuple [ int ], out_size : int , * , key : PRNGKeyArray , padding : Union [ int , str ] = 'SAME' , stride : int = 1 , ** kwargs ): super () . __init__ ( ** kwargs ) H , W , C = input_shape self . input_shape = input_shape self . filter_shape = filter_shape self . out_size = out_size self . padding = padding self . stride = stride w_init = jax . nn . initializers . he_uniform ( in_axis =- 2 , out_axis =- 1 ) self . W = w_init ( key , shape = self . filter_shape + ( C , out_size )) self . g = jnp . array ( 1.0 ) self . b = jnp . zeros ( out_size ) def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None , before_square_plus : Optional [ bool ] = False ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `key`: A `jax.random.PRNGKey` for initialization - `before_square_plus`: In case we want the activations after square plus to be gaussian **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' # Initialize g and b. W = self . W * jax . lax . rsqrt (( self . W ** 2 ) . sum ( axis = ( 0 , 1 , 2 )))[ None , None , None ,:] x = util . conv ( W , x , stride = self . stride , padding = self . padding ) std = jnp . std ( x . reshape (( - 1 , x . shape [ - 1 ])), axis = 0 ) + 1e-5 if before_square_plus : std = std - 1 / std g = 1 / std x *= g mean = jnp . mean ( x . reshape (( - 1 , x . shape [ - 1 ])), axis = 0 ) b = - mean # Turn the new parameters into a new module get_g = lambda tree : tree . g get_b = lambda tree : tree . b updated_layer = eqx . tree_at ( get_g , self , g ) updated_layer = eqx . tree_at ( get_b , updated_layer , b ) return updated_layer def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' W = self . W * jax . lax . rsqrt (( self . W ** 2 ) . sum ( axis = ( 0 , 1 , 2 )))[ None , None , None ,:] x = self . g * util . conv ( W , x , stride = self . stride , padding = self . padding ) + self . b return x __init__ ( self , input_shape : Tuple [ int ], filter_shape : Tuple [ int ], out_size : int , * , key : PRNGKeyArray , padding : Union [ int , str ] = 'SAME' , stride : int = 1 , ** kwargs ) \u00a4 Source code in generax/nn/layers.py def __init__ ( self , input_shape : Tuple [ int ], # in_channels filter_shape : Tuple [ int ], out_size : int , * , key : PRNGKeyArray , padding : Union [ int , str ] = 'SAME' , stride : int = 1 , ** kwargs ): super () . __init__ ( ** kwargs ) H , W , C = input_shape self . input_shape = input_shape self . filter_shape = filter_shape self . out_size = out_size self . padding = padding self . stride = stride w_init = jax . nn . initializers . he_uniform ( in_axis =- 2 , out_axis =- 1 ) self . W = w_init ( key , shape = self . filter_shape + ( C , out_size )) self . g = jnp . array ( 1.0 ) self . b = jnp . zeros ( out_size ) data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None , before_square_plus : Optional [ bool ] = False ) -> Module \u00a4 Initialize the parameters of the layer based on the data. Arguments : x : The data to initialize the parameters with. key : A jax.random.PRNGKey for initialization before_square_plus : In case we want the activations after square plus to be gaussian Returns : A new layer with the parameters initialized. Source code in generax/nn/layers.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None , before_square_plus : Optional [ bool ] = False ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `key`: A `jax.random.PRNGKey` for initialization - `before_square_plus`: In case we want the activations after square plus to be gaussian **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' # Initialize g and b. W = self . W * jax . lax . rsqrt (( self . W ** 2 ) . sum ( axis = ( 0 , 1 , 2 )))[ None , None , None ,:] x = util . conv ( W , x , stride = self . stride , padding = self . padding ) std = jnp . std ( x . reshape (( - 1 , x . shape [ - 1 ])), axis = 0 ) + 1e-5 if before_square_plus : std = std - 1 / std g = 1 / std x *= g mean = jnp . mean ( x . reshape (( - 1 , x . shape [ - 1 ])), axis = 0 ) b = - mean # Turn the new parameters into a new module get_g = lambda tree : tree . g get_b = lambda tree : tree . b updated_layer = eqx . tree_at ( get_g , self , g ) updated_layer = eqx . tree_at ( get_b , updated_layer , b ) return updated_layer __call__ ( self , x : Array , y : Array = None ) -> Array \u00a4 Call self as a function. Source code in generax/nn/layers.py def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' W = self . W * jax . lax . rsqrt (( self . W ** 2 ) . sum ( axis = ( 0 , 1 , 2 )))[ None , None , None ,:] x = self . g * util . conv ( W , x , stride = self . stride , padding = self . padding ) + self . b return x generax.nn.layers.WeightStandardizedConv \u00a4 Weight standardized parametrized convolutional layer https://arxiv.org/pdf/1903.10520.pdf Source code in generax/nn/layers.py class WeightStandardizedConv ( eqx . Module ): \"\"\"Weight standardized parametrized convolutional layer https://arxiv.org/pdf/1903.10520.pdf \"\"\" input_shape : int = eqx . field ( static = True ) out_size : int = eqx . field ( static = True ) filter_shape : Tuple [ int ] = eqx . field ( static = True ) padding : Union [ int , str ] = eqx . field ( static = True ) stride : int = eqx . field ( static = True ) W : Array b : Array def __init__ ( self , input_shape : Tuple [ int ], # in_channels filter_shape : Tuple [ int ], out_size : int , * , key : PRNGKeyArray , padding : Union [ int , str ] = 'SAME' , stride : int = 1 , ** kwargs ): super () . __init__ ( ** kwargs ) H , W , C = input_shape self . input_shape = input_shape self . filter_shape = filter_shape self . out_size = out_size self . padding = padding self . stride = stride w_init = jax . nn . initializers . he_uniform ( in_axis =- 2 , out_axis =- 1 ) self . W = w_init ( key , shape = self . filter_shape + ( C , out_size )) self . b = jnp . zeros ( out_size ) def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" axes = ( 0 , 1 , 2 ) mean = jnp . mean ( self . W , axis = axes , keepdims = True ) var = jnp . var ( self . W , axis = axes , keepdims = True ) W_hat = ( self . W - mean ) / jnp . sqrt ( var + 1e-5 ) x = util . conv ( W_hat , x , stride = self . stride , padding = self . padding ) # Initialize b. mean = jnp . mean ( x . reshape (( - 1 , x . shape [ - 1 ])), axis = 0 ) b = - mean # Turn the new parameters into a new module get_b = lambda tree : tree . b updated_layer = eqx . tree_at ( get_b , self , b ) return updated_layer def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' axes = ( 0 , 1 , 2 ) mean = jnp . mean ( self . W , axis = axes , keepdims = True ) var = jnp . var ( self . W , axis = axes , keepdims = True ) H , W , C_in , C_out = self . W . shape fan_in = H * W * C_in W_hat = ( self . W - mean ) * jax . lax . rsqrt ( fan_in * var + 1e-5 ) x = util . conv ( W_hat , x , stride = self . stride , padding = self . padding ) + self . b return x __init__ ( self , input_shape : Tuple [ int ], filter_shape : Tuple [ int ], out_size : int , * , key : PRNGKeyArray , padding : Union [ int , str ] = 'SAME' , stride : int = 1 , ** kwargs ) \u00a4 Source code in generax/nn/layers.py def __init__ ( self , input_shape : Tuple [ int ], # in_channels filter_shape : Tuple [ int ], out_size : int , * , key : PRNGKeyArray , padding : Union [ int , str ] = 'SAME' , stride : int = 1 , ** kwargs ): super () . __init__ ( ** kwargs ) H , W , C = input_shape self . input_shape = input_shape self . filter_shape = filter_shape self . out_size = out_size self . padding = padding self . stride = stride w_init = jax . nn . initializers . he_uniform ( in_axis =- 2 , out_axis =- 1 ) self . W = w_init ( key , shape = self . filter_shape + ( C , out_size )) self . b = jnp . zeros ( out_size ) data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> Module \u00a4 Initialize the parameters of the layer based on the data. Arguments : x : The data to initialize the parameters with. key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/nn/layers.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" axes = ( 0 , 1 , 2 ) mean = jnp . mean ( self . W , axis = axes , keepdims = True ) var = jnp . var ( self . W , axis = axes , keepdims = True ) W_hat = ( self . W - mean ) / jnp . sqrt ( var + 1e-5 ) x = util . conv ( W_hat , x , stride = self . stride , padding = self . padding ) # Initialize b. mean = jnp . mean ( x . reshape (( - 1 , x . shape [ - 1 ])), axis = 0 ) b = - mean # Turn the new parameters into a new module get_b = lambda tree : tree . b updated_layer = eqx . tree_at ( get_b , self , b ) return updated_layer __call__ ( self , x : Array , y : Array = None ) -> Array \u00a4 Call self as a function. Source code in generax/nn/layers.py def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' axes = ( 0 , 1 , 2 ) mean = jnp . mean ( self . W , axis = axes , keepdims = True ) var = jnp . var ( self . W , axis = axes , keepdims = True ) H , W , C_in , C_out = self . W . shape fan_in = H * W * C_in W_hat = ( self . W - mean ) * jax . lax . rsqrt ( fan_in * var + 1e-5 ) x = util . conv ( W_hat , x , stride = self . stride , padding = self . padding ) + self . b return x generax.nn.layers.ChannelConvention \u00a4 ChannelConvention( args, *kwargs) Source code in generax/nn/layers.py class ChannelConvention ( eqx . Module ): module : eqx . Module def __init__ ( self , module : eqx . Module ): super () . __init__ () self . module = module def __call__ ( self , x ): x = einops . rearrange ( x , 'H W C -> C H W' ) x = self . module ( x ) x = einops . rearrange ( x , 'C H W -> H W C' ) return x __init__ ( self , module : Module ) \u00a4 Source code in generax/nn/layers.py def __init__ ( self , module : eqx . Module ): super () . __init__ () self . module = module __call__ ( self , x ) \u00a4 Call self as a function. Source code in generax/nn/layers.py def __call__ ( self , x ): x = einops . rearrange ( x , 'H W C -> C H W' ) x = self . module ( x ) x = einops . rearrange ( x , 'C H W -> H W C' ) return x generax.nn.layers.ConvAndGroupNorm \u00a4 Weight standardized conv + group norm Source code in generax/nn/layers.py class ConvAndGroupNorm ( eqx . Module ): \"\"\"Weight standardized conv + group norm \"\"\" input_shape : int = eqx . field ( static = True ) conv : WeightStandardizedConv norm : eqx . nn . GroupNorm def __init__ ( self , input_shape : Tuple [ int ], # in_channels filter_shape : Tuple [ int ], out_size : int , groups : int , * , key : PRNGKeyArray , padding : Union [ int , str ] = 'SAME' , stride : int = 1 , ** kwargs ): super () . __init__ ( ** kwargs ) if out_size % groups != 0 : raise ValueError ( \"The number of groups must divide the number of channels.\" ) self . conv = WeightStandardizedConv ( input_shape = input_shape , filter_shape = filter_shape , out_size = out_size , key = key , padding = padding , stride = stride ) self . norm = ChannelConvention ( eqx . nn . GroupNorm ( groups = groups , channels = out_size )) self . input_shape = self . conv . input_shape def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , shift_scale : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" new_conv = self . conv . data_dependent_init ( x , y , key = key ) get_conv = lambda tree : tree . conv updated_layer = eqx . tree_at ( get_conv , self , new_conv ) return updated_layer def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' x = self . conv ( x ) x = self . norm ( x ) return x __init__ ( self , input_shape : Tuple [ int ], filter_shape : Tuple [ int ], out_size : int , groups : int , * , key : PRNGKeyArray , padding : Union [ int , str ] = 'SAME' , stride : int = 1 , ** kwargs ) \u00a4 Source code in generax/nn/layers.py def __init__ ( self , input_shape : Tuple [ int ], # in_channels filter_shape : Tuple [ int ], out_size : int , groups : int , * , key : PRNGKeyArray , padding : Union [ int , str ] = 'SAME' , stride : int = 1 , ** kwargs ): super () . __init__ ( ** kwargs ) if out_size % groups != 0 : raise ValueError ( \"The number of groups must divide the number of channels.\" ) self . conv = WeightStandardizedConv ( input_shape = input_shape , filter_shape = filter_shape , out_size = out_size , key = key , padding = padding , stride = stride ) self . norm = ChannelConvention ( eqx . nn . GroupNorm ( groups = groups , channels = out_size )) self . input_shape = self . conv . input_shape data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , shift_scale : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> Module \u00a4 Initialize the parameters of the layer based on the data. Arguments : x : The data to initialize the parameters with. key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/nn/layers.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , shift_scale : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" new_conv = self . conv . data_dependent_init ( x , y , key = key ) get_conv = lambda tree : tree . conv updated_layer = eqx . tree_at ( get_conv , self , new_conv ) return updated_layer __call__ ( self , x : Array , y : Array = None ) -> Array \u00a4 Call self as a function. Source code in generax/nn/layers.py def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' x = self . conv ( x ) x = self . norm ( x ) return x generax.nn.layers.Upsample \u00a4 https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf Source code in generax/nn/layers.py class Upsample ( eqx . Module ): \"\"\"https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf \"\"\" input_shape : int = eqx . field ( static = True ) out_size : int = eqx . field ( static = True ) conv : WeightStandardizedConv def __init__ ( self , input_shape : Tuple [ int ], out_size : Optional [ int ] = None , * , key : PRNGKeyArray , ** kwargs ): super () . __init__ ( ** kwargs ) H , W , C = input_shape self . input_shape = input_shape self . out_size = out_size if out_size is not None else C self . conv = WeightStandardizedConv ( input_shape = ( H , W , C ), filter_shape = ( 3 , 3 ), out_size = 4 * self . out_size , key = key ) def data_dependent_init ( self , * args , ** kwargs ) -> eqx . Module : return self def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' H , W , C = x . shape x = self . conv ( x ) x = jax . nn . silu ( x ) x = einops . rearrange ( x , 'h w (c k1 k2) -> (h k1) (w k2) c' , k1 = 2 , k2 = 2 ) assert x . shape == ( H * 2 , W * 2 , self . out_size ) return x __init__ ( self , input_shape : Tuple [ int ], out_size : Optional [ int ] = None , * , key : PRNGKeyArray , ** kwargs ) \u00a4 Source code in generax/nn/layers.py def __init__ ( self , input_shape : Tuple [ int ], out_size : Optional [ int ] = None , * , key : PRNGKeyArray , ** kwargs ): super () . __init__ ( ** kwargs ) H , W , C = input_shape self . input_shape = input_shape self . out_size = out_size if out_size is not None else C self . conv = WeightStandardizedConv ( input_shape = ( H , W , C ), filter_shape = ( 3 , 3 ), out_size = 4 * self . out_size , key = key ) data_dependent_init ( self , * args , ** kwargs ) -> Module \u00a4 Source code in generax/nn/layers.py def data_dependent_init ( self , * args , ** kwargs ) -> eqx . Module : return self __call__ ( self , x : Array , y : Array = None ) -> Array \u00a4 Call self as a function. Source code in generax/nn/layers.py def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' H , W , C = x . shape x = self . conv ( x ) x = jax . nn . silu ( x ) x = einops . rearrange ( x , 'h w (c k1 k2) -> (h k1) (w k2) c' , k1 = 2 , k2 = 2 ) assert x . shape == ( H * 2 , W * 2 , self . out_size ) return x generax.nn.layers.Downsample \u00a4 Downsample( args, *kwargs) Source code in generax/nn/layers.py class Downsample ( eqx . Module ): input_shape : int = eqx . field ( static = True ) out_size : int = eqx . field ( static = True ) conv : WeightStandardizedConv def __init__ ( self , input_shape : Tuple [ int ], out_size : Optional [ int ] = None , * , key : PRNGKeyArray , ** kwargs ): super () . __init__ ( ** kwargs ) H , W , C = input_shape self . input_shape = input_shape self . out_size = out_size if out_size is not None else C self . conv = WeightStandardizedConv ( input_shape = ( H // 2 , W // 2 , C * 4 ), filter_shape = ( 3 , 3 ), out_size = self . out_size , key = key ) def data_dependent_init ( self , * args , ** kwargs ) -> eqx . Module : return self def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' H , W , C = x . shape x = einops . rearrange ( x , '(h k1) (w k2) c -> h w (c k1 k2)' , k1 = 2 , k2 = 2 ) x = self . conv ( x ) assert x . shape == ( H // 2 , W // 2 , self . out_size ) return x __init__ ( self , input_shape : Tuple [ int ], out_size : Optional [ int ] = None , * , key : PRNGKeyArray , ** kwargs ) \u00a4 Source code in generax/nn/layers.py def __init__ ( self , input_shape : Tuple [ int ], out_size : Optional [ int ] = None , * , key : PRNGKeyArray , ** kwargs ): super () . __init__ ( ** kwargs ) H , W , C = input_shape self . input_shape = input_shape self . out_size = out_size if out_size is not None else C self . conv = WeightStandardizedConv ( input_shape = ( H // 2 , W // 2 , C * 4 ), filter_shape = ( 3 , 3 ), out_size = self . out_size , key = key ) data_dependent_init ( self , * args , ** kwargs ) -> Module \u00a4 Source code in generax/nn/layers.py def data_dependent_init ( self , * args , ** kwargs ) -> eqx . Module : return self __call__ ( self , x : Array , y : Array = None ) -> Array \u00a4 Call self as a function. Source code in generax/nn/layers.py def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' H , W , C = x . shape x = einops . rearrange ( x , '(h k1) (w k2) c -> h w (c k1 k2)' , k1 = 2 , k2 = 2 ) x = self . conv ( x ) assert x . shape == ( H // 2 , W // 2 , self . out_size ) return x generax.nn.layers.GatedGlobalContext \u00a4 Modified version of https://arxiv.org/pdf/1904.11492.pdf used in imagen https://github.com/lucidrains/imagen-pytorch/ Source code in generax/nn/layers.py class GatedGlobalContext ( eqx . Module ): \"\"\"Modified version of https://arxiv.org/pdf/1904.11492.pdf used in imagen https://github.com/lucidrains/imagen-pytorch/\"\"\" input_shape : int = eqx . field ( static = True ) linear1 : WeightNormConv linear2 : WeightNormConv context_conv : WeightNormConv def __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray , ** kwargs ): super () . __init__ ( ** kwargs ) H , W , C = input_shape self . input_shape = input_shape out_size = C hidden_dim = max ( 3 , out_size // 2 ) k1 , k2 , k3 = random . split ( key , 3 ) self . linear1 = WeightNormDense ( in_size = C , out_size = hidden_dim , key = k1 ) self . linear2 = WeightNormDense ( in_size = hidden_dim , out_size = out_size , key = k2 ) self . context_conv = WeightNormConv ( input_shape = input_shape , filter_shape = ( 1 , 1 ), out_size = 1 , key = k3 ) def data_dependent_init ( self , * args , ** kwargs ) -> eqx . Module : return self def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' x_in = x H , W , C = x . shape # Reduce channels to (H, W, 1) context = self . context_conv ( x ) # Flatten c_flat = einops . rearrange ( context , 'h w c -> (h w) c' ) x_flat = einops . rearrange ( x , 'h w c -> (h w) c' ) # Context over the pixels c_sm = jax . nn . softmax ( c_flat , axis = 0 ) # Reweight the channels out = jnp . einsum ( 'tu,tv->uv' , c_sm , x_flat ) assert out . shape == ( 1 , C ) out = out [ 0 ] out = self . linear1 ( out ) out = jax . nn . silu ( out ) out = self . linear2 ( out ) out = jax . nn . sigmoid ( out ) return x_in * out [ None , None ,:] __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray , ** kwargs ) \u00a4 Source code in generax/nn/layers.py def __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray , ** kwargs ): super () . __init__ ( ** kwargs ) H , W , C = input_shape self . input_shape = input_shape out_size = C hidden_dim = max ( 3 , out_size // 2 ) k1 , k2 , k3 = random . split ( key , 3 ) self . linear1 = WeightNormDense ( in_size = C , out_size = hidden_dim , key = k1 ) self . linear2 = WeightNormDense ( in_size = hidden_dim , out_size = out_size , key = k2 ) self . context_conv = WeightNormConv ( input_shape = input_shape , filter_shape = ( 1 , 1 ), out_size = 1 , key = k3 ) data_dependent_init ( self , * args , ** kwargs ) -> Module \u00a4 Source code in generax/nn/layers.py def data_dependent_init ( self , * args , ** kwargs ) -> eqx . Module : return self __call__ ( self , x : Array , y : Array = None ) -> Array \u00a4 Call self as a function. Source code in generax/nn/layers.py def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' x_in = x H , W , C = x . shape # Reduce channels to (H, W, 1) context = self . context_conv ( x ) # Flatten c_flat = einops . rearrange ( context , 'h w c -> (h w) c' ) x_flat = einops . rearrange ( x , 'h w c -> (h w) c' ) # Context over the pixels c_sm = jax . nn . softmax ( c_flat , axis = 0 ) # Reweight the channels out = jnp . einsum ( 'tu,tv->uv' , c_sm , x_flat ) assert out . shape == ( 1 , C ) out = out [ 0 ] out = self . linear1 ( out ) out = jax . nn . silu ( out ) out = self . linear2 ( out ) out = jax . nn . sigmoid ( out ) return x_in * out [ None , None ,:] generax.nn.layers.Attention \u00a4 Attention( args, *kwargs) Source code in generax/nn/layers.py class Attention ( eqx . Module ): input_shape : int = eqx . field ( static = True ) heads : int = eqx . field ( static = True ) dim_head : int = eqx . field ( static = True ) scale : float = eqx . field ( static = True ) conv_in : eqx . nn . Conv3d conv_out : eqx . nn . Conv3d def __init__ ( self , input_shape : Tuple [ int ], heads : int = 4 , dim_head : int = 32 , scale : float = 10 , * , key : PRNGKeyArray , ** kwargs ): super () . __init__ ( ** kwargs ) H , W , C = input_shape self . input_shape = input_shape self . heads = heads self . dim_head = dim_head self . scale = scale k1 , k2 = random . split ( key , 2 ) dim = self . dim_head * self . heads self . conv_in = ChannelConvention ( eqx . nn . Conv2d ( in_channels = C , out_channels = 3 * dim , kernel_size = 1 , use_bias = False , key = k1 )) self . conv_out = ChannelConvention ( eqx . nn . Conv2d ( in_channels = dim , out_channels = C , kernel_size = 1 , use_bias = True , key = k2 )) def data_dependent_init ( self , * args , ** kwargs ) -> eqx . Module : return self def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' H , W , C = x . shape qkv = self . conv_in ( x ) # (H, W, heads*dim_head*3) qkv = einops . rearrange ( qkv , 'H W (u h d) -> (H W) h d u' , h = self . heads , d = self . dim_head , u = 3 ) q , k , v = jnp . split ( qkv , 3 , axis =- 1 ) q , k , v = q [ ... , 0 ], k [ ... , 0 ], v [ ... , 0 ] assert q . shape == k . shape == v . shape == ( H * W , self . heads , self . dim_head ) def normalize ( x ): return x / jnp . clip ( jnp . linalg . norm ( x , axis = 0 , keepdims = True ), 1e-8 ) q , k = normalize ( q ), normalize ( k ) sim = jnp . einsum ( 'ihd,jhd->hij' , q , k ) * self . scale attn = jax . nn . softmax ( sim , axis =- 1 ) assert attn . shape == ( self . heads , H * W , H * W ) out = jnp . einsum ( 'hij,jhd->hid' , attn , v ) out = einops . rearrange ( out , 'h (H W) d -> H W (h d)' , H = H , W = W , h = self . heads , d = self . dim_head ) assert out . shape == ( H , W , self . dim_head * self . heads ) out = self . conv_out ( out ) return out __init__ ( self , input_shape : Tuple [ int ], heads : int = 4 , dim_head : int = 32 , scale : float = 10 , * , key : PRNGKeyArray , ** kwargs ) \u00a4 Source code in generax/nn/layers.py def __init__ ( self , input_shape : Tuple [ int ], heads : int = 4 , dim_head : int = 32 , scale : float = 10 , * , key : PRNGKeyArray , ** kwargs ): super () . __init__ ( ** kwargs ) H , W , C = input_shape self . input_shape = input_shape self . heads = heads self . dim_head = dim_head self . scale = scale k1 , k2 = random . split ( key , 2 ) dim = self . dim_head * self . heads self . conv_in = ChannelConvention ( eqx . nn . Conv2d ( in_channels = C , out_channels = 3 * dim , kernel_size = 1 , use_bias = False , key = k1 )) self . conv_out = ChannelConvention ( eqx . nn . Conv2d ( in_channels = dim , out_channels = C , kernel_size = 1 , use_bias = True , key = k2 )) data_dependent_init ( self , * args , ** kwargs ) -> Module \u00a4 Source code in generax/nn/layers.py def data_dependent_init ( self , * args , ** kwargs ) -> eqx . Module : return self __call__ ( self , x : Array , y : Array = None ) -> Array \u00a4 Call self as a function. Source code in generax/nn/layers.py def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' H , W , C = x . shape qkv = self . conv_in ( x ) # (H, W, heads*dim_head*3) qkv = einops . rearrange ( qkv , 'H W (u h d) -> (H W) h d u' , h = self . heads , d = self . dim_head , u = 3 ) q , k , v = jnp . split ( qkv , 3 , axis =- 1 ) q , k , v = q [ ... , 0 ], k [ ... , 0 ], v [ ... , 0 ] assert q . shape == k . shape == v . shape == ( H * W , self . heads , self . dim_head ) def normalize ( x ): return x / jnp . clip ( jnp . linalg . norm ( x , axis = 0 , keepdims = True ), 1e-8 ) q , k = normalize ( q ), normalize ( k ) sim = jnp . einsum ( 'ihd,jhd->hij' , q , k ) * self . scale attn = jax . nn . softmax ( sim , axis =- 1 ) assert attn . shape == ( self . heads , H * W , H * W ) out = jnp . einsum ( 'hij,jhd->hid' , attn , v ) out = einops . rearrange ( out , 'h (H W) d -> H W (h d)' , H = H , W = W , h = self . heads , d = self . dim_head ) assert out . shape == ( H , W , self . dim_head * self . heads ) out = self . conv_out ( out ) return out generax.nn.layers.LinearAttention \u00a4 LinearAttention( args, *kwargs) Source code in generax/nn/layers.py class LinearAttention ( eqx . Module ): input_shape : int = eqx . field ( static = True ) heads : int = eqx . field ( static = True ) dim_head : int = eqx . field ( static = True ) conv_in : eqx . nn . Conv3d conv_out : eqx . nn . Conv3d norm : eqx . nn . LayerNorm def __init__ ( self , input_shape : Tuple [ int ], heads : int = 4 , dim_head : int = 32 , * , key : PRNGKeyArray , ** kwargs ): super () . __init__ ( ** kwargs ) H , W , C = input_shape self . input_shape = input_shape self . heads = heads self . dim_head = dim_head k1 , k2 = random . split ( key , 2 ) dim = self . dim_head * self . heads self . conv_in = ChannelConvention ( eqx . nn . Conv2d ( in_channels = C , out_channels = 3 * dim , kernel_size = 1 , use_bias = False , key = k1 )) self . conv_out = ChannelConvention ( eqx . nn . Conv2d ( in_channels = dim , out_channels = C , kernel_size = 1 , use_bias = True , key = k2 )) self . norm = eqx . nn . LayerNorm ( shape = ( C ,), use_bias = False ) def data_dependent_init ( self , * args , ** kwargs ) -> eqx . Module : return self def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' H , W , C = x . shape qkv = self . conv_in ( x ) # (H, W, heads*dim_head*3) qkv = einops . rearrange ( qkv , 'H W (u h d) -> (H W) h d u' , h = self . heads , d = self . dim_head , u = 3 ) q , k , v = jnp . split ( qkv , 3 , axis =- 1 ) q , k , v = q [ ... , 0 ], k [ ... , 0 ], v [ ... , 0 ] assert q . shape == k . shape == v . shape == ( H * W , self . heads , self . dim_head ) q = jax . nn . softmax ( q , axis =- 1 ) k = jax . nn . softmax ( k , axis =- 3 ) q = q / jnp . sqrt ( self . dim_head ) v = v / ( H * W ) context = jnp . einsum ( \"n h d, n h e -> h d e\" , k , v ) out = jnp . einsum ( \"h d e, n h d -> h e n\" , context , q ) out = einops . rearrange ( out , \"h e (x y) -> x y (h e)\" , x = H ) assert out . shape == ( H , W , self . dim_head * self . heads ) out = self . conv_out ( out ) out = eqx . filter_vmap ( eqx . filter_vmap ( self . norm ))( out ) return out __init__ ( self , input_shape : Tuple [ int ], heads : int = 4 , dim_head : int = 32 , * , key : PRNGKeyArray , ** kwargs ) \u00a4 Source code in generax/nn/layers.py def __init__ ( self , input_shape : Tuple [ int ], heads : int = 4 , dim_head : int = 32 , * , key : PRNGKeyArray , ** kwargs ): super () . __init__ ( ** kwargs ) H , W , C = input_shape self . input_shape = input_shape self . heads = heads self . dim_head = dim_head k1 , k2 = random . split ( key , 2 ) dim = self . dim_head * self . heads self . conv_in = ChannelConvention ( eqx . nn . Conv2d ( in_channels = C , out_channels = 3 * dim , kernel_size = 1 , use_bias = False , key = k1 )) self . conv_out = ChannelConvention ( eqx . nn . Conv2d ( in_channels = dim , out_channels = C , kernel_size = 1 , use_bias = True , key = k2 )) self . norm = eqx . nn . LayerNorm ( shape = ( C ,), use_bias = False ) data_dependent_init ( self , * args , ** kwargs ) -> Module \u00a4 Source code in generax/nn/layers.py def data_dependent_init ( self , * args , ** kwargs ) -> eqx . Module : return self __call__ ( self , x : Array , y : Array = None ) -> Array \u00a4 Call self as a function. Source code in generax/nn/layers.py def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' H , W , C = x . shape qkv = self . conv_in ( x ) # (H, W, heads*dim_head*3) qkv = einops . rearrange ( qkv , 'H W (u h d) -> (H W) h d u' , h = self . heads , d = self . dim_head , u = 3 ) q , k , v = jnp . split ( qkv , 3 , axis =- 1 ) q , k , v = q [ ... , 0 ], k [ ... , 0 ], v [ ... , 0 ] assert q . shape == k . shape == v . shape == ( H * W , self . heads , self . dim_head ) q = jax . nn . softmax ( q , axis =- 1 ) k = jax . nn . softmax ( k , axis =- 3 ) q = q / jnp . sqrt ( self . dim_head ) v = v / ( H * W ) context = jnp . einsum ( \"n h d, n h e -> h d e\" , k , v ) out = jnp . einsum ( \"h d e, n h d -> h e n\" , context , q ) out = einops . rearrange ( out , \"h e (x y) -> x y (h e)\" , x = H ) assert out . shape == ( H , W , self . dim_head * self . heads ) out = self . conv_out ( out ) out = eqx . filter_vmap ( eqx . filter_vmap ( self . norm ))( out ) return out generax.nn.layers.AttentionBlock \u00a4 AttentionBlock( args, *kwargs) Source code in generax/nn/layers.py class AttentionBlock ( eqx . Module ): input_shape : int = eqx . field ( static = True ) attn : Union [ Attention , LinearAttention ] norm : eqx . nn . LayerNorm def __init__ ( self , input_shape : Tuple [ int ], heads : int = 4 , dim_head : int = 32 , * , key : PRNGKeyArray , use_linear_attention : bool = True , ** kwargs ): super () . __init__ ( ** kwargs ) if use_linear_attention : self . attn = LinearAttention ( input_shape = input_shape , heads = heads , dim_head = dim_head , key = key ) else : self . attn = Attention ( input_shape = input_shape , heads = heads , dim_head = dim_head , key = key ) self . input_shape = self . attn . input_shape H , W , C = input_shape self . norm = eqx . nn . LayerNorm ( shape = ( C ,), use_bias = False ) def data_dependent_init ( self , * args , ** kwargs ) -> eqx . Module : return self def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' normed_x = eqx . filter_vmap ( eqx . filter_vmap ( self . norm ))( x ) out = self . attn ( normed_x ) return out + x __init__ ( self , input_shape : Tuple [ int ], heads : int = 4 , dim_head : int = 32 , * , key : PRNGKeyArray , use_linear_attention : bool = True , ** kwargs ) \u00a4 Source code in generax/nn/layers.py def __init__ ( self , input_shape : Tuple [ int ], heads : int = 4 , dim_head : int = 32 , * , key : PRNGKeyArray , use_linear_attention : bool = True , ** kwargs ): super () . __init__ ( ** kwargs ) if use_linear_attention : self . attn = LinearAttention ( input_shape = input_shape , heads = heads , dim_head = dim_head , key = key ) else : self . attn = Attention ( input_shape = input_shape , heads = heads , dim_head = dim_head , key = key ) self . input_shape = self . attn . input_shape H , W , C = input_shape self . norm = eqx . nn . LayerNorm ( shape = ( C ,), use_bias = False ) data_dependent_init ( self , * args , ** kwargs ) -> Module \u00a4 Source code in generax/nn/layers.py def data_dependent_init ( self , * args , ** kwargs ) -> eqx . Module : return self __call__ ( self , x : Array , y : Array = None ) -> Array \u00a4 Call self as a function. Source code in generax/nn/layers.py def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' normed_x = eqx . filter_vmap ( eqx . filter_vmap ( self . norm ))( x ) out = self . attn ( normed_x ) return out + x generax.nn.grad_wrapper.GradWrapper \u00a4 An easy wrapper around a function that computes the gradient of a scalar function. Source code in generax/nn/grad_wrapper.py class GradWrapper ( eqx . Module ): \"\"\"An easy wrapper around a function that computes the gradient of a scalar function.\"\"\" net : eqx . Module input_shape : Tuple [ int , ... ] def __init__ ( self , net : eqx . Module ): self . net = net self . input_shape = net . input_shape def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' out = self . net ( x , y = y , key = key ) assert out . shape == ( 1 ,) def __call__ ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"**Arguments:** - `t`: A JAX array with shape `()`. - `x`: A JAX array with shape `(input_shape,)`. - `y`: A JAX array with shape `(cond_shape,)`. **Returns:** A JAX array with shape `(input_shape,)`. \"\"\" assert x . shape == self . input_shape def net ( x ): net_out = self . net ( x , y = y , ** kwargs ) if net_out . shape != ( 1 ,): raise ValueError ( f 'Expected net to return a scalar, but got { net_out . shape } ' ) return net_out . ravel () return eqx . filter_grad ( net )( x ) @property def energy ( self ): return self . net energy property readonly \u00a4 __init__ ( self , net : Module ) \u00a4 Source code in generax/nn/grad_wrapper.py def __init__ ( self , net : eqx . Module ): self . net = net self . input_shape = net . input_shape data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> Module \u00a4 Initialize the parameters of the layer based on the data. Arguments : x : The data to initialize the parameters with. y : The conditioning information key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/nn/grad_wrapper.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' out = self . net ( x , y = y , key = key ) assert out . shape == ( 1 ,) __call__ ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Arguments: t : A JAX array with shape () . x : A JAX array with shape (input_shape,) . y : A JAX array with shape (cond_shape,) . Returns: A JAX array with shape (input_shape,) . Source code in generax/nn/grad_wrapper.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"**Arguments:** - `t`: A JAX array with shape `()`. - `x`: A JAX array with shape `(input_shape,)`. - `y`: A JAX array with shape `(cond_shape,)`. **Returns:** A JAX array with shape `(input_shape,)`. \"\"\" assert x . shape == self . input_shape def net ( x ): net_out = self . net ( x , y = y , ** kwargs ) if net_out . shape != ( 1 ,): raise ValueError ( f 'Expected net to return a scalar, but got { net_out . shape } ' ) return net_out . ravel () return eqx . filter_grad ( net )( x ) generax.nn.grad_wrapper.TimeDependentGradWrapper ( GradWrapper ) \u00a4 An easy wrapper around a function that computes the gradient of a scalar function. Source code in generax/nn/grad_wrapper.py class TimeDependentGradWrapper ( GradWrapper ): \"\"\"An easy wrapper around a function that computes the gradient of a scalar function.\"\"\" def data_dependent_init ( self , t : Array , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `t`: The time to initialize the parameters with. - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' out = self . net ( t , x , y = y , key = key ) assert out . shape == ( 1 ,) def __call__ ( self , t : Array , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"**Arguments:** - `t`: A JAX array with shape `()`. - `x`: A JAX array with shape `(input_shape,)`. - `y`: A JAX array with shape `(cond_shape,)`. **Returns:** A JAX array with shape `(input_shape,)`. \"\"\" assert x . shape == self . input_shape def net ( x ): net_out = self . net ( t , x , y = y , ** kwargs ) if net_out . shape != ( 1 ,): raise ValueError ( f 'Expected net to return a scalar, but got { net_out . shape } ' ) return net_out [ 0 ] return eqx . filter_grad ( net )( x ) @property def energy ( self ): return self . net energy property readonly \u00a4 __init__ ( self , net : Module ) \u00a4 Source code in generax/nn/grad_wrapper.py def __init__ ( self , net : eqx . Module ): self . net = net self . input_shape = net . input_shape data_dependent_init ( self , t : Array , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> Module \u00a4 Initialize the parameters of the layer based on the data. Arguments : t : The time to initialize the parameters with. x : The data to initialize the parameters with. y : The conditioning information key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/nn/grad_wrapper.py def data_dependent_init ( self , t : Array , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `t`: The time to initialize the parameters with. - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' out = self . net ( t , x , y = y , key = key ) assert out . shape == ( 1 ,) __call__ ( self , t : Array , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Arguments: t : A JAX array with shape () . x : A JAX array with shape (input_shape,) . y : A JAX array with shape (cond_shape,) . Returns: A JAX array with shape (input_shape,) . Source code in generax/nn/grad_wrapper.py def __call__ ( self , t : Array , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"**Arguments:** - `t`: A JAX array with shape `()`. - `x`: A JAX array with shape `(input_shape,)`. - `y`: A JAX array with shape `(cond_shape,)`. **Returns:** A JAX array with shape `(input_shape,)`. \"\"\" assert x . shape == self . input_shape def net ( x ): net_out = self . net ( t , x , y = y , ** kwargs ) if net_out . shape != ( 1 ,): raise ValueError ( f 'Expected net to return a scalar, but got { net_out . shape } ' ) return net_out [ 0 ] return eqx . filter_grad ( net )( x )","title":"Neural network layers"},{"location":"api/nn/layers/#neural-network-layers","text":"","title":"Neural network layers"},{"location":"api/nn/layers/#generax.nn.layers.WeightNormDense","text":"Weight normalization parametrized linear layer https://arxiv.org/pdf/1602.07868.pdf Source code in generax/nn/layers.py class WeightNormDense ( eqx . Module ): \"\"\"Weight normalization parametrized linear layer https://arxiv.org/pdf/1602.07868.pdf \"\"\" in_size : int = eqx . field ( static = True ) out_size : int = eqx . field ( static = True ) W : Array b : Array g : Array def __init__ ( self , in_size : int , out_size : int , key : PRNGKeyArray , ** kwargs ): super () . __init__ ( ** kwargs ) self . in_size = in_size self . out_size = out_size w_init = jax . nn . initializers . he_uniform ( in_axis =- 2 , out_axis =- 1 ) self . W = w_init ( key , shape = ( out_size , in_size )) self . g = jnp . array ( 1.0 ) self . b = jnp . zeros ( out_size ) def data_dependent_init ( self , x : Array , key : PRNGKeyArray = None , before_square_plus : Optional [ bool ] = False ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `key`: A `jax.random.PRNGKey` for initialization - `before_square_plus`: In case we want the activations after square plus to be gaussian **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ - 1 ] == self . in_size , 'Only works on batched data' # Initialize g and b. W = self . W * jax . lax . rsqrt (( self . W ** 2 ) . sum ( axis = 1 ))[:, None ] x = jnp . einsum ( 'ij,bj->bi' , W , x ) std = jnp . std ( x . reshape (( - 1 , x . shape [ - 1 ])), axis = 0 ) + 1e-5 if before_square_plus : std = std - 1 / std g = 1 / std x *= g mean = jnp . mean ( x . reshape (( - 1 , x . shape [ - 1 ])), axis = 0 ) b = - mean # Turn the new parameters into a new module get_g = lambda tree : tree . g get_b = lambda tree : tree . b updated_layer = eqx . tree_at ( get_g , self , g ) updated_layer = eqx . tree_at ( get_b , updated_layer , b ) return updated_layer def __call__ ( self , x : Array , y : Array = None ) -> Array : W = self . W * jax . lax . rsqrt (( self . W ** 2 ) . sum ( axis = 1 ))[:, None ] x = self . g * ( W @x ) + self . b return x","title":"WeightNormDense"},{"location":"api/nn/layers/#generax.nn.layers.WeightNormConv","text":"Weight normalization parametrized convolutional layer https://arxiv.org/pdf/1602.07868.pdf Source code in generax/nn/layers.py class WeightNormConv ( eqx . Module ): \"\"\"Weight normalization parametrized convolutional layer https://arxiv.org/pdf/1602.07868.pdf \"\"\" input_shape : int = eqx . field ( static = True ) out_size : int = eqx . field ( static = True ) filter_shape : Tuple [ int ] = eqx . field ( static = True ) padding : Union [ int , str ] = eqx . field ( static = True ) stride : int = eqx . field ( static = True ) W : Array b : Array g : Array def __init__ ( self , input_shape : Tuple [ int ], # in_channels filter_shape : Tuple [ int ], out_size : int , * , key : PRNGKeyArray , padding : Union [ int , str ] = 'SAME' , stride : int = 1 , ** kwargs ): super () . __init__ ( ** kwargs ) H , W , C = input_shape self . input_shape = input_shape self . filter_shape = filter_shape self . out_size = out_size self . padding = padding self . stride = stride w_init = jax . nn . initializers . he_uniform ( in_axis =- 2 , out_axis =- 1 ) self . W = w_init ( key , shape = self . filter_shape + ( C , out_size )) self . g = jnp . array ( 1.0 ) self . b = jnp . zeros ( out_size ) def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None , before_square_plus : Optional [ bool ] = False ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `key`: A `jax.random.PRNGKey` for initialization - `before_square_plus`: In case we want the activations after square plus to be gaussian **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' # Initialize g and b. W = self . W * jax . lax . rsqrt (( self . W ** 2 ) . sum ( axis = ( 0 , 1 , 2 )))[ None , None , None ,:] x = util . conv ( W , x , stride = self . stride , padding = self . padding ) std = jnp . std ( x . reshape (( - 1 , x . shape [ - 1 ])), axis = 0 ) + 1e-5 if before_square_plus : std = std - 1 / std g = 1 / std x *= g mean = jnp . mean ( x . reshape (( - 1 , x . shape [ - 1 ])), axis = 0 ) b = - mean # Turn the new parameters into a new module get_g = lambda tree : tree . g get_b = lambda tree : tree . b updated_layer = eqx . tree_at ( get_g , self , g ) updated_layer = eqx . tree_at ( get_b , updated_layer , b ) return updated_layer def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' W = self . W * jax . lax . rsqrt (( self . W ** 2 ) . sum ( axis = ( 0 , 1 , 2 )))[ None , None , None ,:] x = self . g * util . conv ( W , x , stride = self . stride , padding = self . padding ) + self . b return x","title":"WeightNormConv"},{"location":"api/nn/layers/#generax.nn.layers.WeightStandardizedConv","text":"Weight standardized parametrized convolutional layer https://arxiv.org/pdf/1903.10520.pdf Source code in generax/nn/layers.py class WeightStandardizedConv ( eqx . Module ): \"\"\"Weight standardized parametrized convolutional layer https://arxiv.org/pdf/1903.10520.pdf \"\"\" input_shape : int = eqx . field ( static = True ) out_size : int = eqx . field ( static = True ) filter_shape : Tuple [ int ] = eqx . field ( static = True ) padding : Union [ int , str ] = eqx . field ( static = True ) stride : int = eqx . field ( static = True ) W : Array b : Array def __init__ ( self , input_shape : Tuple [ int ], # in_channels filter_shape : Tuple [ int ], out_size : int , * , key : PRNGKeyArray , padding : Union [ int , str ] = 'SAME' , stride : int = 1 , ** kwargs ): super () . __init__ ( ** kwargs ) H , W , C = input_shape self . input_shape = input_shape self . filter_shape = filter_shape self . out_size = out_size self . padding = padding self . stride = stride w_init = jax . nn . initializers . he_uniform ( in_axis =- 2 , out_axis =- 1 ) self . W = w_init ( key , shape = self . filter_shape + ( C , out_size )) self . b = jnp . zeros ( out_size ) def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" axes = ( 0 , 1 , 2 ) mean = jnp . mean ( self . W , axis = axes , keepdims = True ) var = jnp . var ( self . W , axis = axes , keepdims = True ) W_hat = ( self . W - mean ) / jnp . sqrt ( var + 1e-5 ) x = util . conv ( W_hat , x , stride = self . stride , padding = self . padding ) # Initialize b. mean = jnp . mean ( x . reshape (( - 1 , x . shape [ - 1 ])), axis = 0 ) b = - mean # Turn the new parameters into a new module get_b = lambda tree : tree . b updated_layer = eqx . tree_at ( get_b , self , b ) return updated_layer def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' axes = ( 0 , 1 , 2 ) mean = jnp . mean ( self . W , axis = axes , keepdims = True ) var = jnp . var ( self . W , axis = axes , keepdims = True ) H , W , C_in , C_out = self . W . shape fan_in = H * W * C_in W_hat = ( self . W - mean ) * jax . lax . rsqrt ( fan_in * var + 1e-5 ) x = util . conv ( W_hat , x , stride = self . stride , padding = self . padding ) + self . b return x","title":"WeightStandardizedConv"},{"location":"api/nn/layers/#generax.nn.layers.ChannelConvention","text":"ChannelConvention( args, *kwargs) Source code in generax/nn/layers.py class ChannelConvention ( eqx . Module ): module : eqx . Module def __init__ ( self , module : eqx . Module ): super () . __init__ () self . module = module def __call__ ( self , x ): x = einops . rearrange ( x , 'H W C -> C H W' ) x = self . module ( x ) x = einops . rearrange ( x , 'C H W -> H W C' ) return x","title":"ChannelConvention"},{"location":"api/nn/layers/#generax.nn.layers.ConvAndGroupNorm","text":"Weight standardized conv + group norm Source code in generax/nn/layers.py class ConvAndGroupNorm ( eqx . Module ): \"\"\"Weight standardized conv + group norm \"\"\" input_shape : int = eqx . field ( static = True ) conv : WeightStandardizedConv norm : eqx . nn . GroupNorm def __init__ ( self , input_shape : Tuple [ int ], # in_channels filter_shape : Tuple [ int ], out_size : int , groups : int , * , key : PRNGKeyArray , padding : Union [ int , str ] = 'SAME' , stride : int = 1 , ** kwargs ): super () . __init__ ( ** kwargs ) if out_size % groups != 0 : raise ValueError ( \"The number of groups must divide the number of channels.\" ) self . conv = WeightStandardizedConv ( input_shape = input_shape , filter_shape = filter_shape , out_size = out_size , key = key , padding = padding , stride = stride ) self . norm = ChannelConvention ( eqx . nn . GroupNorm ( groups = groups , channels = out_size )) self . input_shape = self . conv . input_shape def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , shift_scale : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" new_conv = self . conv . data_dependent_init ( x , y , key = key ) get_conv = lambda tree : tree . conv updated_layer = eqx . tree_at ( get_conv , self , new_conv ) return updated_layer def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' x = self . conv ( x ) x = self . norm ( x ) return x","title":"ConvAndGroupNorm"},{"location":"api/nn/layers/#generax.nn.layers.Upsample","text":"https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf Source code in generax/nn/layers.py class Upsample ( eqx . Module ): \"\"\"https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf \"\"\" input_shape : int = eqx . field ( static = True ) out_size : int = eqx . field ( static = True ) conv : WeightStandardizedConv def __init__ ( self , input_shape : Tuple [ int ], out_size : Optional [ int ] = None , * , key : PRNGKeyArray , ** kwargs ): super () . __init__ ( ** kwargs ) H , W , C = input_shape self . input_shape = input_shape self . out_size = out_size if out_size is not None else C self . conv = WeightStandardizedConv ( input_shape = ( H , W , C ), filter_shape = ( 3 , 3 ), out_size = 4 * self . out_size , key = key ) def data_dependent_init ( self , * args , ** kwargs ) -> eqx . Module : return self def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' H , W , C = x . shape x = self . conv ( x ) x = jax . nn . silu ( x ) x = einops . rearrange ( x , 'h w (c k1 k2) -> (h k1) (w k2) c' , k1 = 2 , k2 = 2 ) assert x . shape == ( H * 2 , W * 2 , self . out_size ) return x","title":"Upsample"},{"location":"api/nn/layers/#generax.nn.layers.Downsample","text":"Downsample( args, *kwargs) Source code in generax/nn/layers.py class Downsample ( eqx . Module ): input_shape : int = eqx . field ( static = True ) out_size : int = eqx . field ( static = True ) conv : WeightStandardizedConv def __init__ ( self , input_shape : Tuple [ int ], out_size : Optional [ int ] = None , * , key : PRNGKeyArray , ** kwargs ): super () . __init__ ( ** kwargs ) H , W , C = input_shape self . input_shape = input_shape self . out_size = out_size if out_size is not None else C self . conv = WeightStandardizedConv ( input_shape = ( H // 2 , W // 2 , C * 4 ), filter_shape = ( 3 , 3 ), out_size = self . out_size , key = key ) def data_dependent_init ( self , * args , ** kwargs ) -> eqx . Module : return self def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' H , W , C = x . shape x = einops . rearrange ( x , '(h k1) (w k2) c -> h w (c k1 k2)' , k1 = 2 , k2 = 2 ) x = self . conv ( x ) assert x . shape == ( H // 2 , W // 2 , self . out_size ) return x","title":"Downsample"},{"location":"api/nn/layers/#generax.nn.layers.GatedGlobalContext","text":"Modified version of https://arxiv.org/pdf/1904.11492.pdf used in imagen https://github.com/lucidrains/imagen-pytorch/ Source code in generax/nn/layers.py class GatedGlobalContext ( eqx . Module ): \"\"\"Modified version of https://arxiv.org/pdf/1904.11492.pdf used in imagen https://github.com/lucidrains/imagen-pytorch/\"\"\" input_shape : int = eqx . field ( static = True ) linear1 : WeightNormConv linear2 : WeightNormConv context_conv : WeightNormConv def __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray , ** kwargs ): super () . __init__ ( ** kwargs ) H , W , C = input_shape self . input_shape = input_shape out_size = C hidden_dim = max ( 3 , out_size // 2 ) k1 , k2 , k3 = random . split ( key , 3 ) self . linear1 = WeightNormDense ( in_size = C , out_size = hidden_dim , key = k1 ) self . linear2 = WeightNormDense ( in_size = hidden_dim , out_size = out_size , key = k2 ) self . context_conv = WeightNormConv ( input_shape = input_shape , filter_shape = ( 1 , 1 ), out_size = 1 , key = k3 ) def data_dependent_init ( self , * args , ** kwargs ) -> eqx . Module : return self def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' x_in = x H , W , C = x . shape # Reduce channels to (H, W, 1) context = self . context_conv ( x ) # Flatten c_flat = einops . rearrange ( context , 'h w c -> (h w) c' ) x_flat = einops . rearrange ( x , 'h w c -> (h w) c' ) # Context over the pixels c_sm = jax . nn . softmax ( c_flat , axis = 0 ) # Reweight the channels out = jnp . einsum ( 'tu,tv->uv' , c_sm , x_flat ) assert out . shape == ( 1 , C ) out = out [ 0 ] out = self . linear1 ( out ) out = jax . nn . silu ( out ) out = self . linear2 ( out ) out = jax . nn . sigmoid ( out ) return x_in * out [ None , None ,:]","title":"GatedGlobalContext"},{"location":"api/nn/layers/#generax.nn.layers.Attention","text":"Attention( args, *kwargs) Source code in generax/nn/layers.py class Attention ( eqx . Module ): input_shape : int = eqx . field ( static = True ) heads : int = eqx . field ( static = True ) dim_head : int = eqx . field ( static = True ) scale : float = eqx . field ( static = True ) conv_in : eqx . nn . Conv3d conv_out : eqx . nn . Conv3d def __init__ ( self , input_shape : Tuple [ int ], heads : int = 4 , dim_head : int = 32 , scale : float = 10 , * , key : PRNGKeyArray , ** kwargs ): super () . __init__ ( ** kwargs ) H , W , C = input_shape self . input_shape = input_shape self . heads = heads self . dim_head = dim_head self . scale = scale k1 , k2 = random . split ( key , 2 ) dim = self . dim_head * self . heads self . conv_in = ChannelConvention ( eqx . nn . Conv2d ( in_channels = C , out_channels = 3 * dim , kernel_size = 1 , use_bias = False , key = k1 )) self . conv_out = ChannelConvention ( eqx . nn . Conv2d ( in_channels = dim , out_channels = C , kernel_size = 1 , use_bias = True , key = k2 )) def data_dependent_init ( self , * args , ** kwargs ) -> eqx . Module : return self def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' H , W , C = x . shape qkv = self . conv_in ( x ) # (H, W, heads*dim_head*3) qkv = einops . rearrange ( qkv , 'H W (u h d) -> (H W) h d u' , h = self . heads , d = self . dim_head , u = 3 ) q , k , v = jnp . split ( qkv , 3 , axis =- 1 ) q , k , v = q [ ... , 0 ], k [ ... , 0 ], v [ ... , 0 ] assert q . shape == k . shape == v . shape == ( H * W , self . heads , self . dim_head ) def normalize ( x ): return x / jnp . clip ( jnp . linalg . norm ( x , axis = 0 , keepdims = True ), 1e-8 ) q , k = normalize ( q ), normalize ( k ) sim = jnp . einsum ( 'ihd,jhd->hij' , q , k ) * self . scale attn = jax . nn . softmax ( sim , axis =- 1 ) assert attn . shape == ( self . heads , H * W , H * W ) out = jnp . einsum ( 'hij,jhd->hid' , attn , v ) out = einops . rearrange ( out , 'h (H W) d -> H W (h d)' , H = H , W = W , h = self . heads , d = self . dim_head ) assert out . shape == ( H , W , self . dim_head * self . heads ) out = self . conv_out ( out ) return out","title":"Attention"},{"location":"api/nn/layers/#generax.nn.layers.LinearAttention","text":"LinearAttention( args, *kwargs) Source code in generax/nn/layers.py class LinearAttention ( eqx . Module ): input_shape : int = eqx . field ( static = True ) heads : int = eqx . field ( static = True ) dim_head : int = eqx . field ( static = True ) conv_in : eqx . nn . Conv3d conv_out : eqx . nn . Conv3d norm : eqx . nn . LayerNorm def __init__ ( self , input_shape : Tuple [ int ], heads : int = 4 , dim_head : int = 32 , * , key : PRNGKeyArray , ** kwargs ): super () . __init__ ( ** kwargs ) H , W , C = input_shape self . input_shape = input_shape self . heads = heads self . dim_head = dim_head k1 , k2 = random . split ( key , 2 ) dim = self . dim_head * self . heads self . conv_in = ChannelConvention ( eqx . nn . Conv2d ( in_channels = C , out_channels = 3 * dim , kernel_size = 1 , use_bias = False , key = k1 )) self . conv_out = ChannelConvention ( eqx . nn . Conv2d ( in_channels = dim , out_channels = C , kernel_size = 1 , use_bias = True , key = k2 )) self . norm = eqx . nn . LayerNorm ( shape = ( C ,), use_bias = False ) def data_dependent_init ( self , * args , ** kwargs ) -> eqx . Module : return self def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' H , W , C = x . shape qkv = self . conv_in ( x ) # (H, W, heads*dim_head*3) qkv = einops . rearrange ( qkv , 'H W (u h d) -> (H W) h d u' , h = self . heads , d = self . dim_head , u = 3 ) q , k , v = jnp . split ( qkv , 3 , axis =- 1 ) q , k , v = q [ ... , 0 ], k [ ... , 0 ], v [ ... , 0 ] assert q . shape == k . shape == v . shape == ( H * W , self . heads , self . dim_head ) q = jax . nn . softmax ( q , axis =- 1 ) k = jax . nn . softmax ( k , axis =- 3 ) q = q / jnp . sqrt ( self . dim_head ) v = v / ( H * W ) context = jnp . einsum ( \"n h d, n h e -> h d e\" , k , v ) out = jnp . einsum ( \"h d e, n h d -> h e n\" , context , q ) out = einops . rearrange ( out , \"h e (x y) -> x y (h e)\" , x = H ) assert out . shape == ( H , W , self . dim_head * self . heads ) out = self . conv_out ( out ) out = eqx . filter_vmap ( eqx . filter_vmap ( self . norm ))( out ) return out","title":"LinearAttention"},{"location":"api/nn/layers/#generax.nn.layers.AttentionBlock","text":"AttentionBlock( args, *kwargs) Source code in generax/nn/layers.py class AttentionBlock ( eqx . Module ): input_shape : int = eqx . field ( static = True ) attn : Union [ Attention , LinearAttention ] norm : eqx . nn . LayerNorm def __init__ ( self , input_shape : Tuple [ int ], heads : int = 4 , dim_head : int = 32 , * , key : PRNGKeyArray , use_linear_attention : bool = True , ** kwargs ): super () . __init__ ( ** kwargs ) if use_linear_attention : self . attn = LinearAttention ( input_shape = input_shape , heads = heads , dim_head = dim_head , key = key ) else : self . attn = Attention ( input_shape = input_shape , heads = heads , dim_head = dim_head , key = key ) self . input_shape = self . attn . input_shape H , W , C = input_shape self . norm = eqx . nn . LayerNorm ( shape = ( C ,), use_bias = False ) def data_dependent_init ( self , * args , ** kwargs ) -> eqx . Module : return self def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' normed_x = eqx . filter_vmap ( eqx . filter_vmap ( self . norm ))( x ) out = self . attn ( normed_x ) return out + x","title":"AttentionBlock"},{"location":"api/nn/layers/#generax.nn.grad_wrapper.GradWrapper","text":"An easy wrapper around a function that computes the gradient of a scalar function. Source code in generax/nn/grad_wrapper.py class GradWrapper ( eqx . Module ): \"\"\"An easy wrapper around a function that computes the gradient of a scalar function.\"\"\" net : eqx . Module input_shape : Tuple [ int , ... ] def __init__ ( self , net : eqx . Module ): self . net = net self . input_shape = net . input_shape def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' out = self . net ( x , y = y , key = key ) assert out . shape == ( 1 ,) def __call__ ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"**Arguments:** - `t`: A JAX array with shape `()`. - `x`: A JAX array with shape `(input_shape,)`. - `y`: A JAX array with shape `(cond_shape,)`. **Returns:** A JAX array with shape `(input_shape,)`. \"\"\" assert x . shape == self . input_shape def net ( x ): net_out = self . net ( x , y = y , ** kwargs ) if net_out . shape != ( 1 ,): raise ValueError ( f 'Expected net to return a scalar, but got { net_out . shape } ' ) return net_out . ravel () return eqx . filter_grad ( net )( x ) @property def energy ( self ): return self . net","title":"GradWrapper"},{"location":"api/nn/layers/#generax.nn.grad_wrapper.TimeDependentGradWrapper","text":"An easy wrapper around a function that computes the gradient of a scalar function. Source code in generax/nn/grad_wrapper.py class TimeDependentGradWrapper ( GradWrapper ): \"\"\"An easy wrapper around a function that computes the gradient of a scalar function.\"\"\" def data_dependent_init ( self , t : Array , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `t`: The time to initialize the parameters with. - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' out = self . net ( t , x , y = y , key = key ) assert out . shape == ( 1 ,) def __call__ ( self , t : Array , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"**Arguments:** - `t`: A JAX array with shape `()`. - `x`: A JAX array with shape `(input_shape,)`. - `y`: A JAX array with shape `(cond_shape,)`. **Returns:** A JAX array with shape `(input_shape,)`. \"\"\" assert x . shape == self . input_shape def net ( x ): net_out = self . net ( t , x , y = y , ** kwargs ) if net_out . shape != ( 1 ,): raise ValueError ( f 'Expected net to return a scalar, but got { net_out . shape } ' ) return net_out [ 0 ] return eqx . filter_grad ( net )( x ) @property def energy ( self ): return self . net","title":"TimeDependentGradWrapper"},{"location":"api/nn/resnet/","text":"Resnet \u00a4 generax.nn.resnet.ResNet \u00a4 ResNet for 1d data Source code in generax/nn/resnet.py class ResNet ( eqx . Module ): \"\"\"ResNet for 1d data\"\"\" n_blocks : int = eqx . field ( static = True ) blocks : tuple [ GatedResBlock , ... ] in_projection : eqx . nn . Linear out_projection : eqx . nn . Linear input_shape : int = eqx . field ( static = True ) cond_shape : int = eqx . field ( static = True ) working_size : int = eqx . field ( static = True ) hidden_size : int = eqx . field ( static = True ) out_size : int = eqx . field ( static = True ) filter_shape : Union [ Tuple [ int ], None ] = eqx . field ( static = True ) def __init__ ( self , input_shape : Tuple [ int ], working_size : int , hidden_size : int , out_size : int , n_blocks : int , filter_shape : Optional [ Tuple [ int ]] = ( 3 , 3 ), cond_shape : Optional [ Tuple [ int ]] = None , activation : Callable = jax . nn . swish , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input size. Output size is the same as input_shape. - `working_size`: The size (channels for images) of each hidden layer. - `hidden_size`: The size (channels for images) of each hidden layer. - `out_size`: The output size. For images, this is the number of output channels. - `n_blocks`: The number of residual blocks. - `cond_shape`: The size of the conditioning information. - `activation`: The activation function in each residual block. - `key`: A `jax.random.PRNGKey` for initialization. \"\"\" super () . __init__ ( ** kwargs ) if len ( input_shape ) not in [ 1 , 3 ]: raise ValueError ( f 'Expected 1d or 3d input shape' ) image = False if len ( input_shape ) == 3 : H , W , C = input_shape image = True assert filter_shape is not None , 'Must pass in filter shape when processing images' self . n_blocks = n_blocks self . working_size = working_size self . hidden_size = hidden_size self . filter_shape = filter_shape self . out_size = out_size k1 , k2 , k3 = random . split ( key , 3 ) if isinstance ( input_shape , int ): input_shape = ( input_shape ,) self . input_shape = input_shape self . cond_shape = cond_shape if image == False : self . in_projection = WeightNormDense ( in_size = input_shape [ 0 ], out_size = working_size , key = k1 ) working_shape = ( working_size ,) else : self . in_projection = ConvAndGroupNorm ( input_shape = input_shape , out_size = working_size , filter_shape = filter_shape , groups = 1 , key = k1 ) working_shape = ( H , W , working_size ) def make_resblock ( k ): return GatedResBlock ( input_shape = working_shape , hidden_size = hidden_size , cond_shape = cond_shape , activation = activation , filter_shape = filter_shape , key = k ) keys = random . split ( k2 , n_blocks ) self . blocks = eqx . filter_vmap ( make_resblock )( keys ) if image == False : self . out_projection = WeightNormDense ( in_size = working_size , out_size = out_size , key = k3 ) else : self . out_projection = ConvAndGroupNorm ( input_shape = working_shape , out_size = out_size , filter_shape = filter_shape , groups = 1 , key = k3 ) def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' k1 , k2 , k3 = random . split ( key , 3 ) # Input projection in_proj = self . in_projection . data_dependent_init ( x , key = k1 ) x = eqx . filter_vmap ( in_proj )( x ) # Scan over the vmapped blocks params , state = eqx . partition ( self . blocks , eqx . is_array ) def scan_body ( x , inputs ): key , block_params = inputs block = eqx . combine ( block_params , state ) new_block = block . data_dependent_init ( x , y , key = key ) new_x = eqx . filter_vmap ( new_block )( x , y ) new_params , _ = eqx . partition ( block , eqx . is_array ) return new_x , new_params keys = random . split ( k2 , self . n_blocks ) x , params = jax . lax . scan ( scan_body , x , ( keys , params )) blocks = eqx . combine ( params , state ) out_proj = self . out_projection . data_dependent_init ( x , key = k3 ) # Turn the new parameters into a new module get_in_proj = lambda tree : tree . in_projection get_blocks = lambda tree : tree . blocks get_out_proj = lambda tree : tree . out_projection updated_layer = eqx . tree_at ( get_in_proj , self , in_proj ) updated_layer = eqx . tree_at ( get_blocks , updated_layer , blocks ) updated_layer = eqx . tree_at ( get_out_proj , updated_layer , out_proj ) return updated_layer def __call__ ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"**Arguments:** - `t`: A JAX array with shape `()`. - `x`: A JAX array with shape `(input_shape,)`. - `y`: A JAX array with shape `(cond_shape,)`. **Returns:** A JAX array with shape `(input_shape,)`. \"\"\" assert x . shape == self . input_shape # Input projection x = self . in_projection ( x ) # Resnet blocks dynamic , static = eqx . partition ( self . blocks , eqx . is_array ) def f ( x , params ): block = eqx . combine ( params , static ) return block ( x , y ), None out , _ = jax . lax . scan ( f , x , dynamic ) # Output projection out = self . out_projection ( out ) return out __init__ ( self , input_shape : Tuple [ int ], working_size : int , hidden_size : int , out_size : int , n_blocks : int , filter_shape : Optional [ Tuple [ int ]] = ( 3 , 3 ), cond_shape : Optional [ Tuple [ int ]] = None , activation : Callable = < PjitFunction of < function silu at 0x7fd99f02dfc0 >> , * , key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : input_shape : The input size. Output size is the same as input_shape. working_size : The size (channels for images) of each hidden layer. hidden_size : The size (channels for images) of each hidden layer. out_size : The output size. For images, this is the number of output channels. n_blocks : The number of residual blocks. cond_shape : The size of the conditioning information. activation : The activation function in each residual block. key : A jax.random.PRNGKey for initialization. Source code in generax/nn/resnet.py def __init__ ( self , input_shape : Tuple [ int ], working_size : int , hidden_size : int , out_size : int , n_blocks : int , filter_shape : Optional [ Tuple [ int ]] = ( 3 , 3 ), cond_shape : Optional [ Tuple [ int ]] = None , activation : Callable = jax . nn . swish , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input size. Output size is the same as input_shape. - `working_size`: The size (channels for images) of each hidden layer. - `hidden_size`: The size (channels for images) of each hidden layer. - `out_size`: The output size. For images, this is the number of output channels. - `n_blocks`: The number of residual blocks. - `cond_shape`: The size of the conditioning information. - `activation`: The activation function in each residual block. - `key`: A `jax.random.PRNGKey` for initialization. \"\"\" super () . __init__ ( ** kwargs ) if len ( input_shape ) not in [ 1 , 3 ]: raise ValueError ( f 'Expected 1d or 3d input shape' ) image = False if len ( input_shape ) == 3 : H , W , C = input_shape image = True assert filter_shape is not None , 'Must pass in filter shape when processing images' self . n_blocks = n_blocks self . working_size = working_size self . hidden_size = hidden_size self . filter_shape = filter_shape self . out_size = out_size k1 , k2 , k3 = random . split ( key , 3 ) if isinstance ( input_shape , int ): input_shape = ( input_shape ,) self . input_shape = input_shape self . cond_shape = cond_shape if image == False : self . in_projection = WeightNormDense ( in_size = input_shape [ 0 ], out_size = working_size , key = k1 ) working_shape = ( working_size ,) else : self . in_projection = ConvAndGroupNorm ( input_shape = input_shape , out_size = working_size , filter_shape = filter_shape , groups = 1 , key = k1 ) working_shape = ( H , W , working_size ) def make_resblock ( k ): return GatedResBlock ( input_shape = working_shape , hidden_size = hidden_size , cond_shape = cond_shape , activation = activation , filter_shape = filter_shape , key = k ) keys = random . split ( k2 , n_blocks ) self . blocks = eqx . filter_vmap ( make_resblock )( keys ) if image == False : self . out_projection = WeightNormDense ( in_size = working_size , out_size = out_size , key = k3 ) else : self . out_projection = ConvAndGroupNorm ( input_shape = working_shape , out_size = out_size , filter_shape = filter_shape , groups = 1 , key = k3 ) data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> Module \u00a4 Initialize the parameters of the layer based on the data. Arguments : x : The data to initialize the parameters with. y : The conditioning information key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/nn/resnet.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' k1 , k2 , k3 = random . split ( key , 3 ) # Input projection in_proj = self . in_projection . data_dependent_init ( x , key = k1 ) x = eqx . filter_vmap ( in_proj )( x ) # Scan over the vmapped blocks params , state = eqx . partition ( self . blocks , eqx . is_array ) def scan_body ( x , inputs ): key , block_params = inputs block = eqx . combine ( block_params , state ) new_block = block . data_dependent_init ( x , y , key = key ) new_x = eqx . filter_vmap ( new_block )( x , y ) new_params , _ = eqx . partition ( block , eqx . is_array ) return new_x , new_params keys = random . split ( k2 , self . n_blocks ) x , params = jax . lax . scan ( scan_body , x , ( keys , params )) blocks = eqx . combine ( params , state ) out_proj = self . out_projection . data_dependent_init ( x , key = k3 ) # Turn the new parameters into a new module get_in_proj = lambda tree : tree . in_projection get_blocks = lambda tree : tree . blocks get_out_proj = lambda tree : tree . out_projection updated_layer = eqx . tree_at ( get_in_proj , self , in_proj ) updated_layer = eqx . tree_at ( get_blocks , updated_layer , blocks ) updated_layer = eqx . tree_at ( get_out_proj , updated_layer , out_proj ) return updated_layer __call__ ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Arguments: t : A JAX array with shape () . x : A JAX array with shape (input_shape,) . y : A JAX array with shape (cond_shape,) . Returns: A JAX array with shape (input_shape,) . Source code in generax/nn/resnet.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"**Arguments:** - `t`: A JAX array with shape `()`. - `x`: A JAX array with shape `(input_shape,)`. - `y`: A JAX array with shape `(cond_shape,)`. **Returns:** A JAX array with shape `(input_shape,)`. \"\"\" assert x . shape == self . input_shape # Input projection x = self . in_projection ( x ) # Resnet blocks dynamic , static = eqx . partition ( self . blocks , eqx . is_array ) def f ( x , params ): block = eqx . combine ( params , static ) return block ( x , y ), None out , _ = jax . lax . scan ( f , x , dynamic ) # Output projection out = self . out_projection ( out ) return out generax.nn.resnet.TimeDependentResNet ( ResNet ) \u00a4 A time dependent version of a 1d resnet Source code in generax/nn/resnet.py class TimeDependentResNet ( ResNet ): \"\"\"A time dependent version of a 1d resnet \"\"\" time_features : TimeFeatures def __init__ ( self , input_shape : Tuple [ int ], working_size : int , hidden_size : int , out_size : int , n_blocks : int , filter_shape : Optional [ Tuple [ int ]] = ( 3 , 3 ), cond_shape : Optional [ Tuple [ int ]] = None , activation : Callable = jax . nn . swish , embedding_size : Optional [ int ] = 16 , out_features : int = 8 , * , key : PRNGKeyArray , ** kwargs ): k1 , k2 = random . split ( key , 2 ) self . time_features = TimeFeatures ( embedding_size = embedding_size , out_features = out_features , key = k1 , ** kwargs ) total_cond_size = out_features if cond_shape is not None : if len ( cond_shape ) != 1 : raise ValueError ( f 'Expected 1d conditional input.' ) total_cond_size += cond_shape [ 0 ] super () . __init__ ( input_shape = input_shape , working_size = working_size , hidden_size = hidden_size , out_size = out_size , n_blocks = n_blocks , filter_shape = filter_shape , cond_shape = ( total_cond_size ,), activation = activation , key = k2 , ** kwargs ) def data_dependent_init ( self , t : Array , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `t`: The time to initialize the parameters with. - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert t . ndim == 1 h = eqx . filter_vmap ( self . time_features )( t ) if y is not None : h = jnp . concatenate ([ h , y ], axis =- 1 ) return super () . data_dependent_init ( x , y = h , key = key ) def __call__ ( self , t : Array , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : assert t . shape == () h = self . time_features ( t ) if y is not None : h = jnp . concatenate ([ h , y ], axis =- 1 ) return super () . __call__ ( x , y = h ) __init__ ( self , input_shape : Tuple [ int ], working_size : int , hidden_size : int , out_size : int , n_blocks : int , filter_shape : Optional [ Tuple [ int ]] = ( 3 , 3 ), cond_shape : Optional [ Tuple [ int ]] = None , activation : Callable = < PjitFunction of < function silu at 0x7fd99f02dfc0 >> , embedding_size : Optional [ int ] = 16 , out_features : int = 8 , * , key : PRNGKeyArray , ** kwargs ) \u00a4 Source code in generax/nn/resnet.py def __init__ ( self , input_shape : Tuple [ int ], working_size : int , hidden_size : int , out_size : int , n_blocks : int , filter_shape : Optional [ Tuple [ int ]] = ( 3 , 3 ), cond_shape : Optional [ Tuple [ int ]] = None , activation : Callable = jax . nn . swish , embedding_size : Optional [ int ] = 16 , out_features : int = 8 , * , key : PRNGKeyArray , ** kwargs ): k1 , k2 = random . split ( key , 2 ) self . time_features = TimeFeatures ( embedding_size = embedding_size , out_features = out_features , key = k1 , ** kwargs ) total_cond_size = out_features if cond_shape is not None : if len ( cond_shape ) != 1 : raise ValueError ( f 'Expected 1d conditional input.' ) total_cond_size += cond_shape [ 0 ] super () . __init__ ( input_shape = input_shape , working_size = working_size , hidden_size = hidden_size , out_size = out_size , n_blocks = n_blocks , filter_shape = filter_shape , cond_shape = ( total_cond_size ,), activation = activation , key = k2 , ** kwargs ) data_dependent_init ( self , t : Array , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> Module \u00a4 Initialize the parameters of the layer based on the data. Arguments : t : The time to initialize the parameters with. x : The data to initialize the parameters with. y : The conditioning information key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/nn/resnet.py def data_dependent_init ( self , t : Array , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `t`: The time to initialize the parameters with. - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert t . ndim == 1 h = eqx . filter_vmap ( self . time_features )( t ) if y is not None : h = jnp . concatenate ([ h , y ], axis =- 1 ) return super () . data_dependent_init ( x , y = h , key = key ) __call__ ( self , t : Array , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Arguments: t : A JAX array with shape () . x : A JAX array with shape (input_shape,) . y : A JAX array with shape (cond_shape,) . Returns: A JAX array with shape (input_shape,) . Source code in generax/nn/resnet.py def __call__ ( self , t : Array , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : assert t . shape == () h = self . time_features ( t ) if y is not None : h = jnp . concatenate ([ h , y ], axis =- 1 ) return super () . __call__ ( x , y = h )","title":"Resnet"},{"location":"api/nn/resnet/#resnet","text":"","title":"Resnet"},{"location":"api/nn/resnet/#generax.nn.resnet.ResNet","text":"ResNet for 1d data Source code in generax/nn/resnet.py class ResNet ( eqx . Module ): \"\"\"ResNet for 1d data\"\"\" n_blocks : int = eqx . field ( static = True ) blocks : tuple [ GatedResBlock , ... ] in_projection : eqx . nn . Linear out_projection : eqx . nn . Linear input_shape : int = eqx . field ( static = True ) cond_shape : int = eqx . field ( static = True ) working_size : int = eqx . field ( static = True ) hidden_size : int = eqx . field ( static = True ) out_size : int = eqx . field ( static = True ) filter_shape : Union [ Tuple [ int ], None ] = eqx . field ( static = True ) def __init__ ( self , input_shape : Tuple [ int ], working_size : int , hidden_size : int , out_size : int , n_blocks : int , filter_shape : Optional [ Tuple [ int ]] = ( 3 , 3 ), cond_shape : Optional [ Tuple [ int ]] = None , activation : Callable = jax . nn . swish , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input size. Output size is the same as input_shape. - `working_size`: The size (channels for images) of each hidden layer. - `hidden_size`: The size (channels for images) of each hidden layer. - `out_size`: The output size. For images, this is the number of output channels. - `n_blocks`: The number of residual blocks. - `cond_shape`: The size of the conditioning information. - `activation`: The activation function in each residual block. - `key`: A `jax.random.PRNGKey` for initialization. \"\"\" super () . __init__ ( ** kwargs ) if len ( input_shape ) not in [ 1 , 3 ]: raise ValueError ( f 'Expected 1d or 3d input shape' ) image = False if len ( input_shape ) == 3 : H , W , C = input_shape image = True assert filter_shape is not None , 'Must pass in filter shape when processing images' self . n_blocks = n_blocks self . working_size = working_size self . hidden_size = hidden_size self . filter_shape = filter_shape self . out_size = out_size k1 , k2 , k3 = random . split ( key , 3 ) if isinstance ( input_shape , int ): input_shape = ( input_shape ,) self . input_shape = input_shape self . cond_shape = cond_shape if image == False : self . in_projection = WeightNormDense ( in_size = input_shape [ 0 ], out_size = working_size , key = k1 ) working_shape = ( working_size ,) else : self . in_projection = ConvAndGroupNorm ( input_shape = input_shape , out_size = working_size , filter_shape = filter_shape , groups = 1 , key = k1 ) working_shape = ( H , W , working_size ) def make_resblock ( k ): return GatedResBlock ( input_shape = working_shape , hidden_size = hidden_size , cond_shape = cond_shape , activation = activation , filter_shape = filter_shape , key = k ) keys = random . split ( k2 , n_blocks ) self . blocks = eqx . filter_vmap ( make_resblock )( keys ) if image == False : self . out_projection = WeightNormDense ( in_size = working_size , out_size = out_size , key = k3 ) else : self . out_projection = ConvAndGroupNorm ( input_shape = working_shape , out_size = out_size , filter_shape = filter_shape , groups = 1 , key = k3 ) def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' k1 , k2 , k3 = random . split ( key , 3 ) # Input projection in_proj = self . in_projection . data_dependent_init ( x , key = k1 ) x = eqx . filter_vmap ( in_proj )( x ) # Scan over the vmapped blocks params , state = eqx . partition ( self . blocks , eqx . is_array ) def scan_body ( x , inputs ): key , block_params = inputs block = eqx . combine ( block_params , state ) new_block = block . data_dependent_init ( x , y , key = key ) new_x = eqx . filter_vmap ( new_block )( x , y ) new_params , _ = eqx . partition ( block , eqx . is_array ) return new_x , new_params keys = random . split ( k2 , self . n_blocks ) x , params = jax . lax . scan ( scan_body , x , ( keys , params )) blocks = eqx . combine ( params , state ) out_proj = self . out_projection . data_dependent_init ( x , key = k3 ) # Turn the new parameters into a new module get_in_proj = lambda tree : tree . in_projection get_blocks = lambda tree : tree . blocks get_out_proj = lambda tree : tree . out_projection updated_layer = eqx . tree_at ( get_in_proj , self , in_proj ) updated_layer = eqx . tree_at ( get_blocks , updated_layer , blocks ) updated_layer = eqx . tree_at ( get_out_proj , updated_layer , out_proj ) return updated_layer def __call__ ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"**Arguments:** - `t`: A JAX array with shape `()`. - `x`: A JAX array with shape `(input_shape,)`. - `y`: A JAX array with shape `(cond_shape,)`. **Returns:** A JAX array with shape `(input_shape,)`. \"\"\" assert x . shape == self . input_shape # Input projection x = self . in_projection ( x ) # Resnet blocks dynamic , static = eqx . partition ( self . blocks , eqx . is_array ) def f ( x , params ): block = eqx . combine ( params , static ) return block ( x , y ), None out , _ = jax . lax . scan ( f , x , dynamic ) # Output projection out = self . out_projection ( out ) return out","title":"ResNet"},{"location":"api/nn/resnet/#generax.nn.resnet.TimeDependentResNet","text":"A time dependent version of a 1d resnet Source code in generax/nn/resnet.py class TimeDependentResNet ( ResNet ): \"\"\"A time dependent version of a 1d resnet \"\"\" time_features : TimeFeatures def __init__ ( self , input_shape : Tuple [ int ], working_size : int , hidden_size : int , out_size : int , n_blocks : int , filter_shape : Optional [ Tuple [ int ]] = ( 3 , 3 ), cond_shape : Optional [ Tuple [ int ]] = None , activation : Callable = jax . nn . swish , embedding_size : Optional [ int ] = 16 , out_features : int = 8 , * , key : PRNGKeyArray , ** kwargs ): k1 , k2 = random . split ( key , 2 ) self . time_features = TimeFeatures ( embedding_size = embedding_size , out_features = out_features , key = k1 , ** kwargs ) total_cond_size = out_features if cond_shape is not None : if len ( cond_shape ) != 1 : raise ValueError ( f 'Expected 1d conditional input.' ) total_cond_size += cond_shape [ 0 ] super () . __init__ ( input_shape = input_shape , working_size = working_size , hidden_size = hidden_size , out_size = out_size , n_blocks = n_blocks , filter_shape = filter_shape , cond_shape = ( total_cond_size ,), activation = activation , key = k2 , ** kwargs ) def data_dependent_init ( self , t : Array , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `t`: The time to initialize the parameters with. - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert t . ndim == 1 h = eqx . filter_vmap ( self . time_features )( t ) if y is not None : h = jnp . concatenate ([ h , y ], axis =- 1 ) return super () . data_dependent_init ( x , y = h , key = key ) def __call__ ( self , t : Array , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : assert t . shape == () h = self . time_features ( t ) if y is not None : h = jnp . concatenate ([ h , y ], axis =- 1 ) return super () . __call__ ( x , y = h )","title":"TimeDependentResNet"},{"location":"api/nn/resnet_blocks/","text":"Resnet Blocks \u00a4 generax.nn.resnet_blocks.GatedResBlock \u00a4 Gated residual block for 1d data or images. Source code in generax/nn/resnet_blocks.py class GatedResBlock ( eqx . Module ): \"\"\"Gated residual block for 1d data or images.\"\"\" linear_cond : Union [ Union [ WeightNormDense , ConvAndGroupNorm ], None ] linear1 : Union [ WeightNormDense , ConvAndGroupNorm ] linear2 : Union [ WeightNormDense , ConvAndGroupNorm ] activation : Callable input_shape : Tuple [ int ] = eqx . field ( static = True ) hidden_size : int = eqx . field ( static = True ) cond_shape : Tuple [ int ] = eqx . field ( static = True ) filter_shape : Union [ Tuple [ int ], None ] = eqx . field ( static = True ) groups : Union [ int , None ] = eqx . field ( static = True ) def __init__ ( self , input_shape : Tuple [ int ], hidden_size : int , groups : Optional [ int ] = None , filter_shape : Optional [ Tuple [ int ]] = None , cond_shape : Optional [ Tuple [ int ]] = None , activation : Callable = jax . nn . swish , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input size. Output size is the same as `input_shape`. - `hidden_size`: The hidden layer size. - `cond_shape`: The size of the conditioning information. - `activation`: The activation function after each hidden layer. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( ** kwargs ) if len ( input_shape ) not in [ 1 , 3 ]: raise ValueError ( f 'Expected 1d or 3d input shape' ) image = False if len ( input_shape ) == 3 : H , W , C = input_shape image = True assert filter_shape is not None , 'Must pass in filter shape when processing images' self . input_shape = input_shape self . hidden_size = hidden_size self . cond_shape = cond_shape self . filter_shape = filter_shape self . activation = activation if groups is not None : assert image if hidden_size % groups != 0 : raise ValueError ( f 'Hidden size must be divisible by groups' ) self . groups = groups k1 , k2 , k3 = random . split ( key , 3 ) # Initialize the conditioning parameters if cond_shape is not None : if len ( cond_shape ) == 1 : self . linear_cond = WeightNormDense ( in_size = cond_shape [ 0 ], out_size = 2 * hidden_size , key = k1 ) else : self . linear_cond = ConvAndGroupNorm ( input_shape = cond_shape , out_size = 2 * hidden_size , filter_shape = filter_shape , groups = groups , key = k1 ) else : self . linear_cond = None if image : self . linear1 = ConvAndGroupNorm ( input_shape = input_shape , out_size = hidden_size , filter_shape = filter_shape , groups = groups , key = k2 ) hidden_shape = ( H , W , hidden_size ) self . linear2 = WeightNormConv ( input_shape = hidden_shape , out_size = 2 * C , filter_shape = filter_shape , key = k3 ) else : self . linear1 = WeightNormDense ( in_size = input_shape [ 0 ], out_size = hidden_size , key = k2 ) self . linear2 = WeightNormDense ( in_size = hidden_size , out_size = 2 * input_shape [ 0 ], key = k3 ) def data_dependent_init ( self , x : Array , y : Array = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' k1 , k2 , k3 = random . split ( key , 3 ) # Initialize the conditioning parameters if y is not None : linear_cond = self . linear_cond . data_dependent_init ( y , key = k1 ) h = eqx . filter_vmap ( linear_cond )( y ) shift , scale = jnp . split ( h , 2 , axis =- 1 ) else : linear_cond = None # Linear + shift/scale + activation linear1 = self . linear1 . data_dependent_init ( x , key = k2 ) x = eqx . filter_vmap ( linear1 )( x ) if y is not None : x = shift + x * ( 1 + scale ) x = eqx . filter_vmap ( self . activation )( x ) # Linear + gate linear2 = self . linear2 . data_dependent_init ( x , key = k3 ) # Turn the new parameters into a new module get_linear_cond = lambda tree : tree . linear_cond get_linear1 = lambda tree : tree . linear1 get_linear2 = lambda tree : tree . linear2 updated_layer = eqx . tree_at ( get_linear_cond , self , linear_cond ) updated_layer = eqx . tree_at ( get_linear1 , updated_layer , linear1 ) updated_layer = eqx . tree_at ( get_linear2 , updated_layer , linear2 ) return updated_layer def __call__ ( self , x : Array , y : Array = None ) -> Array : \"\"\"**Arguments:** - `x`: A JAX array with shape `input_shape`. - `y`: A JAX array to condition on with shape `cond_shape`. **Returns:** A JAX array with shape `input_shape`. \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' x_in = x # The conditioning input will shift/scale x if y is not None : h = self . linear_cond ( self . activation ( y )) shift , scale = jnp . split ( h , 2 , axis =- 1 ) # Linear + shift/scale + activation x = self . linear1 ( x ) if y is not None : x = shift + x * ( 1 + scale ) x = self . activation ( x ) # Linear + gate x = self . linear2 ( x ) a , b = jnp . split ( x , 2 , axis =- 1 ) return x_in + a * jax . nn . sigmoid ( b ) __init__ ( self , input_shape : Tuple [ int ], hidden_size : int , groups : Optional [ int ] = None , filter_shape : Optional [ Tuple [ int ]] = None , cond_shape : Optional [ Tuple [ int ]] = None , activation : Callable = < PjitFunction of < function silu at 0x7fd99f02dfc0 >> , * , key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : input_shape : The input size. Output size is the same as input_shape . hidden_size : The hidden layer size. cond_shape : The size of the conditioning information. activation : The activation function after each hidden layer. key : A jax.random.PRNGKey for initialization Source code in generax/nn/resnet_blocks.py def __init__ ( self , input_shape : Tuple [ int ], hidden_size : int , groups : Optional [ int ] = None , filter_shape : Optional [ Tuple [ int ]] = None , cond_shape : Optional [ Tuple [ int ]] = None , activation : Callable = jax . nn . swish , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input size. Output size is the same as `input_shape`. - `hidden_size`: The hidden layer size. - `cond_shape`: The size of the conditioning information. - `activation`: The activation function after each hidden layer. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( ** kwargs ) if len ( input_shape ) not in [ 1 , 3 ]: raise ValueError ( f 'Expected 1d or 3d input shape' ) image = False if len ( input_shape ) == 3 : H , W , C = input_shape image = True assert filter_shape is not None , 'Must pass in filter shape when processing images' self . input_shape = input_shape self . hidden_size = hidden_size self . cond_shape = cond_shape self . filter_shape = filter_shape self . activation = activation if groups is not None : assert image if hidden_size % groups != 0 : raise ValueError ( f 'Hidden size must be divisible by groups' ) self . groups = groups k1 , k2 , k3 = random . split ( key , 3 ) # Initialize the conditioning parameters if cond_shape is not None : if len ( cond_shape ) == 1 : self . linear_cond = WeightNormDense ( in_size = cond_shape [ 0 ], out_size = 2 * hidden_size , key = k1 ) else : self . linear_cond = ConvAndGroupNorm ( input_shape = cond_shape , out_size = 2 * hidden_size , filter_shape = filter_shape , groups = groups , key = k1 ) else : self . linear_cond = None if image : self . linear1 = ConvAndGroupNorm ( input_shape = input_shape , out_size = hidden_size , filter_shape = filter_shape , groups = groups , key = k2 ) hidden_shape = ( H , W , hidden_size ) self . linear2 = WeightNormConv ( input_shape = hidden_shape , out_size = 2 * C , filter_shape = filter_shape , key = k3 ) else : self . linear1 = WeightNormDense ( in_size = input_shape [ 0 ], out_size = hidden_size , key = k2 ) self . linear2 = WeightNormDense ( in_size = hidden_size , out_size = 2 * input_shape [ 0 ], key = k3 ) data_dependent_init ( self , x : Array , y : Array = None , key : PRNGKeyArray = None ) -> Module \u00a4 Initialize the parameters of the layer based on the data. Arguments : x : The data to initialize the parameters with. y : The conditioning information key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/nn/resnet_blocks.py def data_dependent_init ( self , x : Array , y : Array = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' k1 , k2 , k3 = random . split ( key , 3 ) # Initialize the conditioning parameters if y is not None : linear_cond = self . linear_cond . data_dependent_init ( y , key = k1 ) h = eqx . filter_vmap ( linear_cond )( y ) shift , scale = jnp . split ( h , 2 , axis =- 1 ) else : linear_cond = None # Linear + shift/scale + activation linear1 = self . linear1 . data_dependent_init ( x , key = k2 ) x = eqx . filter_vmap ( linear1 )( x ) if y is not None : x = shift + x * ( 1 + scale ) x = eqx . filter_vmap ( self . activation )( x ) # Linear + gate linear2 = self . linear2 . data_dependent_init ( x , key = k3 ) # Turn the new parameters into a new module get_linear_cond = lambda tree : tree . linear_cond get_linear1 = lambda tree : tree . linear1 get_linear2 = lambda tree : tree . linear2 updated_layer = eqx . tree_at ( get_linear_cond , self , linear_cond ) updated_layer = eqx . tree_at ( get_linear1 , updated_layer , linear1 ) updated_layer = eqx . tree_at ( get_linear2 , updated_layer , linear2 ) return updated_layer __call__ ( self , x : Array , y : Array = None ) -> Array \u00a4 Arguments: x : A JAX array with shape input_shape . y : A JAX array to condition on with shape cond_shape . Returns: A JAX array with shape input_shape . Source code in generax/nn/resnet_blocks.py def __call__ ( self , x : Array , y : Array = None ) -> Array : \"\"\"**Arguments:** - `x`: A JAX array with shape `input_shape`. - `y`: A JAX array to condition on with shape `cond_shape`. **Returns:** A JAX array with shape `input_shape`. \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' x_in = x # The conditioning input will shift/scale x if y is not None : h = self . linear_cond ( self . activation ( y )) shift , scale = jnp . split ( h , 2 , axis =- 1 ) # Linear + shift/scale + activation x = self . linear1 ( x ) if y is not None : x = shift + x * ( 1 + scale ) x = self . activation ( x ) # Linear + gate x = self . linear2 ( x ) a , b = jnp . split ( x , 2 , axis =- 1 ) return x_in + a * jax . nn . sigmoid ( b ) generax.nn.resnet_blocks.Block \u00a4 Group norm, (shift+scale), activation, conv Source code in generax/nn/resnet_blocks.py class Block ( eqx . Module ): \"\"\"Group norm, (shift+scale), activation, conv \"\"\" input_shape : int = eqx . field ( static = True ) conv : WeightNormConv norm : eqx . nn . GroupNorm def __init__ ( self , input_shape : Tuple [ int ], out_size : int , groups : int , * , key : PRNGKeyArray , ** kwargs ): super () . __init__ ( ** kwargs ) H , W , C = input_shape if C % groups != 0 : raise ValueError ( \"The number of groups must divide the number of channels.\" ) self . norm = ChannelConvention ( eqx . nn . GroupNorm ( groups = groups , channels = C )) self . conv = WeightNormConv ( input_shape = input_shape , filter_shape = ( 3 , 3 ), out_size = out_size , key = key ) self . input_shape = self . conv . input_shape def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> eqx . Module : return self def __call__ ( self , x : Array , y : Array = None , shift_scale : Optional [ Array ] = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' H , W , C = self . input_shape h = self . norm ( x ) if shift_scale is not None : shift , scale = shift_scale h = shift + h * ( 1 + scale ) h = jax . nn . silu ( h ) h = self . conv ( h ) return h __init__ ( self , input_shape : Tuple [ int ], out_size : int , groups : int , * , key : PRNGKeyArray , ** kwargs ) \u00a4 Source code in generax/nn/resnet_blocks.py def __init__ ( self , input_shape : Tuple [ int ], out_size : int , groups : int , * , key : PRNGKeyArray , ** kwargs ): super () . __init__ ( ** kwargs ) H , W , C = input_shape if C % groups != 0 : raise ValueError ( \"The number of groups must divide the number of channels.\" ) self . norm = ChannelConvention ( eqx . nn . GroupNorm ( groups = groups , channels = C )) self . conv = WeightNormConv ( input_shape = input_shape , filter_shape = ( 3 , 3 ), out_size = out_size , key = key ) self . input_shape = self . conv . input_shape data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> Module \u00a4 Source code in generax/nn/resnet_blocks.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> eqx . Module : return self __call__ ( self , x : Array , y : Array = None , shift_scale : Optional [ Array ] = None ) -> Array \u00a4 Call self as a function. Source code in generax/nn/resnet_blocks.py def __call__ ( self , x : Array , y : Array = None , shift_scale : Optional [ Array ] = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' H , W , C = self . input_shape h = self . norm ( x ) if shift_scale is not None : shift , scale = shift_scale h = shift + h * ( 1 + scale ) h = jax . nn . silu ( h ) h = self . conv ( h ) return h generax.nn.resnet_blocks.ImageResBlock \u00a4 Gated residual block for images. Source code in generax/nn/resnet_blocks.py class ImageResBlock ( eqx . Module ): \"\"\"Gated residual block for images.\"\"\" linear_cond : Union [ ConvAndGroupNorm , None ] block1 : Block block2 : Block res_conv : Union [ ConvAndGroupNorm , eqx . nn . Identity ] gca : GatedGlobalContext input_shape : Tuple [ int ] = eqx . field ( static = True ) hidden_size : int = eqx . field ( static = True ) out_size : int = eqx . field ( static = True ) cond_shape : Tuple [ int ] = eqx . field ( static = True ) groups : Union [ int , None ] = eqx . field ( static = True ) def __init__ ( self , input_shape : Tuple [ int ], hidden_size : int , out_size : int , groups : Optional [ int ] = None , cond_shape : Optional [ Tuple [ int ]] = None , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input size. Output size is the same as `input_shape`. - `hidden_size`: The hidden layer size. - `cond_shape`: The size of the conditioning information. - `activation`: The activation function after each hidden layer. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( ** kwargs ) H , W , C = input_shape self . input_shape = input_shape self . hidden_size = hidden_size self . cond_shape = cond_shape self . out_size = out_size if hidden_size % groups != 0 : raise ValueError ( f 'Hidden size must be divisible by groups' ) self . groups = groups k1 , k2 , k3 , k4 , k5 = random . split ( key , 5 ) # Initialize the conditioning parameters if cond_shape is not None : if len ( cond_shape ) != 1 : raise ValueError ( f 'Conditioning shape must be 1d' ) self . linear_cond = WeightNormDense ( in_size = cond_shape [ 0 ], out_size = 2 * hidden_size , key = k1 ) else : self . linear_cond = None self . block1 = Block ( input_shape = input_shape , out_size = hidden_size , groups = groups , key = k2 ) self . block2 = Block ( input_shape = ( H , W , hidden_size ), out_size = out_size , groups = groups , key = k3 ) self . gca = GatedGlobalContext ( input_shape = ( H , W , out_size ), key = k4 ) if out_size != C : self . res_conv = WeightNormConv ( input_shape = input_shape , out_size = out_size , filter_shape = ( 3 , 3 ), key = k5 ) else : self . res_conv = eqx . nn . Identity () def data_dependent_init ( self , x : Array , y : Array = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" return self def __call__ ( self , x : Array , y : Array = None ) -> Array : \"\"\"**Arguments:** - `x`: A JAX array with shape `input_shape`. - `y`: A JAX array to condition on with shape `cond_shape`. **Returns:** A JAX array with shape `input_shape`. \"\"\" x_in = x h = self . block1 ( x ) # The conditioning input will shift/scale x if y is not None : hh = self . linear_cond ( jax . nn . silu ( y )) shift_scale = jnp . split ( hh , 2 , axis =- 1 ) else : shift_scale = None h = self . block2 ( h , shift_scale = shift_scale ) h = self . gca ( h ) return self . res_conv ( x_in ) + h __init__ ( self , input_shape : Tuple [ int ], hidden_size : int , out_size : int , groups : Optional [ int ] = None , cond_shape : Optional [ Tuple [ int ]] = None , * , key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : input_shape : The input size. Output size is the same as input_shape . hidden_size : The hidden layer size. cond_shape : The size of the conditioning information. activation : The activation function after each hidden layer. key : A jax.random.PRNGKey for initialization Source code in generax/nn/resnet_blocks.py def __init__ ( self , input_shape : Tuple [ int ], hidden_size : int , out_size : int , groups : Optional [ int ] = None , cond_shape : Optional [ Tuple [ int ]] = None , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input size. Output size is the same as `input_shape`. - `hidden_size`: The hidden layer size. - `cond_shape`: The size of the conditioning information. - `activation`: The activation function after each hidden layer. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( ** kwargs ) H , W , C = input_shape self . input_shape = input_shape self . hidden_size = hidden_size self . cond_shape = cond_shape self . out_size = out_size if hidden_size % groups != 0 : raise ValueError ( f 'Hidden size must be divisible by groups' ) self . groups = groups k1 , k2 , k3 , k4 , k5 = random . split ( key , 5 ) # Initialize the conditioning parameters if cond_shape is not None : if len ( cond_shape ) != 1 : raise ValueError ( f 'Conditioning shape must be 1d' ) self . linear_cond = WeightNormDense ( in_size = cond_shape [ 0 ], out_size = 2 * hidden_size , key = k1 ) else : self . linear_cond = None self . block1 = Block ( input_shape = input_shape , out_size = hidden_size , groups = groups , key = k2 ) self . block2 = Block ( input_shape = ( H , W , hidden_size ), out_size = out_size , groups = groups , key = k3 ) self . gca = GatedGlobalContext ( input_shape = ( H , W , out_size ), key = k4 ) if out_size != C : self . res_conv = WeightNormConv ( input_shape = input_shape , out_size = out_size , filter_shape = ( 3 , 3 ), key = k5 ) else : self . res_conv = eqx . nn . Identity () data_dependent_init ( self , x : Array , y : Array = None , key : PRNGKeyArray = None ) -> Module \u00a4 Initialize the parameters of the layer based on the data. Arguments : x : The data to initialize the parameters with. y : The conditioning information key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/nn/resnet_blocks.py def data_dependent_init ( self , x : Array , y : Array = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" return self __call__ ( self , x : Array , y : Array = None ) -> Array \u00a4 Arguments: x : A JAX array with shape input_shape . y : A JAX array to condition on with shape cond_shape . Returns: A JAX array with shape input_shape . Source code in generax/nn/resnet_blocks.py def __call__ ( self , x : Array , y : Array = None ) -> Array : \"\"\"**Arguments:** - `x`: A JAX array with shape `input_shape`. - `y`: A JAX array to condition on with shape `cond_shape`. **Returns:** A JAX array with shape `input_shape`. \"\"\" x_in = x h = self . block1 ( x ) # The conditioning input will shift/scale x if y is not None : hh = self . linear_cond ( jax . nn . silu ( y )) shift_scale = jnp . split ( hh , 2 , axis =- 1 ) else : shift_scale = None h = self . block2 ( h , shift_scale = shift_scale ) h = self . gca ( h ) return self . res_conv ( x_in ) + h","title":"Resnet Blocks"},{"location":"api/nn/resnet_blocks/#resnet-blocks","text":"","title":"Resnet Blocks"},{"location":"api/nn/resnet_blocks/#generax.nn.resnet_blocks.GatedResBlock","text":"Gated residual block for 1d data or images. Source code in generax/nn/resnet_blocks.py class GatedResBlock ( eqx . Module ): \"\"\"Gated residual block for 1d data or images.\"\"\" linear_cond : Union [ Union [ WeightNormDense , ConvAndGroupNorm ], None ] linear1 : Union [ WeightNormDense , ConvAndGroupNorm ] linear2 : Union [ WeightNormDense , ConvAndGroupNorm ] activation : Callable input_shape : Tuple [ int ] = eqx . field ( static = True ) hidden_size : int = eqx . field ( static = True ) cond_shape : Tuple [ int ] = eqx . field ( static = True ) filter_shape : Union [ Tuple [ int ], None ] = eqx . field ( static = True ) groups : Union [ int , None ] = eqx . field ( static = True ) def __init__ ( self , input_shape : Tuple [ int ], hidden_size : int , groups : Optional [ int ] = None , filter_shape : Optional [ Tuple [ int ]] = None , cond_shape : Optional [ Tuple [ int ]] = None , activation : Callable = jax . nn . swish , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input size. Output size is the same as `input_shape`. - `hidden_size`: The hidden layer size. - `cond_shape`: The size of the conditioning information. - `activation`: The activation function after each hidden layer. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( ** kwargs ) if len ( input_shape ) not in [ 1 , 3 ]: raise ValueError ( f 'Expected 1d or 3d input shape' ) image = False if len ( input_shape ) == 3 : H , W , C = input_shape image = True assert filter_shape is not None , 'Must pass in filter shape when processing images' self . input_shape = input_shape self . hidden_size = hidden_size self . cond_shape = cond_shape self . filter_shape = filter_shape self . activation = activation if groups is not None : assert image if hidden_size % groups != 0 : raise ValueError ( f 'Hidden size must be divisible by groups' ) self . groups = groups k1 , k2 , k3 = random . split ( key , 3 ) # Initialize the conditioning parameters if cond_shape is not None : if len ( cond_shape ) == 1 : self . linear_cond = WeightNormDense ( in_size = cond_shape [ 0 ], out_size = 2 * hidden_size , key = k1 ) else : self . linear_cond = ConvAndGroupNorm ( input_shape = cond_shape , out_size = 2 * hidden_size , filter_shape = filter_shape , groups = groups , key = k1 ) else : self . linear_cond = None if image : self . linear1 = ConvAndGroupNorm ( input_shape = input_shape , out_size = hidden_size , filter_shape = filter_shape , groups = groups , key = k2 ) hidden_shape = ( H , W , hidden_size ) self . linear2 = WeightNormConv ( input_shape = hidden_shape , out_size = 2 * C , filter_shape = filter_shape , key = k3 ) else : self . linear1 = WeightNormDense ( in_size = input_shape [ 0 ], out_size = hidden_size , key = k2 ) self . linear2 = WeightNormDense ( in_size = hidden_size , out_size = 2 * input_shape [ 0 ], key = k3 ) def data_dependent_init ( self , x : Array , y : Array = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' k1 , k2 , k3 = random . split ( key , 3 ) # Initialize the conditioning parameters if y is not None : linear_cond = self . linear_cond . data_dependent_init ( y , key = k1 ) h = eqx . filter_vmap ( linear_cond )( y ) shift , scale = jnp . split ( h , 2 , axis =- 1 ) else : linear_cond = None # Linear + shift/scale + activation linear1 = self . linear1 . data_dependent_init ( x , key = k2 ) x = eqx . filter_vmap ( linear1 )( x ) if y is not None : x = shift + x * ( 1 + scale ) x = eqx . filter_vmap ( self . activation )( x ) # Linear + gate linear2 = self . linear2 . data_dependent_init ( x , key = k3 ) # Turn the new parameters into a new module get_linear_cond = lambda tree : tree . linear_cond get_linear1 = lambda tree : tree . linear1 get_linear2 = lambda tree : tree . linear2 updated_layer = eqx . tree_at ( get_linear_cond , self , linear_cond ) updated_layer = eqx . tree_at ( get_linear1 , updated_layer , linear1 ) updated_layer = eqx . tree_at ( get_linear2 , updated_layer , linear2 ) return updated_layer def __call__ ( self , x : Array , y : Array = None ) -> Array : \"\"\"**Arguments:** - `x`: A JAX array with shape `input_shape`. - `y`: A JAX array to condition on with shape `cond_shape`. **Returns:** A JAX array with shape `input_shape`. \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' x_in = x # The conditioning input will shift/scale x if y is not None : h = self . linear_cond ( self . activation ( y )) shift , scale = jnp . split ( h , 2 , axis =- 1 ) # Linear + shift/scale + activation x = self . linear1 ( x ) if y is not None : x = shift + x * ( 1 + scale ) x = self . activation ( x ) # Linear + gate x = self . linear2 ( x ) a , b = jnp . split ( x , 2 , axis =- 1 ) return x_in + a * jax . nn . sigmoid ( b )","title":"GatedResBlock"},{"location":"api/nn/resnet_blocks/#generax.nn.resnet_blocks.Block","text":"Group norm, (shift+scale), activation, conv Source code in generax/nn/resnet_blocks.py class Block ( eqx . Module ): \"\"\"Group norm, (shift+scale), activation, conv \"\"\" input_shape : int = eqx . field ( static = True ) conv : WeightNormConv norm : eqx . nn . GroupNorm def __init__ ( self , input_shape : Tuple [ int ], out_size : int , groups : int , * , key : PRNGKeyArray , ** kwargs ): super () . __init__ ( ** kwargs ) H , W , C = input_shape if C % groups != 0 : raise ValueError ( \"The number of groups must divide the number of channels.\" ) self . norm = ChannelConvention ( eqx . nn . GroupNorm ( groups = groups , channels = C )) self . conv = WeightNormConv ( input_shape = input_shape , filter_shape = ( 3 , 3 ), out_size = out_size , key = key ) self . input_shape = self . conv . input_shape def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> eqx . Module : return self def __call__ ( self , x : Array , y : Array = None , shift_scale : Optional [ Array ] = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' H , W , C = self . input_shape h = self . norm ( x ) if shift_scale is not None : shift , scale = shift_scale h = shift + h * ( 1 + scale ) h = jax . nn . silu ( h ) h = self . conv ( h ) return h","title":"Block"},{"location":"api/nn/resnet_blocks/#generax.nn.resnet_blocks.ImageResBlock","text":"Gated residual block for images. Source code in generax/nn/resnet_blocks.py class ImageResBlock ( eqx . Module ): \"\"\"Gated residual block for images.\"\"\" linear_cond : Union [ ConvAndGroupNorm , None ] block1 : Block block2 : Block res_conv : Union [ ConvAndGroupNorm , eqx . nn . Identity ] gca : GatedGlobalContext input_shape : Tuple [ int ] = eqx . field ( static = True ) hidden_size : int = eqx . field ( static = True ) out_size : int = eqx . field ( static = True ) cond_shape : Tuple [ int ] = eqx . field ( static = True ) groups : Union [ int , None ] = eqx . field ( static = True ) def __init__ ( self , input_shape : Tuple [ int ], hidden_size : int , out_size : int , groups : Optional [ int ] = None , cond_shape : Optional [ Tuple [ int ]] = None , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input size. Output size is the same as `input_shape`. - `hidden_size`: The hidden layer size. - `cond_shape`: The size of the conditioning information. - `activation`: The activation function after each hidden layer. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( ** kwargs ) H , W , C = input_shape self . input_shape = input_shape self . hidden_size = hidden_size self . cond_shape = cond_shape self . out_size = out_size if hidden_size % groups != 0 : raise ValueError ( f 'Hidden size must be divisible by groups' ) self . groups = groups k1 , k2 , k3 , k4 , k5 = random . split ( key , 5 ) # Initialize the conditioning parameters if cond_shape is not None : if len ( cond_shape ) != 1 : raise ValueError ( f 'Conditioning shape must be 1d' ) self . linear_cond = WeightNormDense ( in_size = cond_shape [ 0 ], out_size = 2 * hidden_size , key = k1 ) else : self . linear_cond = None self . block1 = Block ( input_shape = input_shape , out_size = hidden_size , groups = groups , key = k2 ) self . block2 = Block ( input_shape = ( H , W , hidden_size ), out_size = out_size , groups = groups , key = k3 ) self . gca = GatedGlobalContext ( input_shape = ( H , W , out_size ), key = k4 ) if out_size != C : self . res_conv = WeightNormConv ( input_shape = input_shape , out_size = out_size , filter_shape = ( 3 , 3 ), key = k5 ) else : self . res_conv = eqx . nn . Identity () def data_dependent_init ( self , x : Array , y : Array = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" return self def __call__ ( self , x : Array , y : Array = None ) -> Array : \"\"\"**Arguments:** - `x`: A JAX array with shape `input_shape`. - `y`: A JAX array to condition on with shape `cond_shape`. **Returns:** A JAX array with shape `input_shape`. \"\"\" x_in = x h = self . block1 ( x ) # The conditioning input will shift/scale x if y is not None : hh = self . linear_cond ( jax . nn . silu ( y )) shift_scale = jnp . split ( hh , 2 , axis =- 1 ) else : shift_scale = None h = self . block2 ( h , shift_scale = shift_scale ) h = self . gca ( h ) return self . res_conv ( x_in ) + h","title":"ImageResBlock"},{"location":"api/nn/time_condition/","text":"Time conditioner \u00a4 generax.nn.time_condition.GaussianFourierProjection \u00a4 GaussianFourierProjection( args, *kwargs) Source code in generax/nn/time_condition.py class GaussianFourierProjection ( eqx . Module ): embedding_size : int = eqx . field ( static = True ) W : eqx . nn . Linear def __init__ ( self , embedding_size : Optional [ int ] = 16 , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `embedding_size`: The size of the embedding. \"\"\" super () . __init__ ( ** kwargs ) self . embedding_size = embedding_size self . W = eqx . nn . Linear ( in_features = 1 , out_features = embedding_size , use_bias = False , key = key ) def __call__ ( self , t : Array ) -> Array : \"\"\"**Arguments:** - `t`: A JAX array with shape `()`. **Returns:** A JAX array with shape `(2*embedding_size,)`. \"\"\" assert t . shape == () t = jnp . expand_dims ( t , axis =- 1 ) t_proj = self . W ( t * 2 * jnp . pi ) return jnp . concatenate ([ jnp . sin ( t_proj ), jnp . cos ( t_proj )], axis =- 1 ) __init__ ( self , embedding_size : Optional [ int ] = 16 , * , key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : embedding_size : The size of the embedding. Source code in generax/nn/time_condition.py def __init__ ( self , embedding_size : Optional [ int ] = 16 , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `embedding_size`: The size of the embedding. \"\"\" super () . __init__ ( ** kwargs ) self . embedding_size = embedding_size self . W = eqx . nn . Linear ( in_features = 1 , out_features = embedding_size , use_bias = False , key = key ) __call__ ( self , t : Array ) -> Array \u00a4 Arguments: t : A JAX array with shape () . Returns: A JAX array with shape (2*embedding_size,) . Source code in generax/nn/time_condition.py def __call__ ( self , t : Array ) -> Array : \"\"\"**Arguments:** - `t`: A JAX array with shape `()`. **Returns:** A JAX array with shape `(2*embedding_size,)`. \"\"\" assert t . shape == () t = jnp . expand_dims ( t , axis =- 1 ) t_proj = self . W ( t * 2 * jnp . pi ) return jnp . concatenate ([ jnp . sin ( t_proj ), jnp . cos ( t_proj )], axis =- 1 ) generax.nn.time_condition.TimeFeatures \u00a4 TimeFeatures( args, *kwargs) Source code in generax/nn/time_condition.py class TimeFeatures ( eqx . Module ): out_features : int = eqx . field ( static = True ) projection : GaussianFourierProjection W1 : Array W2 : Array activation : Callable def __init__ ( self , embedding_size : Optional [ int ] = 16 , out_features : int = 8 , activation : Callable = jax . nn . gelu , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `embedding_size`: The size of the embedding. - `out_features`: The number of output features. - `activation`: The activation function. \"\"\" super () . __init__ ( ** kwargs ) self . out_features = out_features k1 , k2 , k3 = random . split ( key , 3 ) self . projection = GaussianFourierProjection ( embedding_size = embedding_size , key = k1 ) self . W1 = eqx . nn . Linear ( in_features = 2 * embedding_size , out_features = 4 * embedding_size , key = k2 ) self . activation = activation self . W2 = eqx . nn . Linear ( in_features = 4 * embedding_size , out_features = self . out_features , key = k3 ) def __call__ ( self , t : Array ) -> Array : \"\"\"**Arguments:** - `t`: A JAX array with shape `()`. **Returns:** A JAX array with shape `(out_features,)`. \"\"\" assert t . shape == () x = self . projection ( t ) x = self . W1 ( x ) x = self . activation ( x ) return self . W2 ( x ) __init__ ( self , embedding_size : Optional [ int ] = 16 , out_features : int = 8 , activation : Callable = < function gelu > , * , key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : embedding_size : The size of the embedding. out_features : The number of output features. activation : The activation function. Source code in generax/nn/time_condition.py def __init__ ( self , embedding_size : Optional [ int ] = 16 , out_features : int = 8 , activation : Callable = jax . nn . gelu , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `embedding_size`: The size of the embedding. - `out_features`: The number of output features. - `activation`: The activation function. \"\"\" super () . __init__ ( ** kwargs ) self . out_features = out_features k1 , k2 , k3 = random . split ( key , 3 ) self . projection = GaussianFourierProjection ( embedding_size = embedding_size , key = k1 ) self . W1 = eqx . nn . Linear ( in_features = 2 * embedding_size , out_features = 4 * embedding_size , key = k2 ) self . activation = activation self . W2 = eqx . nn . Linear ( in_features = 4 * embedding_size , out_features = self . out_features , key = k3 ) __call__ ( self , t : Array ) -> Array \u00a4 Arguments: t : A JAX array with shape () . Returns: A JAX array with shape (out_features,) . Source code in generax/nn/time_condition.py def __call__ ( self , t : Array ) -> Array : \"\"\"**Arguments:** - `t`: A JAX array with shape `()`. **Returns:** A JAX array with shape `(out_features,)`. \"\"\" assert t . shape == () x = self . projection ( t ) x = self . W1 ( x ) x = self . activation ( x ) return self . W2 ( x )","title":"Time conditioner"},{"location":"api/nn/time_condition/#time-conditioner","text":"","title":"Time conditioner"},{"location":"api/nn/time_condition/#generax.nn.time_condition.GaussianFourierProjection","text":"GaussianFourierProjection( args, *kwargs) Source code in generax/nn/time_condition.py class GaussianFourierProjection ( eqx . Module ): embedding_size : int = eqx . field ( static = True ) W : eqx . nn . Linear def __init__ ( self , embedding_size : Optional [ int ] = 16 , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `embedding_size`: The size of the embedding. \"\"\" super () . __init__ ( ** kwargs ) self . embedding_size = embedding_size self . W = eqx . nn . Linear ( in_features = 1 , out_features = embedding_size , use_bias = False , key = key ) def __call__ ( self , t : Array ) -> Array : \"\"\"**Arguments:** - `t`: A JAX array with shape `()`. **Returns:** A JAX array with shape `(2*embedding_size,)`. \"\"\" assert t . shape == () t = jnp . expand_dims ( t , axis =- 1 ) t_proj = self . W ( t * 2 * jnp . pi ) return jnp . concatenate ([ jnp . sin ( t_proj ), jnp . cos ( t_proj )], axis =- 1 )","title":"GaussianFourierProjection"},{"location":"api/nn/time_condition/#generax.nn.time_condition.TimeFeatures","text":"TimeFeatures( args, *kwargs) Source code in generax/nn/time_condition.py class TimeFeatures ( eqx . Module ): out_features : int = eqx . field ( static = True ) projection : GaussianFourierProjection W1 : Array W2 : Array activation : Callable def __init__ ( self , embedding_size : Optional [ int ] = 16 , out_features : int = 8 , activation : Callable = jax . nn . gelu , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `embedding_size`: The size of the embedding. - `out_features`: The number of output features. - `activation`: The activation function. \"\"\" super () . __init__ ( ** kwargs ) self . out_features = out_features k1 , k2 , k3 = random . split ( key , 3 ) self . projection = GaussianFourierProjection ( embedding_size = embedding_size , key = k1 ) self . W1 = eqx . nn . Linear ( in_features = 2 * embedding_size , out_features = 4 * embedding_size , key = k2 ) self . activation = activation self . W2 = eqx . nn . Linear ( in_features = 4 * embedding_size , out_features = self . out_features , key = k3 ) def __call__ ( self , t : Array ) -> Array : \"\"\"**Arguments:** - `t`: A JAX array with shape `()`. **Returns:** A JAX array with shape `(out_features,)`. \"\"\" assert t . shape == () x = self . projection ( t ) x = self . W1 ( x ) x = self . activation ( x ) return self . W2 ( x )","title":"TimeFeatures"},{"location":"api/nn/unet/","text":"UNet \u00a4 generax.nn.unet.UNet \u00a4 Unet architecture. Source code in generax/nn/unet.py class UNet ( eqx . Module ): \"\"\"Unet architecture. \"\"\" input_shape : Tuple [ int ] = eqx . field ( static = True ) dim : int = eqx . field ( static = True ) out_channels : int = eqx . field ( static = True ) dim_mults : Tuple [ int ] = eqx . field ( static = True ) in_out : Tuple [ Tuple [ int , int ]] = eqx . field ( static = True ) conv_in : WeightNormConv time_features : TimeFeatures down_blocks : Tuple [ Union [ ImageResBlock , AttentionBlock , Downsample ]] middle_blocks : Tuple [ Union [ ImageResBlock , AttentionBlock ]] up_blocks : Tuple [ Union [ ImageResBlock , AttentionBlock , Upsample ]] final_block : ImageResBlock proj_out : WeightNormConv freeu : bool = eqx . field ( static = True ) time_dependent : bool = eqx . field ( static = True ) cond_shape : Optional [ Tuple [ int ]] = eqx . field ( static = True ) def __init__ ( self , input_shape : Tuple [ int ], dim : int = 16 , out_channels : Optional [ int ] = None , dim_mults : Tuple [ int ] = ( 1 , 2 , 4 , 8 ), resnet_block_groups : int = 8 , attn_heads : int = 4 , attn_dim_head : int = 32 , cond_shape : Optional [ Tuple [ int ]] = None , * , key : PRNGKeyArray , freeu : bool = False , time_dependent : bool = True ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `dim`: The dimension of the features - `out_channels`: The number of output channels. If None, then the same as the input. - `dim_mults`: The dimension of the features at each downsampling - `resnet_block_groups`: The number of resnet blocks per downsampling - `attn_heads`: The number of attention heads per downsampling - `attn_dim_head`: The dimension of the attention heads - `freeu`: Whether to use freeu filtering - `time_dependent`: Whether to use time conditioning \"\"\" H , W , C = input_shape if H // ( 2 ** len ( dim_mults )) == 0 : raise ValueError ( f \"Image size { ( H , W ) } is too small for { len ( dim_mults ) } downsamples.\" ) self . input_shape = input_shape self . dim = dim self . out_channels = C if out_channels is None else out_channels self . dim_mults = dim_mults self . freeu = freeu self . time_dependent = time_dependent keys = random . split ( key , 20 ) key_iter = iter ( keys ) self . conv_in = WeightNormConv ( input_shape = input_shape , out_size = self . dim , filter_shape = ( 7 , 7 ), padding = 3 , key = next ( key_iter )) if self . time_dependent : self . time_features = TimeFeatures ( embedding_size = self . dim , out_features = 4 * self . dim , key = next ( key_iter )) time_shape = ( 4 * self . dim ,) else : self . time_features = None time_shape = None self . cond_shape = cond_shape if cond_shape is not None : assert len ( cond_shape ) == 1 if time_shape : time_shape = ( time_shape [ 0 ] + cond_shape [ 0 ],) else : time_shape = cond_shape def make_resblock ( key , input_shape , dim_out ): return ImageResBlock ( input_shape = input_shape , hidden_size = dim_out , out_size = dim_out , groups = resnet_block_groups , cond_shape = time_shape , key = key ) def make_attention ( key , input_shape , linear = True ): return AttentionBlock ( input_shape = input_shape , heads = attn_heads , dim_head = attn_dim_head , key = key , use_linear_attention = linear ) # Downsampling down_blocks = [] dims = [ self . dim * mult for mult in self . dim_mults ] self . in_out = list ( zip ( dims [: - 1 ], dims [ 1 :])) keys = random . split ( next ( key_iter ), len ( self . in_out )) for i , ( key , ( dim_in , dim_out )) in enumerate ( zip ( keys , self . in_out )): k1 , k2 = random . split ( key , 2 ) down_blocks . append ( make_resblock ( k1 , ( H , W , dim_in ), dim_in )) down_blocks . append ( make_resblock ( k2 , ( H , W , dim_in ), dim_in )) down_blocks . append ( make_attention ( key , ( H , W , dim_in ))) down = Downsample ( input_shape = ( H , W , dim_in ), out_size = dim_out , key = key ) down_blocks . append ( down ) assert H % 2 == 0 assert W % 2 == 0 H , W = H // 2 , W // 2 self . down_blocks = down_blocks # Middle middle_blocks = [] middle_blocks . append ( make_resblock ( next ( key_iter ), ( H , W , dim_out ), dim_out )) middle_blocks . append ( make_attention ( next ( key_iter ), ( H , W , dim_out ), linear = False )) middle_blocks . append ( make_resblock ( next ( key_iter ), ( H , W , dim_out ), dim_out )) self . middle_blocks = middle_blocks # Upsampling keys = random . split ( next ( key_iter ), len ( self . in_out )) up_blocks = [] for i , ( key , ( dim_in , dim_out )) in enumerate ( zip ( keys , self . in_out [:: - 1 ])): k1 , k2 = random . split ( key , 2 ) up = Upsample ( input_shape = ( H , W , dim_out ), out_size = dim_in , key = key ) up_blocks . append ( up ) H , W = H * 2 , W * 2 # Skip connections contribute a dim_in up_blocks . append ( make_resblock ( k1 , ( H , W , dim_in + dim_in ), dim_in )) up_blocks . append ( make_resblock ( k2 , ( H , W , dim_in + dim_in ), dim_in )) up_blocks . append ( make_attention ( key , ( H , W , dim_in ))) self . up_blocks = up_blocks # Final self . final_block = make_resblock ( next ( key_iter ), ( H , W , dim_in + dim_in ), dim_in ) self . proj_out = WeightNormConv ( input_shape = ( H , W , dim_in ), out_size = self . out_channels , filter_shape = ( 1 , 1 ), key = next ( key_iter )) def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" return self def __call__ ( self , * args , ** kwargs ) -> Array : if self . time_dependent : if len ( args ) == 3 : t , x , y = args else : t , x = args y = None assert t . shape == () else : if len ( args ) == 2 : x , y = args else : x = args [ 0 ] y = None assert x . shape == self . input_shape # Time embedding if self . time_dependent : conditional_embedding = self . time_features ( t ) if y is not None : conditional_embedding = jnp . concatenate ([ conditional_embedding , y ], axis =- 1 ) else : conditional_embedding = y hs = [] # Initial convolution h = self . conv_in ( x ) hs . append ( h ) # Downsampling block_iter = iter ( self . down_blocks ) for i , ( dim_in , dim_out ) in enumerate ( self . in_out ): # Resnet block h = next ( block_iter )( h , conditional_embedding ) hs . append ( h ) # Resnet block + attention block h = next ( block_iter )( h , conditional_embedding ) h = next ( block_iter )( h ) hs . append ( h ) # Downsample h = next ( block_iter )( h ) # Middle res_block1 , attn_block , res_block2 = self . middle_blocks h = res_block1 ( h ) h = attn_block ( h ) h = res_block2 ( h ) # Upsampling block_iter = iter ( self . up_blocks ) for i , ( dim_in , dim_out ) in enumerate ( self . in_out [:: - 1 ]): # Upsample h = next ( block_iter )( h ) hs_ = hs . pop () if self . freeu : assert 0 , 'Not tested yet' if i == 0 : h_mean = h . mean ( axis =- 1 )[:,:, None ] h_max = h_mean . max () h_min = h_mean . max () h_mean = ( h_mean - h_min [ None , None ]) / ( h_max - h_min )[ None , None ] b1 = 1.5 s1 = 0.9 h = h . at [:,: 640 ] . mul (( b1 - 1 ) * h_mean + 1 ) hs_ = freeu_filter ( hs_ , threshold = 1.0 , scale = s1 ) # Resnet block h = jnp . concatenate ([ h , hs_ ], axis =- 1 ) h = next ( block_iter )( h , conditional_embedding ) # Resnet block h = jnp . concatenate ([ h , hs . pop ()], axis =- 1 ) h = next ( block_iter )( h , conditional_embedding ) # Attention block h = next ( block_iter )( h ) # Final h_in = hs . pop () h = jnp . concatenate ([ h , h_in ], axis =- 1 ) h = self . final_block ( h , conditional_embedding ) h = self . proj_out ( h ) return h __init__ ( self , input_shape : Tuple [ int ], dim : int = 16 , out_channels : Optional [ int ] = None , dim_mults : Tuple [ int ] = ( 1 , 2 , 4 , 8 ), resnet_block_groups : int = 8 , attn_heads : int = 4 , attn_dim_head : int = 32 , cond_shape : Optional [ Tuple [ int ]] = None , * , key : PRNGKeyArray , freeu : bool = False , time_dependent : bool = True ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. dim : The dimension of the features out_channels : The number of output channels. If None, then the same as the input. dim_mults : The dimension of the features at each downsampling resnet_block_groups : The number of resnet blocks per downsampling attn_heads : The number of attention heads per downsampling attn_dim_head : The dimension of the attention heads freeu : Whether to use freeu filtering time_dependent : Whether to use time conditioning Source code in generax/nn/unet.py def __init__ ( self , input_shape : Tuple [ int ], dim : int = 16 , out_channels : Optional [ int ] = None , dim_mults : Tuple [ int ] = ( 1 , 2 , 4 , 8 ), resnet_block_groups : int = 8 , attn_heads : int = 4 , attn_dim_head : int = 32 , cond_shape : Optional [ Tuple [ int ]] = None , * , key : PRNGKeyArray , freeu : bool = False , time_dependent : bool = True ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `dim`: The dimension of the features - `out_channels`: The number of output channels. If None, then the same as the input. - `dim_mults`: The dimension of the features at each downsampling - `resnet_block_groups`: The number of resnet blocks per downsampling - `attn_heads`: The number of attention heads per downsampling - `attn_dim_head`: The dimension of the attention heads - `freeu`: Whether to use freeu filtering - `time_dependent`: Whether to use time conditioning \"\"\" H , W , C = input_shape if H // ( 2 ** len ( dim_mults )) == 0 : raise ValueError ( f \"Image size { ( H , W ) } is too small for { len ( dim_mults ) } downsamples.\" ) self . input_shape = input_shape self . dim = dim self . out_channels = C if out_channels is None else out_channels self . dim_mults = dim_mults self . freeu = freeu self . time_dependent = time_dependent keys = random . split ( key , 20 ) key_iter = iter ( keys ) self . conv_in = WeightNormConv ( input_shape = input_shape , out_size = self . dim , filter_shape = ( 7 , 7 ), padding = 3 , key = next ( key_iter )) if self . time_dependent : self . time_features = TimeFeatures ( embedding_size = self . dim , out_features = 4 * self . dim , key = next ( key_iter )) time_shape = ( 4 * self . dim ,) else : self . time_features = None time_shape = None self . cond_shape = cond_shape if cond_shape is not None : assert len ( cond_shape ) == 1 if time_shape : time_shape = ( time_shape [ 0 ] + cond_shape [ 0 ],) else : time_shape = cond_shape def make_resblock ( key , input_shape , dim_out ): return ImageResBlock ( input_shape = input_shape , hidden_size = dim_out , out_size = dim_out , groups = resnet_block_groups , cond_shape = time_shape , key = key ) def make_attention ( key , input_shape , linear = True ): return AttentionBlock ( input_shape = input_shape , heads = attn_heads , dim_head = attn_dim_head , key = key , use_linear_attention = linear ) # Downsampling down_blocks = [] dims = [ self . dim * mult for mult in self . dim_mults ] self . in_out = list ( zip ( dims [: - 1 ], dims [ 1 :])) keys = random . split ( next ( key_iter ), len ( self . in_out )) for i , ( key , ( dim_in , dim_out )) in enumerate ( zip ( keys , self . in_out )): k1 , k2 = random . split ( key , 2 ) down_blocks . append ( make_resblock ( k1 , ( H , W , dim_in ), dim_in )) down_blocks . append ( make_resblock ( k2 , ( H , W , dim_in ), dim_in )) down_blocks . append ( make_attention ( key , ( H , W , dim_in ))) down = Downsample ( input_shape = ( H , W , dim_in ), out_size = dim_out , key = key ) down_blocks . append ( down ) assert H % 2 == 0 assert W % 2 == 0 H , W = H // 2 , W // 2 self . down_blocks = down_blocks # Middle middle_blocks = [] middle_blocks . append ( make_resblock ( next ( key_iter ), ( H , W , dim_out ), dim_out )) middle_blocks . append ( make_attention ( next ( key_iter ), ( H , W , dim_out ), linear = False )) middle_blocks . append ( make_resblock ( next ( key_iter ), ( H , W , dim_out ), dim_out )) self . middle_blocks = middle_blocks # Upsampling keys = random . split ( next ( key_iter ), len ( self . in_out )) up_blocks = [] for i , ( key , ( dim_in , dim_out )) in enumerate ( zip ( keys , self . in_out [:: - 1 ])): k1 , k2 = random . split ( key , 2 ) up = Upsample ( input_shape = ( H , W , dim_out ), out_size = dim_in , key = key ) up_blocks . append ( up ) H , W = H * 2 , W * 2 # Skip connections contribute a dim_in up_blocks . append ( make_resblock ( k1 , ( H , W , dim_in + dim_in ), dim_in )) up_blocks . append ( make_resblock ( k2 , ( H , W , dim_in + dim_in ), dim_in )) up_blocks . append ( make_attention ( key , ( H , W , dim_in ))) self . up_blocks = up_blocks # Final self . final_block = make_resblock ( next ( key_iter ), ( H , W , dim_in + dim_in ), dim_in ) self . proj_out = WeightNormConv ( input_shape = ( H , W , dim_in ), out_size = self . out_channels , filter_shape = ( 1 , 1 ), key = next ( key_iter )) __call__ ( self , * args , ** kwargs ) -> Array \u00a4 Call self as a function. Source code in generax/nn/unet.py def __call__ ( self , * args , ** kwargs ) -> Array : if self . time_dependent : if len ( args ) == 3 : t , x , y = args else : t , x = args y = None assert t . shape == () else : if len ( args ) == 2 : x , y = args else : x = args [ 0 ] y = None assert x . shape == self . input_shape # Time embedding if self . time_dependent : conditional_embedding = self . time_features ( t ) if y is not None : conditional_embedding = jnp . concatenate ([ conditional_embedding , y ], axis =- 1 ) else : conditional_embedding = y hs = [] # Initial convolution h = self . conv_in ( x ) hs . append ( h ) # Downsampling block_iter = iter ( self . down_blocks ) for i , ( dim_in , dim_out ) in enumerate ( self . in_out ): # Resnet block h = next ( block_iter )( h , conditional_embedding ) hs . append ( h ) # Resnet block + attention block h = next ( block_iter )( h , conditional_embedding ) h = next ( block_iter )( h ) hs . append ( h ) # Downsample h = next ( block_iter )( h ) # Middle res_block1 , attn_block , res_block2 = self . middle_blocks h = res_block1 ( h ) h = attn_block ( h ) h = res_block2 ( h ) # Upsampling block_iter = iter ( self . up_blocks ) for i , ( dim_in , dim_out ) in enumerate ( self . in_out [:: - 1 ]): # Upsample h = next ( block_iter )( h ) hs_ = hs . pop () if self . freeu : assert 0 , 'Not tested yet' if i == 0 : h_mean = h . mean ( axis =- 1 )[:,:, None ] h_max = h_mean . max () h_min = h_mean . max () h_mean = ( h_mean - h_min [ None , None ]) / ( h_max - h_min )[ None , None ] b1 = 1.5 s1 = 0.9 h = h . at [:,: 640 ] . mul (( b1 - 1 ) * h_mean + 1 ) hs_ = freeu_filter ( hs_ , threshold = 1.0 , scale = s1 ) # Resnet block h = jnp . concatenate ([ h , hs_ ], axis =- 1 ) h = next ( block_iter )( h , conditional_embedding ) # Resnet block h = jnp . concatenate ([ h , hs . pop ()], axis =- 1 ) h = next ( block_iter )( h , conditional_embedding ) # Attention block h = next ( block_iter )( h ) # Final h_in = hs . pop () h = jnp . concatenate ([ h , h_in ], axis =- 1 ) h = self . final_block ( h , conditional_embedding ) h = self . proj_out ( h ) return h generax.nn.unet.Encoder \u00a4 Half of the Unet architecture to use as an encoder. Input is an image and output is a vector Source code in generax/nn/unet.py class Encoder ( eqx . Module ): \"\"\"Half of the Unet architecture to use as an encoder. Input is an image and output is a vector \"\"\" input_shape : Tuple [ int ] = eqx . field ( static = True ) dim : int = eqx . field ( static = True ) dim_mults : Tuple [ int ] = eqx . field ( static = True ) in_out : Tuple [ Tuple [ int , int ]] = eqx . field ( static = True ) conv_in : WeightNormConv down_blocks : Tuple [ Union [ ImageResBlock , AttentionBlock , Downsample ]] middle_blocks : Tuple [ Union [ ImageResBlock , AttentionBlock ]] proj_out : WeightNormConv def __init__ ( self , input_shape : Tuple [ int ], dim : int = 16 , dim_mults : Tuple [ int ] = ( 1 , 2 , 4 , 8 ), resnet_block_groups : int = 8 , attn_heads : int = 4 , attn_dim_head : int = 32 , out_size : int = 16 , cond_shape : Optional [ Tuple [ int ]] = None , * , key : PRNGKeyArray ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `dim`: The dimension of the features - `dim_mults`: The dimension of the features at each downsampling - `resnet_block_groups`: The number of resnet blocks per downsampling - `attn_heads`: The number of attention heads per downsampling - `attn_dim_head`: The dimension of the attention heads - `out_size`: The dimension of the output \"\"\" H , W , C = input_shape if H // ( 2 ** len ( dim_mults )) == 0 : raise ValueError ( f \"Image size { ( H , W ) } is too small for { len ( dim_mults ) } downsamples.\" ) self . input_shape = input_shape self . dim = dim self . dim_mults = dim_mults keys = random . split ( key , 20 ) key_iter = iter ( keys ) self . conv_in = WeightNormConv ( input_shape = input_shape , out_size = self . dim , filter_shape = ( 7 , 7 ), padding = 3 , key = next ( key_iter )) def make_resblock ( key , input_shape , dim_out ): return ImageResBlock ( input_shape = input_shape , hidden_size = dim_out , out_size = dim_out , groups = resnet_block_groups , key = key ) def make_attention ( key , input_shape , linear = True ): return AttentionBlock ( input_shape = input_shape , heads = attn_heads , dim_head = attn_dim_head , key = key , use_linear_attention = linear ) # Downsampling down_blocks = [] dims = [ self . dim * mult for mult in self . dim_mults ] self . in_out = list ( zip ( dims [: - 1 ], dims [ 1 :])) keys = random . split ( next ( key_iter ), len ( self . in_out )) for i , ( key , ( dim_in , dim_out )) in enumerate ( zip ( keys , self . in_out )): k1 , k2 = random . split ( key , 2 ) down_blocks . append ( make_resblock ( k1 , ( H , W , dim_in ), dim_in )) down_blocks . append ( make_resblock ( k2 , ( H , W , dim_in ), dim_in )) down_blocks . append ( make_attention ( key , ( H , W , dim_in ))) down = Downsample ( input_shape = ( H , W , dim_in ), out_size = dim_out , key = key ) down_blocks . append ( down ) assert H % 2 == 0 assert W % 2 == 0 H , W = H // 2 , W // 2 self . down_blocks = down_blocks # Middle middle_blocks = [] middle_blocks . append ( make_resblock ( next ( key_iter ), ( H , W , dim_out ), dim_out )) middle_blocks . append ( make_attention ( next ( key_iter ), ( H , W , dim_out ), linear = False )) middle_blocks . append ( make_resblock ( next ( key_iter ), ( H , W , dim_out ), dim_out )) self . middle_blocks = middle_blocks self . proj_out = WeightNormConv ( input_shape = ( H , W , dim_out ), out_size = out_size , filter_shape = ( H , W ), padding = 0 , key = next ( key_iter )) def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" return self def __call__ ( self , x , y = None ) -> Array : assert x . shape == self . input_shape conditional_embedding = y # Initial convolution h = self . conv_in ( x ) # Downsampling block_iter = iter ( self . down_blocks ) for i , ( dim_in , dim_out ) in enumerate ( self . in_out ): # Resnet block h = next ( block_iter )( h , conditional_embedding ) # Resnet block + attention block h = next ( block_iter )( h , conditional_embedding ) h = next ( block_iter )( h ) # Downsample h = next ( block_iter )( h ) # Middle res_block1 , attn_block , res_block2 = self . middle_blocks h = res_block1 ( h ) h = attn_block ( h ) h = res_block2 ( h ) # Final h = self . proj_out ( h ) return h . ravel () __init__ ( self , input_shape : Tuple [ int ], dim : int = 16 , dim_mults : Tuple [ int ] = ( 1 , 2 , 4 , 8 ), resnet_block_groups : int = 8 , attn_heads : int = 4 , attn_dim_head : int = 32 , out_size : int = 16 , cond_shape : Optional [ Tuple [ int ]] = None , * , key : PRNGKeyArray ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. dim : The dimension of the features dim_mults : The dimension of the features at each downsampling resnet_block_groups : The number of resnet blocks per downsampling attn_heads : The number of attention heads per downsampling attn_dim_head : The dimension of the attention heads out_size : The dimension of the output Source code in generax/nn/unet.py def __init__ ( self , input_shape : Tuple [ int ], dim : int = 16 , dim_mults : Tuple [ int ] = ( 1 , 2 , 4 , 8 ), resnet_block_groups : int = 8 , attn_heads : int = 4 , attn_dim_head : int = 32 , out_size : int = 16 , cond_shape : Optional [ Tuple [ int ]] = None , * , key : PRNGKeyArray ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `dim`: The dimension of the features - `dim_mults`: The dimension of the features at each downsampling - `resnet_block_groups`: The number of resnet blocks per downsampling - `attn_heads`: The number of attention heads per downsampling - `attn_dim_head`: The dimension of the attention heads - `out_size`: The dimension of the output \"\"\" H , W , C = input_shape if H // ( 2 ** len ( dim_mults )) == 0 : raise ValueError ( f \"Image size { ( H , W ) } is too small for { len ( dim_mults ) } downsamples.\" ) self . input_shape = input_shape self . dim = dim self . dim_mults = dim_mults keys = random . split ( key , 20 ) key_iter = iter ( keys ) self . conv_in = WeightNormConv ( input_shape = input_shape , out_size = self . dim , filter_shape = ( 7 , 7 ), padding = 3 , key = next ( key_iter )) def make_resblock ( key , input_shape , dim_out ): return ImageResBlock ( input_shape = input_shape , hidden_size = dim_out , out_size = dim_out , groups = resnet_block_groups , key = key ) def make_attention ( key , input_shape , linear = True ): return AttentionBlock ( input_shape = input_shape , heads = attn_heads , dim_head = attn_dim_head , key = key , use_linear_attention = linear ) # Downsampling down_blocks = [] dims = [ self . dim * mult for mult in self . dim_mults ] self . in_out = list ( zip ( dims [: - 1 ], dims [ 1 :])) keys = random . split ( next ( key_iter ), len ( self . in_out )) for i , ( key , ( dim_in , dim_out )) in enumerate ( zip ( keys , self . in_out )): k1 , k2 = random . split ( key , 2 ) down_blocks . append ( make_resblock ( k1 , ( H , W , dim_in ), dim_in )) down_blocks . append ( make_resblock ( k2 , ( H , W , dim_in ), dim_in )) down_blocks . append ( make_attention ( key , ( H , W , dim_in ))) down = Downsample ( input_shape = ( H , W , dim_in ), out_size = dim_out , key = key ) down_blocks . append ( down ) assert H % 2 == 0 assert W % 2 == 0 H , W = H // 2 , W // 2 self . down_blocks = down_blocks # Middle middle_blocks = [] middle_blocks . append ( make_resblock ( next ( key_iter ), ( H , W , dim_out ), dim_out )) middle_blocks . append ( make_attention ( next ( key_iter ), ( H , W , dim_out ), linear = False )) middle_blocks . append ( make_resblock ( next ( key_iter ), ( H , W , dim_out ), dim_out )) self . middle_blocks = middle_blocks self . proj_out = WeightNormConv ( input_shape = ( H , W , dim_out ), out_size = out_size , filter_shape = ( H , W ), padding = 0 , key = next ( key_iter )) __call__ ( self , x , y = None ) -> Array \u00a4 Call self as a function. Source code in generax/nn/unet.py def __call__ ( self , x , y = None ) -> Array : assert x . shape == self . input_shape conditional_embedding = y # Initial convolution h = self . conv_in ( x ) # Downsampling block_iter = iter ( self . down_blocks ) for i , ( dim_in , dim_out ) in enumerate ( self . in_out ): # Resnet block h = next ( block_iter )( h , conditional_embedding ) # Resnet block + attention block h = next ( block_iter )( h , conditional_embedding ) h = next ( block_iter )( h ) # Downsample h = next ( block_iter )( h ) # Middle res_block1 , attn_block , res_block2 = self . middle_blocks h = res_block1 ( h ) h = attn_block ( h ) h = res_block2 ( h ) # Final h = self . proj_out ( h ) return h . ravel ()","title":"UNet"},{"location":"api/nn/unet/#unet","text":"","title":"UNet"},{"location":"api/nn/unet/#generax.nn.unet.UNet","text":"Unet architecture. Source code in generax/nn/unet.py class UNet ( eqx . Module ): \"\"\"Unet architecture. \"\"\" input_shape : Tuple [ int ] = eqx . field ( static = True ) dim : int = eqx . field ( static = True ) out_channels : int = eqx . field ( static = True ) dim_mults : Tuple [ int ] = eqx . field ( static = True ) in_out : Tuple [ Tuple [ int , int ]] = eqx . field ( static = True ) conv_in : WeightNormConv time_features : TimeFeatures down_blocks : Tuple [ Union [ ImageResBlock , AttentionBlock , Downsample ]] middle_blocks : Tuple [ Union [ ImageResBlock , AttentionBlock ]] up_blocks : Tuple [ Union [ ImageResBlock , AttentionBlock , Upsample ]] final_block : ImageResBlock proj_out : WeightNormConv freeu : bool = eqx . field ( static = True ) time_dependent : bool = eqx . field ( static = True ) cond_shape : Optional [ Tuple [ int ]] = eqx . field ( static = True ) def __init__ ( self , input_shape : Tuple [ int ], dim : int = 16 , out_channels : Optional [ int ] = None , dim_mults : Tuple [ int ] = ( 1 , 2 , 4 , 8 ), resnet_block_groups : int = 8 , attn_heads : int = 4 , attn_dim_head : int = 32 , cond_shape : Optional [ Tuple [ int ]] = None , * , key : PRNGKeyArray , freeu : bool = False , time_dependent : bool = True ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `dim`: The dimension of the features - `out_channels`: The number of output channels. If None, then the same as the input. - `dim_mults`: The dimension of the features at each downsampling - `resnet_block_groups`: The number of resnet blocks per downsampling - `attn_heads`: The number of attention heads per downsampling - `attn_dim_head`: The dimension of the attention heads - `freeu`: Whether to use freeu filtering - `time_dependent`: Whether to use time conditioning \"\"\" H , W , C = input_shape if H // ( 2 ** len ( dim_mults )) == 0 : raise ValueError ( f \"Image size { ( H , W ) } is too small for { len ( dim_mults ) } downsamples.\" ) self . input_shape = input_shape self . dim = dim self . out_channels = C if out_channels is None else out_channels self . dim_mults = dim_mults self . freeu = freeu self . time_dependent = time_dependent keys = random . split ( key , 20 ) key_iter = iter ( keys ) self . conv_in = WeightNormConv ( input_shape = input_shape , out_size = self . dim , filter_shape = ( 7 , 7 ), padding = 3 , key = next ( key_iter )) if self . time_dependent : self . time_features = TimeFeatures ( embedding_size = self . dim , out_features = 4 * self . dim , key = next ( key_iter )) time_shape = ( 4 * self . dim ,) else : self . time_features = None time_shape = None self . cond_shape = cond_shape if cond_shape is not None : assert len ( cond_shape ) == 1 if time_shape : time_shape = ( time_shape [ 0 ] + cond_shape [ 0 ],) else : time_shape = cond_shape def make_resblock ( key , input_shape , dim_out ): return ImageResBlock ( input_shape = input_shape , hidden_size = dim_out , out_size = dim_out , groups = resnet_block_groups , cond_shape = time_shape , key = key ) def make_attention ( key , input_shape , linear = True ): return AttentionBlock ( input_shape = input_shape , heads = attn_heads , dim_head = attn_dim_head , key = key , use_linear_attention = linear ) # Downsampling down_blocks = [] dims = [ self . dim * mult for mult in self . dim_mults ] self . in_out = list ( zip ( dims [: - 1 ], dims [ 1 :])) keys = random . split ( next ( key_iter ), len ( self . in_out )) for i , ( key , ( dim_in , dim_out )) in enumerate ( zip ( keys , self . in_out )): k1 , k2 = random . split ( key , 2 ) down_blocks . append ( make_resblock ( k1 , ( H , W , dim_in ), dim_in )) down_blocks . append ( make_resblock ( k2 , ( H , W , dim_in ), dim_in )) down_blocks . append ( make_attention ( key , ( H , W , dim_in ))) down = Downsample ( input_shape = ( H , W , dim_in ), out_size = dim_out , key = key ) down_blocks . append ( down ) assert H % 2 == 0 assert W % 2 == 0 H , W = H // 2 , W // 2 self . down_blocks = down_blocks # Middle middle_blocks = [] middle_blocks . append ( make_resblock ( next ( key_iter ), ( H , W , dim_out ), dim_out )) middle_blocks . append ( make_attention ( next ( key_iter ), ( H , W , dim_out ), linear = False )) middle_blocks . append ( make_resblock ( next ( key_iter ), ( H , W , dim_out ), dim_out )) self . middle_blocks = middle_blocks # Upsampling keys = random . split ( next ( key_iter ), len ( self . in_out )) up_blocks = [] for i , ( key , ( dim_in , dim_out )) in enumerate ( zip ( keys , self . in_out [:: - 1 ])): k1 , k2 = random . split ( key , 2 ) up = Upsample ( input_shape = ( H , W , dim_out ), out_size = dim_in , key = key ) up_blocks . append ( up ) H , W = H * 2 , W * 2 # Skip connections contribute a dim_in up_blocks . append ( make_resblock ( k1 , ( H , W , dim_in + dim_in ), dim_in )) up_blocks . append ( make_resblock ( k2 , ( H , W , dim_in + dim_in ), dim_in )) up_blocks . append ( make_attention ( key , ( H , W , dim_in ))) self . up_blocks = up_blocks # Final self . final_block = make_resblock ( next ( key_iter ), ( H , W , dim_in + dim_in ), dim_in ) self . proj_out = WeightNormConv ( input_shape = ( H , W , dim_in ), out_size = self . out_channels , filter_shape = ( 1 , 1 ), key = next ( key_iter )) def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" return self def __call__ ( self , * args , ** kwargs ) -> Array : if self . time_dependent : if len ( args ) == 3 : t , x , y = args else : t , x = args y = None assert t . shape == () else : if len ( args ) == 2 : x , y = args else : x = args [ 0 ] y = None assert x . shape == self . input_shape # Time embedding if self . time_dependent : conditional_embedding = self . time_features ( t ) if y is not None : conditional_embedding = jnp . concatenate ([ conditional_embedding , y ], axis =- 1 ) else : conditional_embedding = y hs = [] # Initial convolution h = self . conv_in ( x ) hs . append ( h ) # Downsampling block_iter = iter ( self . down_blocks ) for i , ( dim_in , dim_out ) in enumerate ( self . in_out ): # Resnet block h = next ( block_iter )( h , conditional_embedding ) hs . append ( h ) # Resnet block + attention block h = next ( block_iter )( h , conditional_embedding ) h = next ( block_iter )( h ) hs . append ( h ) # Downsample h = next ( block_iter )( h ) # Middle res_block1 , attn_block , res_block2 = self . middle_blocks h = res_block1 ( h ) h = attn_block ( h ) h = res_block2 ( h ) # Upsampling block_iter = iter ( self . up_blocks ) for i , ( dim_in , dim_out ) in enumerate ( self . in_out [:: - 1 ]): # Upsample h = next ( block_iter )( h ) hs_ = hs . pop () if self . freeu : assert 0 , 'Not tested yet' if i == 0 : h_mean = h . mean ( axis =- 1 )[:,:, None ] h_max = h_mean . max () h_min = h_mean . max () h_mean = ( h_mean - h_min [ None , None ]) / ( h_max - h_min )[ None , None ] b1 = 1.5 s1 = 0.9 h = h . at [:,: 640 ] . mul (( b1 - 1 ) * h_mean + 1 ) hs_ = freeu_filter ( hs_ , threshold = 1.0 , scale = s1 ) # Resnet block h = jnp . concatenate ([ h , hs_ ], axis =- 1 ) h = next ( block_iter )( h , conditional_embedding ) # Resnet block h = jnp . concatenate ([ h , hs . pop ()], axis =- 1 ) h = next ( block_iter )( h , conditional_embedding ) # Attention block h = next ( block_iter )( h ) # Final h_in = hs . pop () h = jnp . concatenate ([ h , h_in ], axis =- 1 ) h = self . final_block ( h , conditional_embedding ) h = self . proj_out ( h ) return h","title":"UNet"},{"location":"api/nn/unet/#generax.nn.unet.Encoder","text":"Half of the Unet architecture to use as an encoder. Input is an image and output is a vector Source code in generax/nn/unet.py class Encoder ( eqx . Module ): \"\"\"Half of the Unet architecture to use as an encoder. Input is an image and output is a vector \"\"\" input_shape : Tuple [ int ] = eqx . field ( static = True ) dim : int = eqx . field ( static = True ) dim_mults : Tuple [ int ] = eqx . field ( static = True ) in_out : Tuple [ Tuple [ int , int ]] = eqx . field ( static = True ) conv_in : WeightNormConv down_blocks : Tuple [ Union [ ImageResBlock , AttentionBlock , Downsample ]] middle_blocks : Tuple [ Union [ ImageResBlock , AttentionBlock ]] proj_out : WeightNormConv def __init__ ( self , input_shape : Tuple [ int ], dim : int = 16 , dim_mults : Tuple [ int ] = ( 1 , 2 , 4 , 8 ), resnet_block_groups : int = 8 , attn_heads : int = 4 , attn_dim_head : int = 32 , out_size : int = 16 , cond_shape : Optional [ Tuple [ int ]] = None , * , key : PRNGKeyArray ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `dim`: The dimension of the features - `dim_mults`: The dimension of the features at each downsampling - `resnet_block_groups`: The number of resnet blocks per downsampling - `attn_heads`: The number of attention heads per downsampling - `attn_dim_head`: The dimension of the attention heads - `out_size`: The dimension of the output \"\"\" H , W , C = input_shape if H // ( 2 ** len ( dim_mults )) == 0 : raise ValueError ( f \"Image size { ( H , W ) } is too small for { len ( dim_mults ) } downsamples.\" ) self . input_shape = input_shape self . dim = dim self . dim_mults = dim_mults keys = random . split ( key , 20 ) key_iter = iter ( keys ) self . conv_in = WeightNormConv ( input_shape = input_shape , out_size = self . dim , filter_shape = ( 7 , 7 ), padding = 3 , key = next ( key_iter )) def make_resblock ( key , input_shape , dim_out ): return ImageResBlock ( input_shape = input_shape , hidden_size = dim_out , out_size = dim_out , groups = resnet_block_groups , key = key ) def make_attention ( key , input_shape , linear = True ): return AttentionBlock ( input_shape = input_shape , heads = attn_heads , dim_head = attn_dim_head , key = key , use_linear_attention = linear ) # Downsampling down_blocks = [] dims = [ self . dim * mult for mult in self . dim_mults ] self . in_out = list ( zip ( dims [: - 1 ], dims [ 1 :])) keys = random . split ( next ( key_iter ), len ( self . in_out )) for i , ( key , ( dim_in , dim_out )) in enumerate ( zip ( keys , self . in_out )): k1 , k2 = random . split ( key , 2 ) down_blocks . append ( make_resblock ( k1 , ( H , W , dim_in ), dim_in )) down_blocks . append ( make_resblock ( k2 , ( H , W , dim_in ), dim_in )) down_blocks . append ( make_attention ( key , ( H , W , dim_in ))) down = Downsample ( input_shape = ( H , W , dim_in ), out_size = dim_out , key = key ) down_blocks . append ( down ) assert H % 2 == 0 assert W % 2 == 0 H , W = H // 2 , W // 2 self . down_blocks = down_blocks # Middle middle_blocks = [] middle_blocks . append ( make_resblock ( next ( key_iter ), ( H , W , dim_out ), dim_out )) middle_blocks . append ( make_attention ( next ( key_iter ), ( H , W , dim_out ), linear = False )) middle_blocks . append ( make_resblock ( next ( key_iter ), ( H , W , dim_out ), dim_out )) self . middle_blocks = middle_blocks self . proj_out = WeightNormConv ( input_shape = ( H , W , dim_out ), out_size = out_size , filter_shape = ( H , W ), padding = 0 , key = next ( key_iter )) def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" return self def __call__ ( self , x , y = None ) -> Array : assert x . shape == self . input_shape conditional_embedding = y # Initial convolution h = self . conv_in ( x ) # Downsampling block_iter = iter ( self . down_blocks ) for i , ( dim_in , dim_out ) in enumerate ( self . in_out ): # Resnet block h = next ( block_iter )( h , conditional_embedding ) # Resnet block + attention block h = next ( block_iter )( h , conditional_embedding ) h = next ( block_iter )( h ) # Downsample h = next ( block_iter )( h ) # Middle res_block1 , attn_block , res_block2 = self . middle_blocks h = res_block1 ( h ) h = attn_block ( h ) h = res_block2 ( h ) # Final h = self . proj_out ( h ) return h . ravel ()","title":"Encoder"},{"location":"notebooks/cnf/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); import jax import jax.numpy as jnp from jax import random from functools import partial from generax.trainer import Trainer import matplotlib.pyplot as plt import equinox as eqx from jaxtyping import Array , PRNGKeyArray import generax as gx from generax.distributions.flow_models import TimeDependentNormalizingFlow , ContinuousNormalizingFlow Here we'll show how to train continuous normalizing flows using flow matching \u00a4 class EmpiricalDistribution ( gx . ProbabilityDistribution ): data : Array def __init__ ( self , data ): self . data = data x_shape = data . shape [ 1 :] super () . __init__ ( input_shape = x_shape ) def sample_and_log_prob ( self ): assert 0 , \"Can't compute\" def log_prob ( self ): assert 0 , \"Can't compute\" def sample ( self , key ): return random . choice ( key , self . data , shape = ( 1 ,))[ 0 ] def train_iterator ( self , key , batch_size ): total_choices = jnp . arange ( self . data . shape [ 0 ]) while True : key , _ = random . split ( key , 2 ) idx = random . choice ( key , total_choices , shape = ( batch_size ,), replace = True ) yield dict ( x = self . data [ idx ]) from sklearn.datasets import make_swiss_roll data , y = make_swiss_roll ( n_samples = 100000 , noise = 0.5 ) data = data [:, [ 0 , 2 ]] data = data - data . mean ( axis = 0 ) data = data / data . std ( axis = 0 ) p1 = EmpiricalDistribution ( data ) key = random . PRNGKey ( 0 ) keys = random . split ( key , 1000 ) samples = eqx . filter_vmap ( p1 . sample )( keys ) plt . scatter ( * samples . T ) <matplotlib.collections.PathCollection at 0x7fcc268e9120> A probability path is a time dependent probability distribution \\(p_t\\) from \\(t=0\\) to \\(t=1\\) . We want \\(p_0 = N(0,I)\\) and \\(p_1 = p_{\\text{data}}\\) . We can construct this path as the expectation of a conditional probability path: $$ p_t(x_t) = \\int p_1(x_1)p_t(x_t|x_1)dx_1 $$ One choice for the conditional distribution is \\(p_t(x_t|x_1) = N(x_t|tx_1,(1-t)^2I)\\) , which is a linear interpolation between \\(x_0=0\\) and \\(x_1\\) (conditional optimal transport). x_shape = p1 . input_shape transform = gx . ConditionalOptionalTransport ( input_shape = x_shape , key = key ) cond_ppath = TimeDependentNormalizingFlow ( transform = transform , prior = gx . Gaussian ( input_shape = x_shape )) p0 = gx . Gaussian ( input_shape = x_shape ) def sample_xt ( t , key ): k1 , k2 = random . split ( key , 2 ) x1 = p1 . sample ( k1 ) x0 = p0 . sample ( k2 ) xt = cond_ppath . to_data_space ( t , x0 , x1 ) return xt ts = jnp . linspace ( 0 , 1 , 6 ) keys = random . split ( key , 1000 ) xt_samples = jax . vmap ( jax . vmap ( sample_xt , in_axes = ( 0 , None )), in_axes = ( None , 0 ))( ts , keys ) n_rows , n_cols = 1 , ts . shape [ 0 ] size = 4 fig , axes = plt . subplots ( n_rows , n_cols , figsize = ( n_cols * size , n_rows * size )) for i in range ( n_cols ): axes [ i ] . scatter ( * xt_samples [:, i ] . T , s = 4 , c = 'red' , alpha = 0.6 ) axes [ i ] . set_title ( f \"t= { ts [ i ] : .2f } \" ) Without using any parameters, we constructed a path between a Gaussian and our data. Next, we'll learn a continuous normalizing flow that will learn this probability path. # Construct the neural network that learn the score net = gx . TimeDependentResNet ( input_shape = x_shape , working_size = 16 , hidden_size = 32 , out_size = x_shape [ - 1 ], n_blocks = 5 , embedding_size = 16 , out_features = 32 , key = key ) flow = ContinuousNormalizingFlow ( input_shape = x_shape , net = net , key = key , controller_atol = 1e-5 , controller_rtol = 1e-5 ) def loss ( flow , data , key ): def unbatched_loss ( data , key ): k1 , k2 = random . split ( key , 2 ) # Sample x1 = data [ 'x' ] x0 = cond_ppath . prior . sample ( k1 ) t = random . uniform ( k2 ) # Compute f_t(x_0; x_1) def ft ( t ): return cond_ppath . to_data_space ( t , x0 , x1 ) xt , ut = jax . jvp ( ft , ( t ,), ( jnp . ones_like ( t ),)) # Compute the parametric vector field vt = flow . net ( t , xt ) # Compute the loss return jnp . sum (( ut - vt ) ** 2 ) keys = random . split ( key , data [ 'x' ] . shape [ 0 ]) objective = jax . vmap ( unbatched_loss )( data , keys ) . mean () aux = dict ( objective = objective ) return objective , aux # Create the optimizer import optax schedule = optax . warmup_cosine_decay_schedule ( init_value = 0.0 , peak_value = 1.0 , warmup_steps = 1000 , decay_steps = 3e5 , end_value = 0.1 , exponent = 1.0 ) chain = [] chain . append ( optax . clip_by_global_norm ( 15.0 )) chain . append ( optax . adamw ( 1e-3 )) chain . append ( optax . scale_by_schedule ( schedule )) optimizer = optax . chain ( * chain ) # Create the trainer and optimize trainer = Trainer ( checkpoint_path = 'tmp/flow/flow_matching' ) flow = trainer . train ( model = flow , objective = loss , evaluate_model = lambda x : x , optimizer = optimizer , num_steps = 30000 , double_batch = 1000 , data_iterator = p1 . train_iterator ( key , batch_size = 128 ), checkpoint_every = 5000 , test_every =- 1 , retrain = True ) loss: 2.9090: 20%|\u2588\u2588 | 6000/30000 [00:22<01:10, 340.63it/s] Checkpointed model loss: 2.8846: 37%|\u2588\u2588\u2588\u258b | 11000/30000 [00:38<01:01, 310.04it/s] Checkpointed model loss: 2.8767: 53%|\u2588\u2588\u2588\u2588\u2588\u258e | 16000/30000 [00:51<00:35, 397.60it/s] Checkpointed model loss: 2.8872: 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 21000/30000 [01:01<00:18, 482.02it/s] Checkpointed model loss: 2.8765: 87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 26000/30000 [01:11<00:07, 502.76it/s] Checkpointed model loss: 2.8664: 0%| | 30/30000 [01:18<21:54:47, 2.63s/it] Checkpointed model ts = jnp . linspace ( 0 , 1 , 6 ) def ode_solve ( x0 ): z , log_det = flow . transform . neural_ode ( x0 , inverse = True , log_likelihood = True , save_at = ts ) return z , log_det n_samples = 10000 keys = random . split ( key , n_samples ) x0 , log_p0s = eqx . filter_vmap ( flow . prior . sample_and_log_prob )( keys ) xts , log_dets = jax . vmap ( ode_solve )( x0 ) log_pxs = log_p0s [:, None ] - log_dets n_rows , n_cols = 1 , ts . shape [ 0 ] size = 4 fig , axes = plt . subplots ( n_rows , n_cols , figsize = ( n_cols * size , n_rows * size )) for i in range ( n_cols ): axes [ i ] . scatter ( * xts [:, i ] . T , c = jnp . exp ( log_pxs [:, i ]), alpha = 0.5 , s = 10 ) axes [ i ] . set_title ( f \"t= { ts [ i ] : .2f } \" ) axes [ i ] . set_aspect ( 'equal' , 'box' )","title":"Flow matching tutorial"},{"location":"notebooks/cnf/#here-well-show-how-to-train-continuous-normalizing-flows-using-flow-matching","text":"class EmpiricalDistribution ( gx . ProbabilityDistribution ): data : Array def __init__ ( self , data ): self . data = data x_shape = data . shape [ 1 :] super () . __init__ ( input_shape = x_shape ) def sample_and_log_prob ( self ): assert 0 , \"Can't compute\" def log_prob ( self ): assert 0 , \"Can't compute\" def sample ( self , key ): return random . choice ( key , self . data , shape = ( 1 ,))[ 0 ] def train_iterator ( self , key , batch_size ): total_choices = jnp . arange ( self . data . shape [ 0 ]) while True : key , _ = random . split ( key , 2 ) idx = random . choice ( key , total_choices , shape = ( batch_size ,), replace = True ) yield dict ( x = self . data [ idx ]) from sklearn.datasets import make_swiss_roll data , y = make_swiss_roll ( n_samples = 100000 , noise = 0.5 ) data = data [:, [ 0 , 2 ]] data = data - data . mean ( axis = 0 ) data = data / data . std ( axis = 0 ) p1 = EmpiricalDistribution ( data ) key = random . PRNGKey ( 0 ) keys = random . split ( key , 1000 ) samples = eqx . filter_vmap ( p1 . sample )( keys ) plt . scatter ( * samples . T ) <matplotlib.collections.PathCollection at 0x7fcc268e9120> A probability path is a time dependent probability distribution \\(p_t\\) from \\(t=0\\) to \\(t=1\\) . We want \\(p_0 = N(0,I)\\) and \\(p_1 = p_{\\text{data}}\\) . We can construct this path as the expectation of a conditional probability path: $$ p_t(x_t) = \\int p_1(x_1)p_t(x_t|x_1)dx_1 $$ One choice for the conditional distribution is \\(p_t(x_t|x_1) = N(x_t|tx_1,(1-t)^2I)\\) , which is a linear interpolation between \\(x_0=0\\) and \\(x_1\\) (conditional optimal transport). x_shape = p1 . input_shape transform = gx . ConditionalOptionalTransport ( input_shape = x_shape , key = key ) cond_ppath = TimeDependentNormalizingFlow ( transform = transform , prior = gx . Gaussian ( input_shape = x_shape )) p0 = gx . Gaussian ( input_shape = x_shape ) def sample_xt ( t , key ): k1 , k2 = random . split ( key , 2 ) x1 = p1 . sample ( k1 ) x0 = p0 . sample ( k2 ) xt = cond_ppath . to_data_space ( t , x0 , x1 ) return xt ts = jnp . linspace ( 0 , 1 , 6 ) keys = random . split ( key , 1000 ) xt_samples = jax . vmap ( jax . vmap ( sample_xt , in_axes = ( 0 , None )), in_axes = ( None , 0 ))( ts , keys ) n_rows , n_cols = 1 , ts . shape [ 0 ] size = 4 fig , axes = plt . subplots ( n_rows , n_cols , figsize = ( n_cols * size , n_rows * size )) for i in range ( n_cols ): axes [ i ] . scatter ( * xt_samples [:, i ] . T , s = 4 , c = 'red' , alpha = 0.6 ) axes [ i ] . set_title ( f \"t= { ts [ i ] : .2f } \" ) Without using any parameters, we constructed a path between a Gaussian and our data. Next, we'll learn a continuous normalizing flow that will learn this probability path. # Construct the neural network that learn the score net = gx . TimeDependentResNet ( input_shape = x_shape , working_size = 16 , hidden_size = 32 , out_size = x_shape [ - 1 ], n_blocks = 5 , embedding_size = 16 , out_features = 32 , key = key ) flow = ContinuousNormalizingFlow ( input_shape = x_shape , net = net , key = key , controller_atol = 1e-5 , controller_rtol = 1e-5 ) def loss ( flow , data , key ): def unbatched_loss ( data , key ): k1 , k2 = random . split ( key , 2 ) # Sample x1 = data [ 'x' ] x0 = cond_ppath . prior . sample ( k1 ) t = random . uniform ( k2 ) # Compute f_t(x_0; x_1) def ft ( t ): return cond_ppath . to_data_space ( t , x0 , x1 ) xt , ut = jax . jvp ( ft , ( t ,), ( jnp . ones_like ( t ),)) # Compute the parametric vector field vt = flow . net ( t , xt ) # Compute the loss return jnp . sum (( ut - vt ) ** 2 ) keys = random . split ( key , data [ 'x' ] . shape [ 0 ]) objective = jax . vmap ( unbatched_loss )( data , keys ) . mean () aux = dict ( objective = objective ) return objective , aux # Create the optimizer import optax schedule = optax . warmup_cosine_decay_schedule ( init_value = 0.0 , peak_value = 1.0 , warmup_steps = 1000 , decay_steps = 3e5 , end_value = 0.1 , exponent = 1.0 ) chain = [] chain . append ( optax . clip_by_global_norm ( 15.0 )) chain . append ( optax . adamw ( 1e-3 )) chain . append ( optax . scale_by_schedule ( schedule )) optimizer = optax . chain ( * chain ) # Create the trainer and optimize trainer = Trainer ( checkpoint_path = 'tmp/flow/flow_matching' ) flow = trainer . train ( model = flow , objective = loss , evaluate_model = lambda x : x , optimizer = optimizer , num_steps = 30000 , double_batch = 1000 , data_iterator = p1 . train_iterator ( key , batch_size = 128 ), checkpoint_every = 5000 , test_every =- 1 , retrain = True ) loss: 2.9090: 20%|\u2588\u2588 | 6000/30000 [00:22<01:10, 340.63it/s] Checkpointed model loss: 2.8846: 37%|\u2588\u2588\u2588\u258b | 11000/30000 [00:38<01:01, 310.04it/s] Checkpointed model loss: 2.8767: 53%|\u2588\u2588\u2588\u2588\u2588\u258e | 16000/30000 [00:51<00:35, 397.60it/s] Checkpointed model loss: 2.8872: 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 21000/30000 [01:01<00:18, 482.02it/s] Checkpointed model loss: 2.8765: 87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 26000/30000 [01:11<00:07, 502.76it/s] Checkpointed model loss: 2.8664: 0%| | 30/30000 [01:18<21:54:47, 2.63s/it] Checkpointed model ts = jnp . linspace ( 0 , 1 , 6 ) def ode_solve ( x0 ): z , log_det = flow . transform . neural_ode ( x0 , inverse = True , log_likelihood = True , save_at = ts ) return z , log_det n_samples = 10000 keys = random . split ( key , n_samples ) x0 , log_p0s = eqx . filter_vmap ( flow . prior . sample_and_log_prob )( keys ) xts , log_dets = jax . vmap ( ode_solve )( x0 ) log_pxs = log_p0s [:, None ] - log_dets n_rows , n_cols = 1 , ts . shape [ 0 ] size = 4 fig , axes = plt . subplots ( n_rows , n_cols , figsize = ( n_cols * size , n_rows * size )) for i in range ( n_cols ): axes [ i ] . scatter ( * xts [:, i ] . T , c = jnp . exp ( log_pxs [:, i ]), alpha = 0.5 , s = 10 ) axes [ i ] . set_title ( f \"t= { ts [ i ] : .2f } \" ) axes [ i ] . set_aspect ( 'equal' , 'box' )","title":"Here we'll show how to train continuous normalizing flows using flow matching"}]}