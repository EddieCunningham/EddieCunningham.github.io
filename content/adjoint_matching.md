Title: Adjoint matching
Date: 2025-08-18
Category: Blog
status: hidden
Slug: adjoint_matching
Summary: Derivation of the adjoint matching method for finetuning diffusion models

# Motivation
Let $p(x)$ be the distribution generated by a flow based generative model and suppose that we want to control this model to generate samples from a target distribution, $p^*(x)$.  For example, suppose we pre-train a model on a dataset and want to finetune it so that its samples maximize a reward function, $r(x)$.  To do so, we might want to control the pretrained model so that it generates from the distribution proportional to $p(x)e^{r(x)}$.  Adjoint matching is an efficient way to finetune flow-based generative models by solving a specially designed stochastic control problem.  It is comprised of three parts:
1. An efficient algorithm for minimizing an SOC problem.
2. A special formulation of the pretrained model that ensures that the solution to the SOC problem results in a model that generates samples from the target distribution.
3. For added computational efficiency, a multi-level optimization approach that alternates between sampling from the controlled process and optimizing the control over the samples.

In this post, we will derive each of these different parts.

## Part 1: Adjoint matching
Adjoint matching is an algorithm to solve general stochastic control problems of the form
$$
\begin{align}
  J_t[u_\theta](x_t) = \mathbb{E}\left[ \int_t^1 \frac{1}{2}\|u_s(x_s;\theta)\|^2 + f_s(x_s) ds + g(x_1) \;\;|\;\; x_t \right], \\
  \text{where }\; dx_s = \left(b_s(x_s) + L_s u_s(x_s;\theta)\right)ds + L_s dW_s \; \text{ and }x_{s=t} = x_t
\end{align}
$$
where the control, $u_s(x_s;\theta)$, depends on the parameters, $\theta$.  The adjoint matching algorithm is founded on the fact that the derivative of the cost function with respect to the parameters of the control is $0$ only at the optimal control.  The derivative of the cost function with respect to the parameters of the control can be computed using the adjoint method, which introduces a new process called the *adjoint process*, denoted by $a_s$.  For a fixed trajectory sampled from the SDE, the adjoint process satisfies the following ODE:
$$
\begin{align}
  \frac{da_s}{ds} &= -\nabla \left(b_s(x_s) + L_s u_s(x_s)\right)^T a_s - \nabla \left(\frac{1}{2}\|u_s(x_s)\|^2 + f_s(x_s)\right), \quad a_1 = \nabla g(x_1) \\
  &= -\nabla b_s(x_s)^T a_s - \nabla f_s(x_s) - \nabla u_s(x_s)^T \left(L_s^T a_s + u_s(x_s)\right)
\end{align}
$$
Using the adjoint process, we can compute the derivative of the cost function with respect to the parameters of the control as follows:
$$
\begin{align}
  \frac{\partial J_t[u_\theta](x_t)}{\partial \theta} &= \mathbb{E}\left[ \int_t^1 \frac{\partial \frac{1}{2}\|u_s(x_s;\theta)\|^2}{\partial \theta} + \langle \frac{\partial b_s(x_s) + L_s u_s(x_s;\theta)}{\partial \theta}, a_s \rangle \;\;|\;\; x_t \right] \\
  &= \mathbb{E}\left[ \int_t^1 \langle \frac{\partial u_s(x_s;\theta)}{\partial \theta}, L_s^T a_s + u_s(x_s;\theta)\rangle \;\;|\;\; x_t \right]
\end{align}
$$
This expression is $0$ only when $L_s^T a_s + u_s(x_s;\theta) = 0$, and so the basic form of adjoint matching uses the following loss function:
$$
\begin{align}
  \mathcal{L}_\text{basic}(\theta) = \mathbb{E}\left[ \int_0^1 \|L_s^T a_s + u_s(x_s;\theta)\|^2 ds \right]
\end{align}
$$
where the expectation is taken over trajectories sampled from the SDE
$$
\begin{align}
  dx_s = \left(b_s(x_s) + L_s u_s(x_s;\theta)\right)ds + L_s dW_s, \quad x_{0} \sim p_0(x_0)
\end{align}
$$
and the adjoint process is solved for each trajectory.

This is called the *basic form* of adjoint matching because we can go a step further by noticing that a part of the time derivative of the adjoint process will be $0$ at the optimal control.  To this end, we define a new process, called the *lean adjoint*, which is given by
$$
\begin{align}
  \frac{d\tilde{a}_s}{ds} = -\nabla b_s(x_s)^T \tilde{a}_s - \nabla f_s(x_s), \quad \tilde{a}_1 = \nabla g(x_1)
\end{align}
$$
Notice that the difference between the adjoint process and the lean adjoint process is that the absence of the term $\nabla u_s(x_s)^T \left(L_s^T a_s + u_s(x_s)\right)$ in the time derivative of the lean adjoint process.  The adjoint matching paper uses this lean adjoint in conjunction with the same matching loss to define a loss that they call the *adjoint matching loss*, given by
$$
\begin{align}
  \mathcal{L}(\theta) = \mathbb{E}\left[ \int_0^1 \|L_s^T \tilde{a}_s + u_s(x_s;\theta)\|^2 ds \right]
\end{align}
$$
where the expectation is taken over trajectories sampled from the SDE
$$
\begin{align}
  dx_s = \left(b_s(x_s) + L_s u_s(x_s;\theta)\right)ds + L_s dW_s, \quad x_{0} \sim p_0(x_0)
\end{align}
$$
and
$$
\begin{align}
  \frac{d\tilde{a}_s}{ds} = -\nabla b_s(x_s)^T \tilde{a}_s - \nabla f_s(x_s), \quad \tilde{a}_1 = \nabla g(x_1)
\end{align}
$$
To perform gradient descent on $\mathcal{L}(\theta)$, we follow the following steps:
1. Select a discretization of the time interval $(t_1,\dots,t_N) \subset [0,1]$
2. Simulate the controlled SDE and save the samples at $(x_{t_1},\dots,x_{t_N})$
3. Solve the lean adjoint process for each trajectory using Euler's method to obtain the lean adjoints $(\tilde{a}_{t_1},\dots,\tilde{a}_{t_N})$
4. Stop the gradients with respect to $\theta$ through $x_{t_{1:N}}$ and $\tilde{a}_{t_{1:N}}$
5. Compute the integrand of the loss function, $\mathcal{L}(\theta)$, at each time step, $t_i$, and average the results to obtain an estimate of the gradient of $\mathcal{L}(\theta)$ with respect to $\theta$


## Part 2: The stochastic optimal control problem for finetuning
To connect the general SOC problem to our finetuning problem, we need to show that the solution to the SOC problem results in a model that generates samples from the target distribution.  To do this, we will show that a particular choice of the base drift, $b_t(x_t)$ and the terminal cost, $g(x_1)$, results in a model that generates samples from the target distribution.

Recall the general SOC problem:
$$
\begin{align}
  J_t[u](x_t) = \mathbb{E}\left[ \int_t^1 \frac{1}{2}\|u_s(x_s)\|^2 + f_s(x_s) ds + g(x_1) \;\;|\;\; x_t \right], \\
  \text{where }\; dx_s = \left(b_s(x_s) + L_s u_s(x_s)\right)ds + L_s dW_s \; \text{ and }x_{s=t} = x_t
\end{align}
$$
where $u_s(x_s)$ is the control at time $s$.  A classical result from control theory is that the value function, $V_t(x_t) := \underset{u}{\text{min}}J_t[u](x_t)$ and optimal control, $u_t^*(x_t) := \underset{u}{\text{min}}J_t[u](x_t)$ satisfy $u_t^* = -L_t^T \nabla V_t$ and the Hamilton-Jacobi-Bellman (HJB) equation:
$$
\begin{align}
  &\frac{\partial V_t}{\partial t} + \langle \nabla V_t, b_t \rangle + \frac{1}{2}\text{Div}(L_tL_t^T \nabla V_t) - \frac{1}{2}\|L_t^T \nabla V_t\|^2 + f_t = 0, \quad V_1 = g
\end{align}
$$
Additionally, the reverse is true - a function $V_t$ that satisfies the HJB equation is the value function for a stochastic control problem of the same form we started with.  We will use this last fact to show that the control we need to finetune our generative model is the solution to a stochastic control problem.

### Memoryless SDEs
Recall that a large majority of flow-based generative models are constructed using linear stochastic interpolants.  It is always possible to reparameterize these models so that they are memoryless, i.e. are able to written in the form
$$
\begin{align}
  dx_t = (-K_t x_t - l_t + L_tL_t^T \nabla \log p_t(x_t))dt + L_t dW_t
\end{align}
$$
where $p_t(x_t)$ is the marginal distribution of the SDE at time $t$.  This process is called **memoryless** because its joint distribution at its initial and final times factors independently.  This is easy to see because the reverse of the SDE is given by
$$
\begin{align}
  dx_s = (K_s x_s + l_s)ds + L_s dW_s, \quad s = 1 - t
\end{align}
$$
which is a linear SDE that is also guaranteed to be the Gaussian prior distribution, $p_0$ at $s=1$, regardless of the initial distribution at $s=1$.  Since linear SDEs have Gaussian transitions, the only way for the marginal distribution at $s=1$ to be Gaussian, regardless of the distribution at $s=0$ is if $p(x_{s=1} | x_{s=0}) = p(x_{s=1})$, i.e. the SDE is memoryless.

With this in mind, we can similarly construct a memoryless SDE that has a marginal distribution at $t=1$ that is proportional to the finetuned model.  Let $p_1^*(x_1)$ be the distribution of our the finetuned model at time $t=1$ that we want to learn a control to generate from.  Since $(K_t,l_t,L_t)$ are the parameters of a memoryless linear SDE, it is also true that the following SDE has a marginal distribution at $t=1$ that is proportional to the finetuned model,
$$
\begin{align}
  dx_t = (-K_t x_t - l_t + L_tL_t^T \nabla \log p_t^*(x_t))dt + L_t dW_t
\end{align}
$$
where $p_t^*(x_t)$ is the marginal distribution of the finetuned model at time $t$.  The marginal distribution of both of these SDEs satisfy the Fokker-Planck equation, which is the partial differential equation that describes the time evolution of the marginal distribution.  For our base SDE, the Fokker-Planck equation is given by:
$$
\begin{align}
  \frac{\partial p_t}{\partial t} + \text{Div}(p_t (-K_t x_t - l_t + L_tL_t^T \nabla \log p_t)) + \frac{1}{2}\text{Div}(L_tL_t^T \nabla p_t) = 0
\end{align}
$$
It will help us to rewrite this equation in a form that is more convenient for our purposes by dividing both sides by $p_t$ to get the evolution of the log-density.  After some algebra that involves expanding the divergence terms, we get the following equation:
$$
\begin{align}
  \frac{\partial \log p_t}{\partial t} + \langle \nabla \log p_t, -K_t x_t - l_t\rangle - \text{Tr}(K_t) + \frac{1}{2}\text{Div}(L_tL_t^T \nabla \log p_t) + \frac{1}{2}\|L_t^T \nabla \log p_t\|^2 = 0
\end{align}
$$
where $\text{Div}$ is the divergence operator and $\text{Tr}$ is the trace operator.  The same expression also holds for the finetuned model, given by
$$
\begin{align}
  \frac{\partial \log p_t^*}{\partial t} + \langle \nabla \log p_t^*, -K_t x_t - l_t\rangle - \text{Tr}(K_t) + \frac{1}{2}\text{Div}(L_tL_t^T \nabla \log p_t^*) + \frac{1}{2}\|L_t^T \nabla \log p_t^*\|^2 = 0
\end{align}
$$
The key step to show that the control we need to finetune our generative model is the solution to a stochastic control problem is to show that the difference between the log-densities of the two SDEs satisfies the HJB equation.  Let $V_t := \log p_t - \log p_t^*$.  Then, we can subtract the two Fokker-Planck equations, and use the fact that $\frac{1}{2}\|L_t^T \nabla \log p_t\|^2 - \frac{1}{2}\|L_t^T \nabla \log p_t^*\|^2 = -\frac{1}{2}\|L_t^T \nabla V_t\|^2 + \langle \nabla V_t, L_tL_t^T \nabla \log p_t \rangle$ to get the following equation:
$$
\begin{align}
  \frac{\partial V_t}{\partial t} + \langle \nabla V_t, -K_tx_t - l_t + L_tL_t^T \nabla \log p_t \rangle + \frac{1}{2}\text{Div}(L_tL_t^T \nabla V_t) - \frac{1}{2}\|L_t^T \nabla V_t\|^2 = 0
\end{align}
$$
We can notice that this is exactly the form of the HJB equations with $f_t(x_t) = 0$ and, by our definition of $V_t$, $g(x_1) = \log p_1(x_1) - \log p_1^*(x_1)$.  This means that $V_t$ is the value function for a stochastic control problem where the base SDE is given by
$$
\begin{align}
  J_t[u](x_t) = \mathbb{E}\left[ \int_t^1 \frac{1}{2}\|u_s(x_s)\|^2 ds + \log \frac{p_1(x_1)}{p_1^*(x_1)} \;\;|\;\; x_t \right],\quad\quad\quad\quad \\
  \text{where }\; dx_s = \left(b_s(x_s) + L_s u_s(x_s)\right)ds + L_s dW_s \; \text{ and }x_{s=t} = x_t
\end{align}
$$
where $b_s(x_s) = -K_s x_s - l_s + L_sL_s^T \nabla \log p_t$ is the drift of the base SDE.  We can do a sanity check to make sure that this is the correct form by plugging in the optimal control, $u_t^*(x_t) = -L_t^T \nabla V_t = -L_t^T \nabla \left(\log p_t - \log p_t^*\right)$.  We can easily see that the controlled drift is given by $b_s(x_s) + L_s u_s(x_s) = -K_s x_s - l_s + L_sL_s^T \nabla \log p_t^*$, which is exactly the drift of the finetuned model.



# Part 3: Reciprocal adjoint matching
Adjoint matching is a simple way to solve general SOC problems, but is still relatively expensive as a single gradient descent step requires simulating the controlled SDE, evaluating the gradient of the terminal cost and computing the lean adjoint process.  Reciprocal adjoint matching is an improvement of adjoint matching that maintains a fixed buffer of samples from the controlled SDE that are then used to perform multiple gradient descent steps on the adjoint matching loss.  This is made possible by exploiting the precise form of the finetuning problem from part 2 where our goal is to learn a control that generates samples from the finetuned model.

