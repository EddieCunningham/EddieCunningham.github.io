<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <meta name="generator" content="Pelican" />
        <title>Flow matching</title>
        <link rel="stylesheet" href="/theme/css/main.css" />
        <meta name="description" content="A tutorial on flow matching" />
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="/">The Weeds</a></h1>
                <nav><ul>
                    <li class="active"><a href="/category/blog.html">Blog</a></li>
                </ul></nav>
        </header><!-- /#banner -->
<section id="content" class="body">
  <article>
    <header>
      <h1 class="entry-title">
        <a href="/flow-matching.html" rel="bookmark"
           title="Permalink to Flow matching">Flow matching</a></h1>
    </header>

    <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2023-07-15T00:00:00-05:00">
                Published: Sat 15 July 2023
        </abbr>
		<br />
        <abbr class="modified" title="2023-07-15T00:00:00-05:00">
                Updated: Sat 15 July 2023
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/eddie-cunningham.html">Eddie Cunningham</a>
        </address>
<p>In <a href="/category/blog.html">Blog</a>.</p>
<p>tags: <a href="/tag/flows.html">flows</a> <a href="/tag/matching.html">matching</a> <a href="/tag/generative-models.html">generative models</a> </p>
</footer><!-- /.post-info -->      <p>Flow matching is probably the most important contribution to the field of normalizing flows in the last few years.  It allows us to train continuous normalizing flows in a simulation free way while also avoiding the pitfalls that comes with working with likelihoods.  In this tutorial, we'll go over what flow matching is and some of its practical properties.</p>
<h1>What problem are we trying to solve?</h1>
<p>We are interested in learning a parametric approximation of an unknown probability distribution that we can sample from and compute likelihood on.  Given a target probability distribution called <span class="math">\(p_\text{data}(x)\)</span>, where <span class="math">\(x\in \mathcal{X}\)</span>, we want to learn a parametric approximation called <span class="math">\(p_\text{model}(x;\theta)\)</span>.  In our problem setup, we assume that we can sample from <span class="math">\(p_\text{data}(x)\)</span> and that <span class="math">\(p_\text{data}(x)&gt;0, \forall x\in \mathcal{X}\)</span>.  The second assumption ensures that a solution exists, as we will see later.</p>
<p>Our goals are two fold:
1. Sample from <span class="math">\(p_\text{model}(x;\theta)\)</span>.
2. Compute the probability of a given sample <span class="math">\(x\)</span> under <span class="math">\(p_\text{model}(x)\)</span>.</p>
<h1>How do we solve this problem?</h1>
<p>We will learn an invertible function, <span class="math">\(f: \mathcal{X}_0 \to \mathcal{X}_1\)</span> that is parametrized by <span class="math">\(\theta\)</span>, and specify a prior probability distribution over <span class="math">\(\mathcal{X}_0\)</span> called <span class="math">\(p_0\)</span> such that the following model produces a sample <span class="math">\(x\sim p_\text{model}(x;\theta)\)</span>:
</p>
<div class="math">$$
\begin{align}
  z &amp;\sim p_0(z) \\
  x &amp;= f(z;\theta)
\end{align}
$$</div>
<p>We can compute the log likelihood of a sample <span class="math">\(x\)</span> under <span class="math">\(p_\text{model}(x;\theta)\)</span> by applying the change of variables formula:
</p>
<div class="math">$$
\begin{align}
  \log p_\text{model}(x;\theta) &amp;= \log p_0(f^{-1}(x;\theta)) + \log\left| \frac{\partial f^{-1}(x;\theta)}{\partial x} \right| \\
  &amp;= \log p_0(z) - \log \left| \frac{\partial f(x;\theta)}{\partial z} \right|^{-1} \text{ where }z=f^{-1}(x;\theta)
\end{align}
$$</div>
<p>The class of models that we just described is called <strong>normalizing flows</strong>.  Under the assumption that <span class="math">\(p_\text{data}(x) &gt; 0, \forall x\in \mathcal{X}\)</span>, there always exists an <span class="math">\(f\)</span> so that <span class="math">\(p_\text{model} = p_\text{data}\)</span>.  Typically normalizing flows are trained to minimize <span class="math">\(\text{KL}\left[p_\text{model}||p_\text{data}\right]\)</span>, leading us to one of an infinite number of solutions for <span class="math">\(f\)</span>.  The specific kinds of flows that we will look at are called <strong>continuous normalizing flows</strong>.</p>
<h1>Continuous normalizing flows</h1>
<p>Continuous normalizing flows are a type of normalizing flow where <span class="math">\(f\)</span> is a continuous function of <span class="math">\(t\in [0,1]\)</span> and <span class="math">\(f_1\)</span> is used in the computation of <span class="math">\(p_\text{model}(x;\theta)\)</span>.  In this setting, we do not parametrize <span class="math">\(f_t: \mathcal{X}_0 \to \mathcal{X}_t\)</span> directly, but we instead parametrize its infinitesmal generator,
</p>
<div class="math">$$
\begin{align}
u_t(x_t;\theta) = \frac{dx_t(x_0)}{dt}, \text{ where } x_t = f_t(x_0;\theta)
\end{align}
$$</div>
<p>In this parametrization, we can write <span class="math">\(f_t(x_0;\theta) = x_0 + \int_0^t u_s(x_s;\theta) ds\)</span> because
</p>
<div class="math">$$
\begin{align}
  f_t(x_0;\theta) &amp;= x_0 + \int_0^t u_s(x_s;\theta) ds \\
  &amp;= x_0 + \int_0^t \frac{dx_s(x_0)}{ds} ds \\
  &amp;= x_0 + \int_{x_0}^{x_t} dx_s \\
  &amp;= x_t
\end{align}
$$</div>
<p>Using the change of variables formula from before, we have that samples the model <span class="math">\(x_0\sim p_0(x_0)\)</span>, <span class="math">\(x_t = f_t(x_0;\theta)\)</span> have the log likelihood <span class="math">\(\log p_t(x_t)\)</span> where
</p>
<div class="math">$$
\begin{align}
\log p_t(x_t;\theta) = \log p_0(x_0) - \log \left| \frac{\partial f_t(x_0;\theta)}{\partial x_0} \right| \text{ where }x_0=f_t^{-1}(x_t;\theta)
\end{align}
$$</div>
<p>
and <span class="math">\(p_1(x_1;\theta) = p_\text{model}(x_1;\theta)\)</span>.  We can relate <span class="math">\(p_t(x_t;\theta)\)</span> with <span class="math">\(u_t(x_t;\theta)\)</span> using the instantaneous change of variables formula:
</p>
<div class="math">$$
\begin{align}
\frac{d\log p_t(x_t)}{dt} = -\text{Div}(u_t(x_t))
\end{align}
$$</div>
<p>Like <span class="math">\(f_t(x_0;\theta)\)</span>, we can compute the log likelihood of a sample by integrating the instantaneous change of variables formula:
</p>
<div class="math">$$
\begin{align}
  \log p_t(x_t;\theta) &amp;= \log p_0(x_0) - \log \left| \frac{\partial f_t(x_0;\theta)}{\partial x_0} \right| \\
  &amp;= \log p_0(x_0) - \int_0^1 \text{Tr}(u_t(x_t;\theta)) dt
\end{align}
$$</div>
<h1>Optimal continuous normalizing flow</h1>
<p>Next, we'll show how to construct <span class="math">\(u_t\)</span> so that <span class="math">\(\log p_1(x_1;\theta) = \log p_\text{data}(x_1)\)</span>.  Assume that <span class="math">\(p_t(x_t)\)</span> is a marginal probability distribution over <span class="math">\(x_t\)</span> and some random variable <span class="math">\(y\)</span> so that
</p>
<div class="math">$$
\begin{align}
  p_t(x_t) = \int p_y(y)p_{t|y}(x_t|y) dy
\end{align}
$$</div>
<p>
We need to choose <span class="math">\(p_y(y)\)</span> and <span class="math">\(p_{t|y}(x_t|y)\)</span> so that the conditions at <span class="math">\(t=0\)</span> and <span class="math">\(t=1\)</span> are satisfied: <span class="math">\(p_{t=0} = p_0\)</span> and <span class="math">\(p_{t=1} = p_\text{data}\)</span>.  Notice that <span class="math">\(p_{t|y}(x_t|y)\)</span> is time dependent, so it can also be written as the likelihood of <span class="math">\(x_t\)</span> under the flow of a conditional vector field, <span class="math">\(u_t(x_t|y)\)</span>.  It turns out that <span class="math">\(u_t(x)\)</span> can be written as the expected value of conditional vector fields over the posterior distribution of <span class="math">\(y\)</span> given <span class="math">\(x_t\)</span>:
</p>
<div class="math">$$
\begin{align}
u_t(x_t) = \mathbb{E}_{p_{t|y}(y|x_t)}[u_t(x_t|y)]
\end{align}
$$</div>
<p>
Thus, we have constructed a continuous normalizing flow that satisfies our conditions.  To summarize, we can build a continuous normalizing flow that generates samples on <span class="math">\(p_\text{data}\)</span> by:
1. Choose <span class="math">\(p_y(y)\)</span> and <span class="math">\(p_{t|y}(x_t|y)\)</span> so that <span class="math">\(p_{t=0} = p_0\)</span> and <span class="math">\(p_{t=1} = p_\text{data}\)</span>
2. Find a flow <span class="math">\(u_t(x_t|y)\)</span> for <span class="math">\(p_{t|y}(x_t|y)\)</span>
3. Compute <span class="math">\(u_t(x_t) = \mathbb{E}_{p_{t|y}(y|x_t)}[u_t(x_t|y)]\)</span></p>
<p>The point of introducing <span class="math">\(u_t(x_t|y)\)</span> is not to estimate <span class="math">\(u_t(x_t)\)</span>, but to estimate its <strong>expectation</strong> as follows:
</p>
<div class="math">$$
\begin{align}
\mathbb{E}_{p_t(x_t)}[u_t(x_t)] &amp;= \int p_t(x_t)u_t(x_t) dx_t \\
&amp;= \int p_t(x_t)\mathbb{E}_{p_{t|y}(y|x_t)}[u_t(x_t|y)] dx_t \\
&amp;= \int \int p_t(x_t)p_{t|y}(y|x_t)u_t(x_t|y) dx_t dy \\
&amp;= \int \int p_y(y)p_{t|y}(x_t|y)u_t(x_t|y) dx_t dy \\
&amp;= \mathbb{E}_{p_y(y)p_t(x_t|y)}\left[u_t(x_t|y)\right]
\end{align}
$$</div>
<p>Thus if we can sample from <span class="math">\(p_y(y)\)</span> and <span class="math">\(p_t(x_t|y)\)</span>, then we can compute an unbiased estimate of <span class="math">\(\mathbb{E}_{p_t(x_t)}[u_t(x_t)]\)</span>.  Before we see how this is useful for learning <span class="math">\(u_t(x_t)\)</span>, we'll show an example of how to choose <span class="math">\(p_y(y)\)</span> and <span class="math">\(p_{t|y}(x_t|y)\)</span>.</p>
<h1>Gaussian conditional flows</h1>
<p>Let <span class="math">\(p_y = p_\text{data}\)</span> and <span class="math">\(p_{t|y}(x_t|y) = \mathcal{N}(x_t; \mu_t(y), \Sigma_t(y))\)</span> and let <span class="math">\((\mu_0(y), \Sigma_0(y)) = (0, I)\)</span> and <span class="math">\((\mu_1(y), \Sigma_1(y)) = (y, 0)\)</span> so that <span class="math">\(p_0(x_0) = N(x_0|0,I)\)</span> and <span class="math">\(p_{1|y}(x_1|y) = \delta(x_1 - y)\)</span>.  We have that <span class="math">\(p_1(x_1) = p_\text{data}(x_1)\)</span> because
</p>
<div class="math">$$
\begin{align}
  p_1(x_1) &amp;= \int p_y(y)p_{t=1|y}(x_1|y) dy \\
  &amp;= \int p_\text{data}(y)\mathcal{N}(x_1; \mu_1(y), \Sigma_1(y)) dy \\
  &amp;= \mathcal{N}(x_1; \mu_1, \Sigma_1) \\
  &amp;= p_\text{data}(x_1)
\end{align}
$$</div>
<p>Notice that we can sample <span class="math">\(x_t \sim p_{t|y}(x_t|y)\)</span> using the model:
</p>
<div class="math">$$
\begin{align}
x_0 &amp;\sim N(0,I) \\
x_t &amp;= \mu_t(y) + \Sigma_t(y)^{1/2}x_0
\end{align}
$$</div>
<p>
So we can differentiate <span class="math">\(x_t\)</span> to get the conditional vector field:
</p>
<div class="math">$$
\begin{align}
u_t(x_t|y) &amp;= \frac{dx_t}{dt} \\
&amp;= \frac{d}{dt}\left[\mu_t(y) + \Sigma_t(y)^{1/2}x_0\right] \\
&amp;= \frac{d \mu_t(y)}{dt} + \frac{d \Sigma_t(y)^{1/2}}{dt}\underbrace{x_0}_{\Sigma_t^{\frac{-1}{2}}(x_t - \mu_t(y))} \\
&amp;= \frac{d \mu_t(y)}{dt} + \frac{d \Sigma_t(y)^{1/2}}{dt}\Sigma_t^{\frac{-1}{2}}(x_t - \mu_t(y))
\end{align}
$$</div>
<h1>Flow matching</h1>
<p>Now that we have a way to construct a target continuous normalizing flow whose generating vector field is <span class="math">\(u_t(x_t)\)</span>, we can train a model to match <span class="math">\(u_t(x_t)\)</span>.  Let <span class="math">\(v_t(x_t;\theta)\)</span> be a parametric neural network whose flow corresponds to the probability density function <span class="math">\(q_t(x_t)\)</span>.  We can train <span class="math">\(v_t(x_t;\theta)\)</span> to match <span class="math">\(u_t(x_t)\)</span> by minimizing the following loss function:
</p>
<div class="math">$$
\begin{align}
\mathcal{L}_{\text{FM}}(\theta) &amp;= \int_0^1 \mathbb{E}_{p_t(x_t)}\left[\left\|u_t(x_t) - v_t(x_t;\theta)\right\|^2\right] dt \\
&amp;= \small{\underbrace{\int_0^1 \mathbb{E}_{p_t(x_t)}[\|u_t(x_t)\|^2]dt}_{C_1} - 2\int_0^1 \mathbb{E}_{p_t(x_t)}[u_t(x_t)^Tv_t(x_t;\theta)]dt + \int_0^1 \mathbb{E}_{p_t(x_t)}[\|v_t(x_t;\theta)\|^2]dt} \\
&amp;= C_1 - 2\int_0^1 \mathbb{E}_{p_t(x_t)}\left[u_t(x_t)\right]^Tv_t(x_t;\theta)dt + \int_0^1 \mathbb{E}_{p_t(x_t)}\left[\|v_t(x_t;\theta)\|^2\right]dt \\
&amp;= C_1 - 2\int_0^1 \mathbb{E}_{p_y(y)p_t(x_t|y)}\left[u_t(x_t|y)\right]^Tv_t(x_t;\theta)dt + \int_0^1 \mathbb{E}_{p_t(x_t)}\left[\|v_t(x_t;\theta)\|^2\right]dt \\
&amp;= C_1 - C_2 + \underbrace{\int_0^1 \mathbb{E}_{p_y(y)p_t(x_t|y)}\left[\|u_t(x_t|y) - v_t(x_t;\theta)\|^2\right]dt}_{\mathcal{L}_{\text{CFM}}(\theta)}
\end{align}
$$</div>
<p>
where <span class="math">\(C_2 = \int_0^1 \mathbb{E}_{p_y(y)p_t(x_t|y)}\left[\|u_t(x_t|y)\|^2\right]dt\)</span>.  So we can minimize <span class="math">\(\mathcal{L}_{\text{FM}}(\theta)\)</span> by minimizing <span class="math">\(\mathcal{L}_{\text{CFM}}(\theta)\)</span> because <span class="math">\(C_1\)</span> and <span class="math">\(C_2\)</span> are constants.  <span class="math">\(\mathcal{L}_{\text{CFM}}(\theta)\)</span> is called the <strong>conditional flow matching</strong> loss.</p>
<p>We can also reparametrize <span class="math">\(x_t\)</span> with <span class="math">\(f_t(x_t|y)\)</span> to get the following loss function:
</p>
<div class="math">$$
\begin{align}
\mathcal{L}_{\text{CFM}}(\theta) &amp;= \int_0^1 \mathbb{E}_{p_y(y)p_t(x_t|y)}\left[\|u_t(x_t|y) - v_t(x_t;\theta)\|^2\right]dt \\
&amp;= \int_0^1 \mathbb{E}_{p_y(y)p_0(x_0)}\left[\left\|u_t(f_t(x_0)|y) - v_t(x_t;\theta)\right\|^2\right]dt
\end{align}
$$</div>
<p>In this form, we see a new recipe for what we need to do to train <span class="math">\(v_t(x_t;\theta)\)</span>:
1. Sample <span class="math">\(y\sim p_y(y)\)</span> and <span class="math">\(x_0\sim p_0(x_0)\)</span>
2. Construct some path that starts at <span class="math">\(x_0\)</span> and ends at <span class="math">\(y\)</span>
3. Sample any point on this path (to get <span class="math">\(f_t(x_0)\)</span>) and get the tangent vector at that point (to get <span class="math">\(u_t(f_t(x_0)|y)\)</span>)
4. Compute the loss between <span class="math">\(u_t(f_t(x_0)|y)\)</span> and <span class="math">\(v_t(x_t;\theta)\)</span></p>
<p>As we saw in the Gaussian flow matching section, we will often choose <span class="math">\(p_y(y) = p_\text{data}(y)\)</span> and <span class="math">\(p_0(x_0) = \mathcal{N}(x_0; 0, I)\)</span>, so we are free to choose any path that starts at <span class="math">\(x_0\)</span> and ends at <span class="math">\(y\)</span>.  The simplest choice is a straight line between <span class="math">\(x_0\)</span> and <span class="math">\(y\)</span>, but we can also choose a more complicated path.</p>
<h1>Riemannian flow matching</h1>
<p>Riemannian flow matching chooses the path between <span class="math">\(x_0\)</span> and <span class="math">\(y\)</span> using a gradient flow.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
    </div><!-- /.entry-content -->

  </article>
</section>
        <section id="extras" class="body">
                <div class="blogroll">
                        <h2>links</h2>
                        <ul>
                            <li><a href="https://getpelican.com/">Pelican</a></li>
                            <li><a href="https://www.python.org/">Python.org</a></li>
                            <li><a href="https://palletsprojects.com/p/jinja/">Jinja2</a></li>
                            <li><a href="#">You can modify those links in your config file</a></li>
                        </ul>
                </div><!-- /.blogroll -->
                <div class="social">
                        <h2>social</h2>
                        <ul>

                            <li><a href="#">You can add links in your config file</a></li>
                            <li><a href="#">Another social link</a></li>
                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="https://getpelican.com/">Pelican</a>, which takes great advantage of <a href="https://www.python.org/">Python</a>.
                </address><!-- /#about -->

                <p>The theme is by <a href="https://www.smashingmagazine.com/2009/08/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
        </footer><!-- /#contentinfo -->

</body>
</html>