<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="author" content="Eddie Cunningham" />

        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="probability paths, KL-divergence, score matching, flow matching, Blog, " />

<meta property="og:title" content="KL-Divergence, Score Matching, and Flow Matching "/>
<meta property="og:url" content="/kl-div-and-matching.html" />
<meta property="og:description" content="The relationship between KL-divergence, score matching, and flow matching" />
<meta property="og:site_name" content="The Weeds" />
<meta property="og:article:author" content="Eddie Cunningham" />
<meta property="og:article:published_time" content="2023-07-15T00:00:00-05:00" />
<meta property="og:article:modified_time" content="2023-07-15T00:00:00-05:00" />
<meta name="twitter:title" content="KL-Divergence, Score Matching, and Flow Matching ">
<meta name="twitter:description" content="The relationship between KL-divergence, score matching, and flow matching">

        <title>KL-Divergence, Score Matching, and Flow Matching  Â· The Weeds
</title>
        <link href="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/css/bootstrap-combined.min.css" rel="stylesheet">
        <link rel="stylesheet" type="text/css" href="/theme/css/elegant.prod.css" media="screen">
        <link rel="stylesheet" type="text/css" href="/theme/css/custom.css" media="screen">



    </head>
    <body>
        <div id="content">
            <div class="navbar navbar-static-top">
                <div class="navbar-inner">
                    <div class="container-fluid">
                        <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                        </a>
                        <a class="brand" href="/"><span class=site-name>The Weeds</span></a>
                        <div class="nav-collapse collapse">
                            <ul class="nav pull-right top-menu">
                                <li >
                                    <a href=
                                       "/"
                                    >Home</a>
                                </li>
                                <li ><a href="/pages/about.html">About</a></li>
                                <li ><a href="/categories.html">Categories</a></li>
                                <li ><a href="/tags.html">Tags</a></li>
                                <li ><a href="/archives.html">Archives</a></li>
                                <li><form class="navbar-search" action="/search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
            <div class="container-fluid">
                <div class="row-fluid">
                    <div class="span1"></div>
                    <div class="span10">
<article itemscope>
<div class="row-fluid">
    <header class="page-header span10 offset2">
        <h1>
            <a href="/kl-div-and-matching.html">
                KL-Divergence, Score Matching, and Flow Matching
            </a>
        </h1>
    </header>
</div>

<div class="row-fluid">
        <div class="span8 offset2 article-content">
            
            <h1>Overview</h1>
<p>KL-divergence minimization, score matching and flow matching are all different ways to train generative models.  KL-divergence minimization is typically used to train normalizing flows, score matching for diffusion models and flow matching for continuous normalizing flows.  In this post, we'll go over how these three methods are related and when to use each one.</p>
<h1>KL-divergence</h1>
<p>Let <span class="math">\(p\)</span> and <span class="math">\(q\)</span> be two probability density functions defined on a Riemannian manifold <span class="math">\((\mathcal{M}, g)\)</span> and let <span class="math">\(dV_g\)</span> be the volume form on <span class="math">\(\mathcal{M}\)</span>.  The KL-divergence between <span class="math">\(p\)</span> and <span class="math">\(q\)</span> is defined as
</p>
<div class="math">$$
\begin{align}
\text{KL}[p\|q] &amp;= \int_{\mathcal{M}}p \log \frac{p}{q} dV_g
\end{align}
$$</div>
<p>
It has the nice property that <span class="math">\(KL[p\|q] \geq 0\)</span> and equals <span class="math">\(0\)</span> if and only if <span class="math">\(p=q\)</span>.  Because of this, the KL divergence can be used to minimize the difference between two distributions.</p>
<p>In the context of generative models, we can use the KL divergence to minimize the difference between the distribution of the data and the distribution of our model.  Let <span class="math">\(p\)</span> be the distribution of the data and <span class="math">\(q(\theta)\)</span> be the distribution of the model with parameters <span class="math">\(\theta\)</span>.  Then
</p>
<div class="math">$$
\begin{align}
  \nabla_{\theta}\text{KL}[p\|q(\theta)] &amp;= \nabla_{\theta}\int_{\mathcal{M}}p \log \frac{p}{q(\theta)} dV_g \\
  &amp;= \underbrace{\nabla_{\theta}\int_{\mathcal{M}}p \log p dV_g}_{0} - \nabla_{\theta}\int_{\mathcal{M}}p \log q(\theta) dV_g \\
  &amp;= -\int_{\mathcal{M}}p \nabla_{\theta}\log q(\theta) dV_g \\
  &amp;= -\nabla_{\theta}\mathbb{E}_p\left[\log q(\theta)\right]
\end{align}
$$</div>
<p>
Because the gradients of the KL divergence are equal to the gradients of the expectation of the log likelihood of the data under the model, we can train our model by maximizing the likelihood of our model under the data.  This is the standard way to train a normalizing flow where we can compute the log likelihood under the model exactly.</p>
<h1>Matching</h1>
<p>A more modern approach to training generative models is to use either score matching or flow matching.  These methods are based on the idea that we can construct a <strong>probability path</strong> for the target probability distribution and our model.  A probability path is a time dependent probability distribution that is characterized by how its samples change over time using a time dependent function called a flow.</p>
<h3>Probability paths</h3>
<p>Let <span class="math">\(p_t\)</span> is a time dependent probability density function defined on a Riemannian manifold <span class="math">\((\mathcal{M},g)\)</span>.  Let <span class="math">\(x_0\sim p_{t=0}\)</span> be a sample at time <span class="math">\(t=0\)</span>.  A flow associated with the probability path is a time dependent diffeomorphism <span class="math">\(f_t\)</span> such that <span class="math">\(f_t(x_0)\)</span> is distributed according to <span class="math">\(p_t\)</span>, meaning <span class="math">\(f_t(x_0) \sim p_t\)</span>.  It is often more convenient to work with the time derivative of the flow, which is a vector field <span class="math">\(V_t \in \mathfrak{X}(\mathcal{M})\)</span> so that <span class="math">\(\frac{df_t}{dt} = V_t\)</span>.  If we evaluate the flow at a point <span class="math">\(x_0\)</span> and write <span class="math">\(x_t = f_t(x_0)\)</span>, then we can write the equation in a more suggestive way
</p>
<div class="math">$$
\begin{align}
  \frac{dx_t}{dt} &amp;= V_t(x_t),\text{ where }x_t := f_t(x_0)
\end{align}
$$</div>
<p>
The probability path and vector field are related via the continuity equation (see my <a href="continuous_normalizing_flows.md">continuous normalizing flows post</a> to see where it comes from):
</p>
<div class="math">$$
\begin{align}
  \frac{\partial p_t}{\partial t} &amp;= -\text{Div}(p_tV_t)
\end{align}
$$</div>
<p>
where <span class="math">\(\text{Div}\)</span> is the divergence operator on <span class="math">\(\mathcal{M}\)</span>.  A useful property that we'll use later is that for any function <span class="math">\(f\in C^\infty(\mathcal{M})\)</span>,
</p>
<div class="math">$$
\begin{align}
  \text{Div}(fV) &amp;= f\text{Div}(V) + \langle \nabla f, V \rangle_g
\end{align}
$$</div>
<p>But where do probability paths come from if we are trying to learn how to model some fixed probabiltiy distribution?  It turns out that as long as we can sample from our target distribution <span class="math">\(p_\text{data}\)</span>, which is the case when we are trying to learn the unknown data generating probability distribution, then we can always construct a probability path from any starting distribution <span class="math">\(p_{t=0}\)</span> to the target at <span class="math">\(t=1\)</span>.  See my <a href="flow_matching.md">flow matching post</a> for more details.</p>
<h2>Score matching</h2>
<p>Now that we know what probability paths are, lets introduce score matching.  The score of a probability density function, in the context of generative modeling, is the gradient of the log likelihood function <span class="math">\(\nabla_x \log p_t(x)\)</span>.  Score matching aims to minimize the norm between the score of the target distribution and the score of the model distribution at all times:
</p>
<div class="math">$$
\begin{align}
  \mathcal{L}_{\text{SM}}(\theta) &amp;= \int_{t_0}^{t_1} \int_{\mathcal{M}}p_t \left\|\nabla_x \log p_t - \nabla_x \log q_t(\theta)\right\|_g^2 dV_g dt
\end{align}
$$</div>
<p>
where <span class="math">\(q(x;\theta)\)</span> is the model distribution with parameters <span class="math">\(\theta\)</span>.  Score matching is typically used to train <a href="https://arxiv.org/pdf/2011.13456.pdf">diffusion models</a>, but can suffer when <span class="math">\(p_t(x)\)</span> approaches 0 because the gradient of the log likelihood will approach infinity.</p>
<h2>Flow matching</h2>
<p>Flow matching is similar to score matching, but instead of matching the score of the target distribution, it matches the vector field that generates the flow for each distribution.  Let <span class="math">\(V_t\)</span> be the vector field that generates the flow for <span class="math">\(p_t\)</span> and <span class="math">\(W_t\)</span> be the vector field that generates the flow for <span class="math">\(q_t(\theta)\)</span>.  Then flow matching aims to minimize the norm between the vector fields of the target and model distributions at all times:
</p>
<div class="math">$$
\begin{align}
  \mathcal{L}_{\text{FM}}(\theta) &amp;= \int_{t_0}^{t_1} \int_{\mathcal{M}}p_t \left\|V_t - W_t\right\|_g^2 dV_g dt
\end{align}
$$</div>
<p>
Flow matching is used to train <a href="https://arxiv.org/pdf/2002.02428.pdf">continuous normalizing flows</a> and has been shown to be a more stable alternative to score matching.</p>
<h1>Entropy</h1>
<p>We can see how a generating vector field and score function interact through the time derivative of the entropy of a probability distribution.  If <span class="math">\((p_t,V_t)\)</span> is a probability path, then
</p>
<div class="math">$$
\begin{align}
  \frac{d}{dt}H(p_t) &amp;= -\frac{d}{dt}\int_{\mathcal{M}}p_t \log p_t dV_g \\
  &amp;= -\int_{\mathcal{M}}\frac{dp_t}{dt} \log p_t dV_g - \underbrace{\int_{\mathcal{M}}\frac{dp_t}{dt}dV_g}_{0} \\
  &amp;= \int_{\mathcal{M}}\text{Div}(p_tV_t) \log p_t dV_g \\
  &amp;= -\int_{\mathcal{M}}\langle p_tV_t, \nabla\log p_t \rangle_g dV_g \\
  &amp;= -\int_{\mathcal{M}}p_t\langle V_t, \nabla\log p_t \rangle_g dV_g \\
\end{align}
$$</div>
<p>
So the inner product between the score function and the vector field tells us how the entropy changes.  We can see that to maximize this quantity, we need <span class="math">\(V_t\)</span> to be in the same opposite direction as <span class="math">\(\nabla \log p_t\)</span>.  This makes sense as this corresponds to flowing outwards from the peak of the distribution.</p>
<h1>KL-Divergence</h1>
<p>Now that we've introduced KL-divergence, score matching, and flow matching, we can see how they are related.  Say that we have a probability path for our target distribution <span class="math">\(p_t\)</span> and a path for our model distribution <span class="math">\(q_t\)</span> and assume that <span class="math">\(p_0 = q_0\)</span> (we construct the probability paths like this in flow matching).  We can express the KL divergence between the final two distributions, <span class="math">\(p_{t=1}\)</span> and <span class="math">\(q_{t=1}\)</span>, as
</p>
<div class="math">$$
\begin{align}
  \text{KL}\left[p_1\|q_1\right] &amp;= \underbrace{\text{KL}\left[p_0\|q_0\right]}_{0} + \int_{t=0}^1 \frac{d}{dt}\text{KL}\left[p_t\|q_t\right] dt \\
  &amp;= \int_{t=0}^1 \frac{d}{dt}\text{KL}\left[p_t\|q_t\right] dt
\end{align}
$$</div>
<p>
Before expanding the time derivative of the KL-divergence, lets go over the identities that we need to know.  Recall that we are assuming that <span class="math">\(\mathcal{M}\)</span> is a Riemannian manifold with metric <span class="math">\(g\)</span> and no boundary.  The identities we're using are:
1. Continuity equation: <span class="math">\(\frac{\partial p_t}{\partial t} = -\text{Div}(p_tV_t)\)</span>
2. Integration by parts for boundryless manifolds: <span class="math">\(-\int_{\mathcal{M}}f\text{Div}(V)dV_g = \int_\mathcal{M}\langle \nabla f, V\rangle_g dV_g\)</span>
See my post on <a href="continuous_normalizing_flows.md">continuous normalizing flows</a> for a derivation of the continuity equation and <a href="https://math.berkeley.edu/~jchaidez/materials/reu/lee_smooth_manifolds.pdf">problem 16-12 of Lee</a> for integration by parts.</p>
<p>Next, lets expand the time derivative of KL-divergence:
</p>
<div class="math">$$
\begin{align}
  \frac{d}{dt}\text{KL}\left[p_t\|q_t\right] &amp;= \frac{d}{dt}\int_{\mathcal{M}}p_t \log \frac{p_t}{q_t} dV_g \\
  &amp;= \int_{\mathcal{M}}\frac{d p_t}{dt} \log \frac{p_t}{q_t} dV_g + \int_{\mathcal{M}}p_t \frac{d}{dt}\log \frac{p_t}{q_t} dV_g \\
  &amp;= -\int_{\mathcal{M}}\text{Div}(p_tV_t) \log \frac{p_t}{q_t} dV_g + \underbrace{\int_{\mathcal{M}}p_t \frac{d}{dt}\log p_t dV_g}_{0} - \int_{\mathcal{M}}\frac{p_t}{q_t} \frac{dq_t}{dt} dV_g \\
  &amp;= \int_{\mathcal{M}}\langle p_t V_t, \nabla \log \frac{p_t}{q_t} \rangle_g dV_g + \int_{\mathcal{M}}\frac{p_t}{q_t} \text{Div}(q_tW_t) dV_g \\
  &amp;= \int_{\mathcal{M}}p_t\langle V_t, \nabla \log \frac{p_t}{q_t} \rangle_g dV_g - \int_{\mathcal{M}}\langle \nabla \frac{p_t}{q_t}, q_tW_t\rangle_g  dV_g \\
  &amp;= \int_{\mathcal{M}}p_t\langle V_t, \nabla \log \frac{p_t}{q_t} \rangle_g dV_g - \int_{\mathcal{M}}p_t \langle \nabla \log \frac{p_t}{q_t}, W_t\rangle_g  dV_g \\
  &amp;= \int_{\mathcal{M}}p_t\langle V_t - W_t, \nabla \log p_t - \nabla \log q_t \rangle_g dV_g
\end{align}
$$</div>
<p>
Notice that we have an equation that combines both the scores and vector fields.  We can write the score matching and flow matching objectives more suggestively by writing the <span class="math">\(L2\)</span> norm as an inner product:</p>
<div class="math">$$
\begin{align}
  \text{KL}\left[p_1\|q_1\right] &amp;= \int \int_{\mathcal{M}}p_t\langle V_t - W_t, \nabla \log p_t - \nabla \log q_t \rangle_g dV_g dt \\
  \mathcal{L}_{\text{SM}} &amp;= \int_{t_0}^{t_1} \int_{\mathcal{M}}p_t \langle \nabla_x \log p_t - \nabla_x \log q_t, \nabla_x \log p_t - \nabla_x \log q_t \rangle_g dV_g dt \\
  \mathcal{L}_{\text{FM}} &amp;= \int_{t_0}^{t_1} \int_{\mathcal{M}}p_t \langle V_t - W_t, V_t - W_t \rangle_g dV_g dt
\end{align}
$$</div>
<p>
KL divergence, score matching and flow matching all take an expectation with respect to <span class="math">\(p_t\)</span> but differ in what they take the inner product with.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>


             
 
            
            
            






            <hr/>
        </div>
        <section id="article-sidebar" class="span2">
            <h4>Published</h4>
            <time itemprop="dateCreated" datetime="2023-07-15T00:00:00-05:00">Jul 15, 2023</time>

            <h4>Category</h4>
            <a class="category-link" href="/categories.html#blog-ref">Blog</a>
            <h4>Tags</h4>
            <ul class="list-of-tags tags-in-article">
                <li><a href="/tags.html#flow-matching-ref">flow matching
                    <span>1</span>
</a></li>
                <li><a href="/tags.html#kl-divergence-ref">KL-divergence
                    <span>1</span>
</a></li>
                <li><a href="/tags.html#probability-paths-ref">probability paths
                    <span>1</span>
</a></li>
                <li><a href="/tags.html#score-matching-ref">score matching
                    <span>1</span>
</a></li>
            </ul>
            





            





        </section>
</div>
</article>
                    </div>
                    <div class="span1"></div>
                </div>
            </div>
        </div>
<footer>




    <div id="fpowered">
        Powered by: <a href="http://getpelican.com/" title="Pelican Home Page" target="_blank" rel="nofollow noopener noreferrer">Pelican</a>
        Theme: <a href="https://elegant.oncrashreboot.com/" title="Theme Elegant Home Page" target="_blank" rel="nofollow noopener noreferrer">Elegant</a>
    </div>
</footer>            <script src="//code.jquery.com/jquery.min.js"></script>
        <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/js/bootstrap.min.js"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>

    <script>
    (function () {
        if (window.location.hash.match(/^#comment-\d+$/)) {
            $('#comment_thread').collapse('show');
        }
    })();
    window.onhashchange=function(){
        if (window.location.hash.match(/^#comment-\d+$/))
            window.location.reload(true);
    }
    $('#comment_thread').on('shown', function () {
        var link = document.getElementById('comment-accordion-toggle');
        var old_innerHTML = link.innerHTML;
        $(link).fadeOut(200, function() {
            $(this).text('Click here to hide comments').fadeIn(200);
        });
        $('#comment_thread').on('hidden', function () {
            $(link).fadeOut(200, function() {
                $(this).text(old_innerHTML).fadeIn(200);
            });
        })
    })
</script>

    </body>
    <!-- Theme: Elegant built for Pelican
        License : MIT -->
</html>