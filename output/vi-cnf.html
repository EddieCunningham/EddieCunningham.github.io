<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="author" content="Eddie Cunningham" />

        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content=", Blog, " />

<meta property="og:title" content="VI with CNFs "/>
<meta property="og:url" content="/vi-cnf.html" />
<meta property="og:description" content="How to do variational inference with continuous normalizing flows" />
<meta property="og:site_name" content="" />
<meta property="og:article:author" content="Eddie Cunningham" />
<meta property="og:article:published_time" content="2023-11-17T00:00:00-05:00" />
<meta property="og:article:modified_time" content="2023-11-17T00:00:00-05:00" />
<meta name="twitter:title" content="VI with CNFs ">
<meta name="twitter:description" content="How to do variational inference with continuous normalizing flows">

        <title>VI with CNFs  Â· 
</title>
        <link href="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/css/bootstrap-combined.min.css" rel="stylesheet">
        <link rel="stylesheet" type="text/css" href="/theme/css/elegant.prod.css" media="screen">
        <link rel="stylesheet" type="text/css" href="/theme/css/custom.css" media="screen">



    </head>
    <body>
        <div id="content">
            <div class="navbar navbar-static-top">
                <div class="navbar-inner">
                    <div class="container-fluid">
                        <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                        </a>
                        <a class="brand" href="/"><span class=site-name></span></a>
                        <div class="nav-collapse collapse">
                            <ul class="nav pull-right top-menu">
                                <li >
                                    <a href=
                                       "/"
                                    >Home</a>
                                </li>
                                <li ><a href="/pages/about.html">About</a></li>
                                <li ><a href="/pages/research.html">Research</a></li>
                                <li ><a href="/categories.html">Categories</a></li>
                                <li ><a href="/tags.html">Tags</a></li>
                                <li ><a href="/archives.html">Archives</a></li>
                                <li><form class="navbar-search" action="/search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
            <div class="container-fluid">
                <div class="row-fluid">
                    <div class="span1"></div>
                    <div class="span10">
<article itemscope>
<div class="row-fluid">
    <header class="page-header span10 offset2">
        <h1>
            <a href="/vi-cnf.html">
                VI with CNFs
            </a>
        </h1>
    </header>
</div>

<div class="row-fluid">
        <div class="span8 offset2 article-content">
            
            <h1>Motivation</h1>
<p>Suppose we want to be able to sample from some probability density function, <span class="math">\(p\)</span>, that we can evaluate.  We can do this by using a continuous normalizing flow (CNF) to transform a simple distribution, <span class="math">\(q_0\)</span>, into <span class="math">\(p\)</span>.  CNFs do this by flowing samples from <span class="math">\(q_0\)</span> along a vector field <span class="math">\(V_t\)</span> that is parameterized by a neural network to time <span class="math">\(t=1\)</span>.  The probability density function of the samples at time <span class="math">\(t\)</span> is denoted by <span class="math">\(q_t\)</span> and if the model is trained correctly then <span class="math">\(q_1 = p\)</span>.</p>
<p>The current way of training CNFs is to minimize <span class="math">\(KL[q_1||p]\)</span> by maximizing the ELBO.  However, this can be expensive and lead to vector fields that are difficult to integrate numerically.  We will try to remedy this issue by first constructing a target probability path that starts at <span class="math">\(q_0\)</span> and ends at <span class="math">\(p\)</span> as:
</p>
<div class="math">$$
\begin{align}
  p_\beta &amp;= \frac{1}{Z_\beta}q_0(x)^{1-\beta}p(x)^\beta
\end{align}
$$</div>
<p>
Then we will try to learn a CNF by minimizing the objective
</p>
<div class="math">$$
\begin{align}
  \mathcal{L} = \int_{0}^1 \lambda(\beta)\text{KL}\left[q_{\beta}||p_{\beta}\right] d\beta
\end{align}
$$</div>
<p>
where <span class="math">\(\lambda(\beta)\)</span> is a weighting function that we will choose later.  By doing so, the CNF will learn to transport samples from a fixed target distribution to the target distribution.  This should help the flow learn multimodal distributions better because the modes of the distribution are slowly formed as <span class="math">\(\beta\)</span> increases.</p>
<h1>Objective derivation</h1>
<p>Let's start by deriving the KL term.  We'll take an approach that lets us completely bypass the normalization constant.</p>
<div class="math">$$
\begin{align}
  \text{KL}\left[q_{\beta}||p_{\beta}\right] &amp;= \text{KL}\left[q_0||p_{\beta}\right] + \int_{0}^\beta \frac{d}{dt}\text{KL}\left[q_t||p_\beta \right]dt \\
\end{align}
$$</div>
<p>
Next,
</p>
<div class="math">$$
\begin{align}
  \frac{d}{dt}\text{KL}\left[q_t||p_\beta\right] &amp;= \int \frac{d}{dt}q_t\log \frac{q_t}{p_\beta}dV \\
  &amp;= \int \left((\frac{d q_t}{dt})\log \frac{q_t}{p_\beta} + q_t\frac{d \log q_t}{dt}\right)dV \\
  &amp;= \int \left(-\text{Div}(q_t V_t) \log \frac{q_t}{p_\beta} + \underbrace{\frac{d q}{dt}}_{\text{$0$ in expectation}}\right)dV \\
  &amp;= \int \langle \nabla \log \frac{q_t}{p_\beta} , q_t V_t\rangle dz \\
  &amp;= \int q_t\left( \langle \nabla \log q_t , V_t\rangle  - \langle \nabla \log p_\beta, V_t\rangle  \right)dV \\
  &amp;= \int \langle \nabla q_t , V_t\rangle  dV - \int q_t\langle \nabla \log p_\beta, V_t\rangle  dV \\
  &amp;= -\int q_t \text{Div}(V_t) dV - \int q_t\langle \nabla \log p_\beta, V_t\rangle  dV \\
  &amp;= -\mathbb{E}_{q_t}\left[\langle \nabla \log p_\beta, V_t \rangle  + \text{Div}(V_t)\right]
\end{align}
$$</div>
<p>This means that we can get an unbiased estimate of the KL term without having to compute the normalization constant.  Instead, we only need access to the score function:
</p>
<div class="math">$$
\begin{align}
  \nabla \log p_\beta &amp;= (1-\beta)\nabla \log q_0 + \beta\nabla \log p
\end{align}
$$</div>
<p>Putting everything together yields the objective:
</p>
<div class="math">$$
\begin{align}
  \mathcal{L} &amp;= \int_{0}^1 \text{KL}\left[q_{\beta}||p_{\beta}\right] d\beta \\
  &amp;= \underbrace{\int_{0}^1 \text{KL}\left[q_0||p_{\beta}\right] d\beta}_{\text{C}} + \int_{0}^1 \int_{0}^\beta \frac{d}{dt}\text{KL}\left[q_t||p_\beta \right]dt d\beta \\
  &amp;= \text{C} - \int_{0}^1 \int_{0}^\beta \mathbb{E}_{q_t}\left[\langle \nabla \log p_\beta, V_t \rangle  + \text{Div}(V_t)\right]dt d\beta \\
  &amp;= \text{C} - \mathbb{E}_{q_0}\left[\int_{0}^1 \int_{0}^\beta \langle \nabla \log p_\beta, V_t \rangle  + \text{Div}(V_t)dt d\beta\right] \\
  &amp;= \text{C} - \mathbb{E}_{q_0}[\int_{0}^1 \int_{0}^\beta (1-\beta)\langle \nabla \log q_0, V_t \rangle + \beta \langle \nabla \log p , V_t \rangle + \text{Div}(V_t)dt d\beta]
\end{align}
$$</div>
<h2>Integrating out <span class="math">\(\beta\)</span></h2>
<p>Next, we'll integrate out <span class="math">\(\beta\)</span> by using the identity:
</p>
<div class="math">$$
\begin{align}
  \int_0^1 \int_0^\beta h(\beta)f(t)dt d\beta &amp;= \int_0^1 \left[H(1) - H(t)\right]f(t) dt
\end{align}
$$</div>
<p>
where <span class="math">\(\frac{dH(\beta)}{d\beta} = h(\beta)\)</span>.</p>
<p>Proof:
We can change the region of integration by letting <span class="math">\(s=1-t\)</span> and <span class="math">\(\gamma = 1-\beta\)</span> and then we have:
</p>
<div class="math">$$
\begin{align}
  \int_{\beta=0}^1 \int_{t=0}^\beta h(\beta)f(t)dt d\beta &amp;= \int_{s=0}^1 \int_{\gamma=s}^0 h(1-\gamma)f(1-s)d\gamma ds \\
  &amp;= \int_{s=0}^1 f(1-s) \int_{\gamma=s}^0 h(1-\gamma)d\gamma ds \\
  &amp;= \int_{s=0}^1 f(1-s) \left[H(1-s) - H(1)\right] ds \\
  &amp;= \int_{t=0}^1 f(t) \left[H(1) - H(t)\right] ds
\end{align}
$$</div>
<h3>Choosing <span class="math">\(\lambda(\beta)\)</span></h3>
<p><span class="math">\(\lambda(\beta)\)</span> will be absorbed into <span class="math">\(h(\beta)\)</span> above.  In particular, <span class="math">\(h(\beta)\)</span> will take the form of either:
</p>
<div class="math">$$
\begin{align}
  h_1(\beta) &amp;= \lambda(\beta) \\
  h_2(\beta) &amp;= \beta \lambda(\beta)
\end{align}
$$</div>
<p>
Therefore, we will need to be able to evaluate the anti-derivative of <span class="math">\(\lambda(\beta)\)</span> and <span class="math">\(\beta \lambda(\beta)\)</span>.  The anti-derivative of <span class="math">\(x\lambda(\beta)\)</span> can be evaluated using integration by parts:
</p>
<div class="math">$$
\begin{align}
  \int_a^b \beta\lambda(\beta) d\beta &amp;= (\beta \Lambda(\beta))\big|_a^b - \int_a^b \Lambda(\beta) d\beta
\end{align}
$$</div>
<p>
where <span class="math">\(\Lambda(\beta)\)</span> is the anti-derivative of <span class="math">\(\lambda(\beta)\)</span>.  Therefore, we need to be able to evaluate 2 anti-derivatives of <span class="math">\(\lambda(\beta)\)</span> in order to have a closed form expression for the objective.</p>
<p>The trivial choice of letting <span class="math">\(\lambda(\beta) = 1\)</span> gives us the expreessions:
</p>
<div class="math">$$
\begin{align}
  \int_{0}^1 \int_{0}^\beta C(t) dt d\beta &amp;= \int_0^1 (1-t)C(t)dt \\
  \int_{0}^1 \int_{0}^\beta \beta B(t) dt d\beta &amp;= \int_0^1 \frac{1}{2}(1-t^2)B(t)dt \\
  \int_{0}^1 \int_{0}^\beta (1-\beta)A(t) dt d\beta &amp;= \int_0^1 \frac{1}{2}(1-t)^2A(t)dt
\end{align}
$$</div>
<p>
With the choice of <span class="math">\(\lambda(\beta) = 1\)</span>, the objective becomes:
</p>
<div class="math">$$
\begin{align}
  \mathcal{L} &amp;= \text{C} - \mathbb{E}_{q_0}[\int_{0}^1 \frac{1}{2}(1-t)^2\langle \nabla \log q_0, V_t \rangle + \frac{1}{2}(1-t^2) \langle \nabla \log p , V_t \rangle + (1-t)\text{Div}(V_t)dt]
\end{align}
$$</div>
<h3>Polynomial <span class="math">\(\lambda(\beta)\)</span></h3>
<p>We can also choose <span class="math">\(\lambda(\beta)\)</span> to be a polynomial of degree <span class="math">\(n\)</span>:
</p>
<div class="math">$$
\begin{align}
  \lambda(\beta) &amp;= \sum_{i=0}^n a_i \beta^i
\end{align}
$$</div>
<p>
The anti-derivatives are then:
</p>
<div class="math">$$
\begin{align}
  H_1(\beta) = \int \lambda(\beta) d\beta &amp;= \sum_{i=0}^n \frac{a_i}{i+1}\beta^{i+1} \\
  H_2(\beta) = \int \beta \lambda(\beta) d\beta &amp;= \sum_{i=0}^n \frac{a_i}{i+2}\beta^{i+2} \\
  H_3(\beta) = \int (1 - \beta) \lambda(\beta) d\beta &amp;= \sum_{i=0}^n a_i\beta^{i+1}(\frac{1}{i+1} - \frac{1}{i+2}\beta)
\end{align}
$$</div>
<p>
And plugging this into <span class="math">\(H(1) - H(t)\)</span> gives us:
</p>
<div class="math">$$
\begin{align}
  H_1(1) - H_1(t) &amp;= \sum_{i=0}^n \frac{a_i}{i+1}(1 - t^{i+1}) \\
  H_2(1) - H_2(t) &amp;= \sum_{i=0}^n \frac{a_i}{i+2}(1-t^{i+2}) \\
  H_3(1) - H_3(t) &amp;= \sum_{i=0}^n \frac{a_i}{i+1}(1 - t^{i+1})(1 - \frac{i+1}{i+2}t)
\end{align}
$$</div>
<h1>Comparison to ELBO maximization</h1>
<p>Lets see how this compares to maximizing the ELBO
</p>
<div class="math">$$
\begin{align}
  -\log p(x) &amp;= \int q_1(z_1)\log \frac{q_1(z_1)}{p(x,z_1)}dz_1 - \text{KL}\left[q_1(z_1)||p(z_1|x)\right] \\
  &amp;\leq \int q_1(z_1)\log \frac{q_1(z_1)}{p(x,z_1)}dz_1 \\
  &amp;= \int q_0(z_0)\log q_1(F_1(z_0))dz_0 - \int q_1(z_1)\log p(x,z_1)dz_1 \\
  &amp;= \int q_0(z_0)\left[\log q_0(z_0) + \int_0^1 \frac{d\log q_t(z_t)}{dt}dt\right]dz_0 - \int q_1(z_1)\log p(x,z_1)dz_1 \\
  &amp;= \int q_0(z_0)\left[\log q_0(z_0) - \int_0^1 \text{Div}(V_t(z_t))\right]dz_0 - \int q_1(z_1)\log p(x,z_1)dz_1 \\
\end{align}
$$</div>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>


             
 
            
            
            






            <hr/>
        </div>
        <section id="article-sidebar" class="span2">
            <h4>Published</h4>
            <time itemprop="dateCreated" datetime="2023-11-17T00:00:00-05:00">Nov 17, 2023</time>

            <h4>Category</h4>
            <a class="category-link" href="/categories.html#blog-ref">Blog</a>
            





            





        </section>
</div>
</article>
                    </div>
                    <div class="span1"></div>
                </div>
            </div>
        </div>
<footer>




    <div id="fpowered">
        Powered by: <a href="http://getpelican.com/" title="Pelican Home Page" target="_blank" rel="nofollow noopener noreferrer">Pelican</a>
        Theme: <a href="https://elegant.oncrashreboot.com/" title="Theme Elegant Home Page" target="_blank" rel="nofollow noopener noreferrer">Elegant</a>
    </div>
</footer>            <script src="//code.jquery.com/jquery.min.js"></script>
        <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/js/bootstrap.min.js"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>

    <script>
    (function () {
        if (window.location.hash.match(/^#comment-\d+$/)) {
            $('#comment_thread').collapse('show');
        }
    })();
    window.onhashchange=function(){
        if (window.location.hash.match(/^#comment-\d+$/))
            window.location.reload(true);
    }
    $('#comment_thread').on('shown', function () {
        var link = document.getElementById('comment-accordion-toggle');
        var old_innerHTML = link.innerHTML;
        $(link).fadeOut(200, function() {
            $(this).text('Click here to hide comments').fadeIn(200);
        });
        $('#comment_thread').on('hidden', function () {
            $(link).fadeOut(200, function() {
                $(this).text(old_innerHTML).fadeIn(200);
            });
        })
    })
</script>

    </body>
    <!-- Theme: Elegant built for Pelican
        License : MIT -->
</html>