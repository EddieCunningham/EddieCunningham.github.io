{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"generax \u00a4 generax provides implementations of flow based generative models. The library is built on top of Equinox which removes the need to worry about keeping track of model parameters. key = random . PRNGKey ( 0 ) # JAX random key x = ... # some data # Create a flow model model = NeuralSpline ( input_shape = x . shape [ 1 :], n_flow_layers = 3 , n_blocks = 4 , hidden_size = 32 , working_size = 16 , n_spline_knots = 8 , key = key ) # Data dependent initialization model = model . data_dependent_init ( x , key = key ) # Take multiple samples using vmap keys = random . split ( key , 1000 ) samples = eqx . filter_vmap ( model . sample )( keys ) # Compute the log probability of data log_prob = eqx . filter_vmap ( model . log_prob )( x ) There is also support for probability paths (time-dependent probability distributions) which can be used to train continuous normalizing flows with flow matching. See the examples on flow matching and multi-sample flow matching for more details. Installation \u00a4 generax is available on pip: pip install generax Training \u00a4 Generax provides an easy interface to train these models: trainer = Trainer ( checkpoint_path = 'tmp/model_path' ) model = trainer . train ( model = model , # Generax model objective = my_objective , # Objective function evaluate_model = tester , # Testing function optimizer = optimizer , # Optax optimizer num_steps = 10000 , # Number of training steps data_iterator = train_ds , # Training data iterator double_batch = 1000 , # Train these many batches in a scan loop checkpoint_every = 1000 , # Checkpoint interval test_every = 1000 , # Test interval retrain = True ) # Retrain from checkpoint See the examples folder for more details.","title":"generax"},{"location":"#generax","text":"generax provides implementations of flow based generative models. The library is built on top of Equinox which removes the need to worry about keeping track of model parameters. key = random . PRNGKey ( 0 ) # JAX random key x = ... # some data # Create a flow model model = NeuralSpline ( input_shape = x . shape [ 1 :], n_flow_layers = 3 , n_blocks = 4 , hidden_size = 32 , working_size = 16 , n_spline_knots = 8 , key = key ) # Data dependent initialization model = model . data_dependent_init ( x , key = key ) # Take multiple samples using vmap keys = random . split ( key , 1000 ) samples = eqx . filter_vmap ( model . sample )( keys ) # Compute the log probability of data log_prob = eqx . filter_vmap ( model . log_prob )( x ) There is also support for probability paths (time-dependent probability distributions) which can be used to train continuous normalizing flows with flow matching. See the examples on flow matching and multi-sample flow matching for more details.","title":"generax"},{"location":"#installation","text":"generax is available on pip: pip install generax","title":"Installation"},{"location":"#training","text":"Generax provides an easy interface to train these models: trainer = Trainer ( checkpoint_path = 'tmp/model_path' ) model = trainer . train ( model = model , # Generax model objective = my_objective , # Objective function evaluate_model = tester , # Testing function optimizer = optimizer , # Optax optimizer num_steps = 10000 , # Number of training steps data_iterator = train_ds , # Training data iterator double_batch = 1000 , # Train these many batches in a scan loop checkpoint_every = 1000 , # Checkpoint interval test_every = 1000 , # Test interval retrain = True ) # Retrain from checkpoint See the examples folder for more details.","title":"Training"},{"location":"api/distributions/coupling/","text":"Couplings \u00a4 generax.distributions.coupling.Coupling \u00a4 Given two batches of samples from two distributions, this will compute a discrete distribution as done in multisample flow matching \\[q(x_0,x_1) = \\sum_{i,j}\\pi_{i,j}\\delta(x_0 - x_0^i)\\delta(x_1 - x_1^j))\\] Source code in generax/distributions/coupling.py class Coupling ( eqx . Module , ABC ): \"\"\"Given two batches of samples from two distributions, this will compute a discrete distribution as done in [multisample flow matching](https://arxiv.org/pdf/2304.14772.pdf) $$q(x_0,x_1) = \\sum_{i,j}\\pi_{i,j}\\delta(x_0 - x_0^i)\\delta(x_1 - x_1^j))$$ \"\"\" batch_size : int x0 : Array x1 : Array logits : Array def __init__ ( self , x0 : Array , x1 : Array ): \"\"\"Initialize the coupling **Arguments**: - x0: A batch of samples from p(x_0) - x1: A batch of samples from p(x_1) \"\"\" self . batch_size = x0 . shape [ 0 ] self . x0 = x0 self . x1 = x1 self . logits = self . compute_logits () assert self . logits . shape == ( self . batch_size , self . batch_size ) @abstractmethod def compute_logits ( self ): \"\"\"Compute $\\log \\pi_{i,j}$\"\"\" pass def sample_x0_given_x1 ( self , rng_key : PRNGKeyArray ) -> Array : \"\"\"Resample from the coupling **Arguments**: - rng_key: The random number generator key **Returns**: A sample from q(x_0|x_1) \"\"\" idx = jax . random . categorical ( rng_key , self . logits , axis = 0 ) return self . x0 [ idx ] __init__ ( self , x0 : Array , x1 : Array ) \u00a4 Initialize the coupling Arguments : - x0: A batch of samples from p(x_0) - x1: A batch of samples from p(x_1) Source code in generax/distributions/coupling.py def __init__ ( self , x0 : Array , x1 : Array ): \"\"\"Initialize the coupling **Arguments**: - x0: A batch of samples from p(x_0) - x1: A batch of samples from p(x_1) \"\"\" self . batch_size = x0 . shape [ 0 ] self . x0 = x0 self . x1 = x1 self . logits = self . compute_logits () assert self . logits . shape == ( self . batch_size , self . batch_size ) compute_logits ( self ) abstractmethod \u00a4 Compute \\(\\log \\pi_{i,j}\\) Source code in generax/distributions/coupling.py @abstractmethod def compute_logits ( self ): \"\"\"Compute $\\log \\pi_{i,j}$\"\"\" pass sample_x0_given_x1 ( self , rng_key : PRNGKeyArray ) -> Array \u00a4 Resample from the coupling Arguments : - rng_key: The random number generator key Returns : A sample from q(x_0|x_1) Source code in generax/distributions/coupling.py def sample_x0_given_x1 ( self , rng_key : PRNGKeyArray ) -> Array : \"\"\"Resample from the coupling **Arguments**: - rng_key: The random number generator key **Returns**: A sample from q(x_0|x_1) \"\"\" idx = jax . random . categorical ( rng_key , self . logits , axis = 0 ) return self . x0 [ idx ] generax.distributions.coupling.UniformCoupling ( Coupling ) \u00a4 This is a uniform coupling between two distributions Source code in generax/distributions/coupling.py class UniformCoupling ( Coupling ): \"\"\"This is a uniform coupling between two distributions\"\"\" def compute_logits ( self ) -> Array : \"\"\"Compute the logits for the coupling\"\"\" return jnp . ones (( self . batch_size , self . batch_size )) / self . batch_size def sample_x0_given_x1 ( self , rng_key : PRNGKeyArray ) -> Array : return self . x0 __init__ ( self , x0 : Array , x1 : Array ) \u00a4 Initialize the coupling Arguments : - x0: A batch of samples from p(x_0) - x1: A batch of samples from p(x_1) Source code in generax/distributions/coupling.py def __init__ ( self , x0 : Array , x1 : Array ): \"\"\"Initialize the coupling **Arguments**: - x0: A batch of samples from p(x_0) - x1: A batch of samples from p(x_1) \"\"\" self . batch_size = x0 . shape [ 0 ] self . x0 = x0 self . x1 = x1 self . logits = self . compute_logits () assert self . logits . shape == ( self . batch_size , self . batch_size ) compute_logits ( self ) -> Array \u00a4 Compute the logits for the coupling Source code in generax/distributions/coupling.py def compute_logits ( self ) -> Array : \"\"\"Compute the logits for the coupling\"\"\" return jnp . ones (( self . batch_size , self . batch_size )) / self . batch_size generax.distributions.coupling.OTTCoupling ( Coupling ) \u00a4 Optimal transport coupling using the ott library . This class uses the sinkhorn solver to compute the optimal transport coupling. Source code in generax/distributions/coupling.py class OTTCoupling ( Coupling ): \"\"\"Optimal transport coupling using the [ott library](https://ott-jax.readthedocs.io/en/latest/). This class uses the sinkhorn solver to compute the optimal transport coupling. \"\"\" def compute_logits ( self ) -> Array : \"\"\"Solve for the optimal transport couplings\"\"\" # Create a point cloud object geom = pointcloud . PointCloud ( self . x0 , self . x1 ) # Define the loss function ot_prob = linear_problem . LinearProblem ( geom ) # Create a sinkhorn solver solver = sinkhorn . Sinkhorn ( ot_prob ) # Solve the OT problem ot = solver ( ot_prob ) # Return the coupling mat = ot . matrix return jnp . log ( mat + 1e-8 ) __init__ ( self , x0 : Array , x1 : Array ) \u00a4 Initialize the coupling Arguments : - x0: A batch of samples from p(x_0) - x1: A batch of samples from p(x_1) Source code in generax/distributions/coupling.py def __init__ ( self , x0 : Array , x1 : Array ): \"\"\"Initialize the coupling **Arguments**: - x0: A batch of samples from p(x_0) - x1: A batch of samples from p(x_1) \"\"\" self . batch_size = x0 . shape [ 0 ] self . x0 = x0 self . x1 = x1 self . logits = self . compute_logits () assert self . logits . shape == ( self . batch_size , self . batch_size ) compute_logits ( self ) -> Array \u00a4 Solve for the optimal transport couplings Source code in generax/distributions/coupling.py def compute_logits ( self ) -> Array : \"\"\"Solve for the optimal transport couplings\"\"\" # Create a point cloud object geom = pointcloud . PointCloud ( self . x0 , self . x1 ) # Define the loss function ot_prob = linear_problem . LinearProblem ( geom ) # Create a sinkhorn solver solver = sinkhorn . Sinkhorn ( ot_prob ) # Solve the OT problem ot = solver ( ot_prob ) # Return the coupling mat = ot . matrix return jnp . log ( mat + 1e-8 )","title":"Couplings"},{"location":"api/distributions/coupling/#couplings","text":"","title":"Couplings"},{"location":"api/distributions/coupling/#generax.distributions.coupling.Coupling","text":"Given two batches of samples from two distributions, this will compute a discrete distribution as done in multisample flow matching \\[q(x_0,x_1) = \\sum_{i,j}\\pi_{i,j}\\delta(x_0 - x_0^i)\\delta(x_1 - x_1^j))\\] Source code in generax/distributions/coupling.py class Coupling ( eqx . Module , ABC ): \"\"\"Given two batches of samples from two distributions, this will compute a discrete distribution as done in [multisample flow matching](https://arxiv.org/pdf/2304.14772.pdf) $$q(x_0,x_1) = \\sum_{i,j}\\pi_{i,j}\\delta(x_0 - x_0^i)\\delta(x_1 - x_1^j))$$ \"\"\" batch_size : int x0 : Array x1 : Array logits : Array def __init__ ( self , x0 : Array , x1 : Array ): \"\"\"Initialize the coupling **Arguments**: - x0: A batch of samples from p(x_0) - x1: A batch of samples from p(x_1) \"\"\" self . batch_size = x0 . shape [ 0 ] self . x0 = x0 self . x1 = x1 self . logits = self . compute_logits () assert self . logits . shape == ( self . batch_size , self . batch_size ) @abstractmethod def compute_logits ( self ): \"\"\"Compute $\\log \\pi_{i,j}$\"\"\" pass def sample_x0_given_x1 ( self , rng_key : PRNGKeyArray ) -> Array : \"\"\"Resample from the coupling **Arguments**: - rng_key: The random number generator key **Returns**: A sample from q(x_0|x_1) \"\"\" idx = jax . random . categorical ( rng_key , self . logits , axis = 0 ) return self . x0 [ idx ]","title":"Coupling"},{"location":"api/distributions/coupling/#generax.distributions.coupling.UniformCoupling","text":"This is a uniform coupling between two distributions Source code in generax/distributions/coupling.py class UniformCoupling ( Coupling ): \"\"\"This is a uniform coupling between two distributions\"\"\" def compute_logits ( self ) -> Array : \"\"\"Compute the logits for the coupling\"\"\" return jnp . ones (( self . batch_size , self . batch_size )) / self . batch_size def sample_x0_given_x1 ( self , rng_key : PRNGKeyArray ) -> Array : return self . x0","title":"UniformCoupling"},{"location":"api/distributions/coupling/#generax.distributions.coupling.OTTCoupling","text":"Optimal transport coupling using the ott library . This class uses the sinkhorn solver to compute the optimal transport coupling. Source code in generax/distributions/coupling.py class OTTCoupling ( Coupling ): \"\"\"Optimal transport coupling using the [ott library](https://ott-jax.readthedocs.io/en/latest/). This class uses the sinkhorn solver to compute the optimal transport coupling. \"\"\" def compute_logits ( self ) -> Array : \"\"\"Solve for the optimal transport couplings\"\"\" # Create a point cloud object geom = pointcloud . PointCloud ( self . x0 , self . x1 ) # Define the loss function ot_prob = linear_problem . LinearProblem ( geom ) # Create a sinkhorn solver solver = sinkhorn . Sinkhorn ( ot_prob ) # Solve the OT problem ot = solver ( ot_prob ) # Return the coupling mat = ot . matrix return jnp . log ( mat + 1e-8 )","title":"OTTCoupling"},{"location":"api/distributions/distributions/","text":"Base \u00a4 generax.distributions.base.ProbabilityDistribution \u00a4 An object that we can sample from and use to evaluate log probabilities. This is an abstract base class. Atributes : input_shape : The shape of samples. Methods : sample_and_log_prob(key) -> (x,log_px) : Sample from the distribution and compute the log probability. sample(key) -> x : Pull a single sample from the model log_prob(x) -> log_px : Compute the log probability of a point under the model Source code in generax/distributions/base.py class ProbabilityDistribution ( eqx . Module , ABC ): \"\"\"An object that we can sample from and use to evaluate log probabilities. This is an abstract base class. **Atributes**: - `input_shape`: The shape of samples. **Methods**: - `sample_and_log_prob(key) -> (x,log_px)`: Sample from the distribution and compute the log probability. - `sample(key) -> x`: Pull a single sample from the model - `log_prob(x) -> log_px`: Compute the log probability of a point under the model \"\"\" input_shape : int = eqx . field ( static = True ) def __init__ ( self , * , input_shape : Union [ int , Tuple [ int ]], ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The dimension of the space. This can be either an integer or a tuple of integers to represent images \"\"\" assert isinstance ( input_shape , tuple ) or isinstance ( input_shape , list ) self . input_shape = input_shape @abstractmethod def sample_and_log_prob ( self , key : PRNGKeyArray ) -> Array : \"\"\"**Arguments**: - `key`: The random number generator key. **Returns**: A single sample from the model with its log probability. Use eqx.filter_vmap to get more samples! For example, ```python keys = random.split(key, n_samples) x, log_px = eqx.filter_vmap(self.sample_and_log_prob)(keys) ``` \"\"\" pass def sample ( self , key : PRNGKeyArray , y : Optional [ Array ] = None ) -> Array : \"\"\" **Arguments**: - `key`: The random number generator key. **Returns**: Samples from the model Use eqx.filter_vmap to get more samples! For example, ```python keys = random.split(key, n_samples) samples = eqx.filter_vmap(self.sample)(keys) ``` \"\"\" return self . sample_and_log_prob ( key , y )[ 0 ] @abstractmethod def log_prob ( self , x : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array : \"\"\"**Arguments**: - `x`: The point we want to compute logp(x) at. - `y`: The (optional) conditioning information. - `key`: The random number generator key. Can be passed in the event that we're getting a stochastic estimate of the log prob. **Returns**: The log likelihood of x under the model. \"\"\" pass def score ( self , x : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array : \"\"\"**Arguments**: - `x`: The point we want to compute grad logp(x) at. - `y`: The (optional) conditioning information. - `key`: The random number generator key. Can be passed in the event that we're getting a stochastic estimate of the log prob. **Returns**: The log likelihood of x under the model. \"\"\" return eqx . filter_grad ( self . log_prob )( x , y = y , key = key ) __init__ ( self , * , input_shape : Union [ int , Tuple [ int ]], ** kwargs ) \u00a4 Arguments : input_shape : The dimension of the space. This can be either an integer or a tuple of integers to represent images Source code in generax/distributions/base.py def __init__ ( self , * , input_shape : Union [ int , Tuple [ int ]], ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The dimension of the space. This can be either an integer or a tuple of integers to represent images \"\"\" assert isinstance ( input_shape , tuple ) or isinstance ( input_shape , list ) self . input_shape = input_shape sample_and_log_prob ( self , key : PRNGKeyArray ) -> Array abstractmethod \u00a4 Arguments : key : The random number generator key. Returns : A single sample from the model with its log probability. Use eqx.filter_vmap to get more samples! For example, keys = random . split ( key , n_samples ) x , log_px = eqx . filter_vmap ( self . sample_and_log_prob )( keys ) Source code in generax/distributions/base.py @abstractmethod def sample_and_log_prob ( self , key : PRNGKeyArray ) -> Array : \"\"\"**Arguments**: - `key`: The random number generator key. **Returns**: A single sample from the model with its log probability. Use eqx.filter_vmap to get more samples! For example, ```python keys = random.split(key, n_samples) x, log_px = eqx.filter_vmap(self.sample_and_log_prob)(keys) ``` \"\"\" pass sample ( self , key : PRNGKeyArray , y : Optional [ Array ] = None ) -> Array \u00a4 Arguments : key : The random number generator key. Returns : Samples from the model Use eqx.filter_vmap to get more samples! For example, keys = random . split ( key , n_samples ) samples = eqx . filter_vmap ( self . sample )( keys ) Source code in generax/distributions/base.py def sample ( self , key : PRNGKeyArray , y : Optional [ Array ] = None ) -> Array : \"\"\" **Arguments**: - `key`: The random number generator key. **Returns**: Samples from the model Use eqx.filter_vmap to get more samples! For example, ```python keys = random.split(key, n_samples) samples = eqx.filter_vmap(self.sample)(keys) ``` \"\"\" return self . sample_and_log_prob ( key , y )[ 0 ] log_prob ( self , x : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array abstractmethod \u00a4 Arguments : x : The point we want to compute logp(x) at. y : The (optional) conditioning information. key : The random number generator key. Can be passed in the event that we're getting a stochastic estimate of the log prob. Returns : The log likelihood of x under the model. Source code in generax/distributions/base.py @abstractmethod def log_prob ( self , x : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array : \"\"\"**Arguments**: - `x`: The point we want to compute logp(x) at. - `y`: The (optional) conditioning information. - `key`: The random number generator key. Can be passed in the event that we're getting a stochastic estimate of the log prob. **Returns**: The log likelihood of x under the model. \"\"\" pass score ( self , x : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array \u00a4 Arguments : x : The point we want to compute grad logp(x) at. y : The (optional) conditioning information. key : The random number generator key. Can be passed in the event that we're getting a stochastic estimate of the log prob. Returns : The log likelihood of x under the model. Source code in generax/distributions/base.py def score ( self , x : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array : \"\"\"**Arguments**: - `x`: The point we want to compute grad logp(x) at. - `y`: The (optional) conditioning information. - `key`: The random number generator key. Can be passed in the event that we're getting a stochastic estimate of the log prob. **Returns**: The log likelihood of x under the model. \"\"\" return eqx . filter_grad ( self . log_prob )( x , y = y , key = key ) generax.distributions.base.ProbabilityPath ( ProbabilityDistribution ) \u00a4 A time dependent probability distribution. Atributes : input_shape : The dimension of the sampling space. Methods : sample_and_log_prob(key) -> (x,log_px) : Sample from the distribution and compute the log probability. sample(key) -> x : Pull a single sample from the model log_prob(x) -> log_px : Compute the log probability of a point under the model Source code in generax/distributions/base.py class ProbabilityPath ( ProbabilityDistribution ): \"\"\"A time dependent probability distribution. **Atributes**: - `input_shape`: The dimension of the sampling space. **Methods**: - `sample_and_log_prob(key) -> (x,log_px)`: Sample from the distribution and compute the log probability. - `sample(key) -> x`: Pull a single sample from the model - `log_prob(x) -> log_px`: Compute the log probability of a point under the model \"\"\" input_shape : int = eqx . field ( static = True ) def __init__ ( self , * , input_shape : Union [ int , Tuple [ int ]], ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The dimension of the space. This can be either an integer or a tuple of integers to represent images \"\"\" assert isinstance ( input_shape , tuple ) or isinstance ( input_shape , list ) self . input_shape = input_shape @abstractmethod def sample_and_log_prob ( self , t : Array , key : PRNGKeyArray ) -> Array : \"\"\"**Arguments**: - `t`: The time at which we want to sample. - `key`: The random number generator key. **Returns**: A single sample from the model with its log probability. \"\"\" pass @abstractmethod def log_prob ( self , t : Array , xt : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array : \"\"\"**Arguments**: - `t`: The time at which we want to sample. - `xt`: The point we want to compute logp(x) at. - `y`: The (optional) conditioning information. - `key`: The random number generator key. Can be passed in the event that we're getting a stochastic estimate of the log prob. **Returns**: The log likelihood of x under the model. \"\"\" pass def sample ( self , t : Array , key : PRNGKeyArray , y : Optional [ Array ] = None ) -> Array : \"\"\" Use eqx.filter_vmap to get more samples! For example, keys = random.split(key, n_samples) samples = eqx.filter_vmap(self.sample, in_axes=(None, 0))(t, keys) **Arguments**: - `t`: The time at which we want to sample. - `key`: The random number generator key. - `n_samples`: The number of samples to draw. If `None`, then we just draw a single sample. **Returns**: Samples from the model \"\"\" return self . sample_and_log_prob ( t , key , y )[ 0 ] def score ( self , t : Array , xt : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array : \"\"\"**Arguments**: - `x`: The point we want to compute grad logp(x) at. - `y`: The (optional) conditioning information. - `key`: The random number generator key. Can be passed in the event that we're getting a stochastic estimate of the log prob. **Returns**: The log likelihood of x under the model. \"\"\" def log_prob ( xt ): return self . log_prob ( t , xt , y = y , key = key ) return eqx . filter_grad ( log_prob )( xt ) @abstractmethod def transform_and_vector_field ( self , t : Array , x0 : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"The vector field that samples evolve on as t changes **Arguments**: - `t`: Time. - `x0`: A point in the base space. - `y`: The (optional) conditioning information. **Returns**: (xt, dxt/dt) \"\"\" pass @abstractmethod def vector_field ( self , t : Array , xt : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"The vector field that samples evolve on as t changes **Arguments**: - `t`: Time. - `xt`: A point in the base space. - `y`: The (optional) conditioning information. **Returns**: dxt/dt \"\"\" pass __init__ ( self , * , input_shape : Union [ int , Tuple [ int ]], ** kwargs ) \u00a4 Arguments : input_shape : The dimension of the space. This can be either an integer or a tuple of integers to represent images Source code in generax/distributions/base.py def __init__ ( self , * , input_shape : Union [ int , Tuple [ int ]], ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The dimension of the space. This can be either an integer or a tuple of integers to represent images \"\"\" assert isinstance ( input_shape , tuple ) or isinstance ( input_shape , list ) self . input_shape = input_shape sample_and_log_prob ( self , t : Array , key : PRNGKeyArray ) -> Array abstractmethod \u00a4 Arguments : t : The time at which we want to sample. key : The random number generator key. Returns : A single sample from the model with its log probability. Source code in generax/distributions/base.py @abstractmethod def sample_and_log_prob ( self , t : Array , key : PRNGKeyArray ) -> Array : \"\"\"**Arguments**: - `t`: The time at which we want to sample. - `key`: The random number generator key. **Returns**: A single sample from the model with its log probability. \"\"\" pass log_prob ( self , t : Array , xt : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array abstractmethod \u00a4 Arguments : t : The time at which we want to sample. xt : The point we want to compute logp(x) at. y : The (optional) conditioning information. key : The random number generator key. Can be passed in the event that we're getting a stochastic estimate of the log prob. Returns : The log likelihood of x under the model. Source code in generax/distributions/base.py @abstractmethod def log_prob ( self , t : Array , xt : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array : \"\"\"**Arguments**: - `t`: The time at which we want to sample. - `xt`: The point we want to compute logp(x) at. - `y`: The (optional) conditioning information. - `key`: The random number generator key. Can be passed in the event that we're getting a stochastic estimate of the log prob. **Returns**: The log likelihood of x under the model. \"\"\" pass sample ( self , t : Array , key : PRNGKeyArray , y : Optional [ Array ] = None ) -> Array \u00a4 Use eqx.filter_vmap to get more samples! For example, keys = random.split(key, n_samples) samples = eqx.filter_vmap(self.sample, in_axes=(None, 0))(t, keys) Arguments : t : The time at which we want to sample. key : The random number generator key. n_samples : The number of samples to draw. If None , then we just draw a single sample. Returns : Samples from the model Source code in generax/distributions/base.py def sample ( self , t : Array , key : PRNGKeyArray , y : Optional [ Array ] = None ) -> Array : \"\"\" Use eqx.filter_vmap to get more samples! For example, keys = random.split(key, n_samples) samples = eqx.filter_vmap(self.sample, in_axes=(None, 0))(t, keys) **Arguments**: - `t`: The time at which we want to sample. - `key`: The random number generator key. - `n_samples`: The number of samples to draw. If `None`, then we just draw a single sample. **Returns**: Samples from the model \"\"\" return self . sample_and_log_prob ( t , key , y )[ 0 ] score ( self , t : Array , xt : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array \u00a4 Arguments : x : The point we want to compute grad logp(x) at. y : The (optional) conditioning information. key : The random number generator key. Can be passed in the event that we're getting a stochastic estimate of the log prob. Returns : The log likelihood of x under the model. Source code in generax/distributions/base.py def score ( self , t : Array , xt : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array : \"\"\"**Arguments**: - `x`: The point we want to compute grad logp(x) at. - `y`: The (optional) conditioning information. - `key`: The random number generator key. Can be passed in the event that we're getting a stochastic estimate of the log prob. **Returns**: The log likelihood of x under the model. \"\"\" def log_prob ( xt ): return self . log_prob ( t , xt , y = y , key = key ) return eqx . filter_grad ( log_prob )( xt ) transform_and_vector_field ( self , t : Array , x0 : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array abstractmethod \u00a4 The vector field that samples evolve on as t changes Arguments : t : Time. x0 : A point in the base space. y : The (optional) conditioning information. Returns : (xt, dxt/dt) Source code in generax/distributions/base.py @abstractmethod def transform_and_vector_field ( self , t : Array , x0 : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"The vector field that samples evolve on as t changes **Arguments**: - `t`: Time. - `x0`: A point in the base space. - `y`: The (optional) conditioning information. **Returns**: (xt, dxt/dt) \"\"\" pass @abstractmethod def vector_field ( self , t : Array , xt : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"The vector field that samples evolve on as t changes **Arguments**: - `t`: Time. - `xt`: A point in the base space. - `y`: The (optional) conditioning information. **Returns**: dxt/dt \"\"\" pass generax.distributions.base.Gaussian ( ProbabilityDistribution ) \u00a4 This represents a Gaussian distribution Source code in generax/distributions/base.py class Gaussian ( ProbabilityDistribution ): \"\"\"This represents a Gaussian distribution\"\"\" def sample ( self , key : PRNGKeyArray , y : Optional [ Array ] = None ) -> Array : \"\"\"**Arguments**: - `key`: The random number generator key. **Returns**: A single sample from the model. Use eqx.filter_vmap to get more samples. \"\"\" return random . normal ( key , shape = self . input_shape ) def log_prob ( self , x : Array ) -> Array : \"\"\"**Arguments**: - `x`: The point we want to compute logp(x) at. **Returns**: The log likelihood of x under the model. \"\"\" return jax . scipy . stats . norm . logpdf ( x ) . sum () def sample_and_log_prob ( self , key : PRNGKeyArray ) -> Array : \"\"\"**Arguments**: - `key`: The random number generator key. **Returns**: A single sample from the model with its log probability. \"\"\" x = self . sample ( key ) log_px = self . log_prob ( x ) return x , log_px __init__ ( self , * , input_shape : Union [ int , Tuple [ int ]], ** kwargs ) \u00a4 Arguments : input_shape : The dimension of the space. This can be either an integer or a tuple of integers to represent images Source code in generax/distributions/base.py def __init__ ( self , * , input_shape : Union [ int , Tuple [ int ]], ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The dimension of the space. This can be either an integer or a tuple of integers to represent images \"\"\" assert isinstance ( input_shape , tuple ) or isinstance ( input_shape , list ) self . input_shape = input_shape sample ( self , key : PRNGKeyArray , y : Optional [ Array ] = None ) -> Array \u00a4 Arguments : key : The random number generator key. Returns : A single sample from the model. Use eqx.filter_vmap to get more samples. Source code in generax/distributions/base.py def sample ( self , key : PRNGKeyArray , y : Optional [ Array ] = None ) -> Array : \"\"\"**Arguments**: - `key`: The random number generator key. **Returns**: A single sample from the model. Use eqx.filter_vmap to get more samples. \"\"\" return random . normal ( key , shape = self . input_shape ) log_prob ( self , x : Array ) -> Array \u00a4 Arguments : x : The point we want to compute logp(x) at. Returns : The log likelihood of x under the model. Source code in generax/distributions/base.py def log_prob ( self , x : Array ) -> Array : \"\"\"**Arguments**: - `x`: The point we want to compute logp(x) at. **Returns**: The log likelihood of x under the model. \"\"\" return jax . scipy . stats . norm . logpdf ( x ) . sum ()","title":"Base"},{"location":"api/distributions/distributions/#base","text":"","title":"Base"},{"location":"api/distributions/distributions/#generax.distributions.base.ProbabilityDistribution","text":"An object that we can sample from and use to evaluate log probabilities. This is an abstract base class. Atributes : input_shape : The shape of samples. Methods : sample_and_log_prob(key) -> (x,log_px) : Sample from the distribution and compute the log probability. sample(key) -> x : Pull a single sample from the model log_prob(x) -> log_px : Compute the log probability of a point under the model Source code in generax/distributions/base.py class ProbabilityDistribution ( eqx . Module , ABC ): \"\"\"An object that we can sample from and use to evaluate log probabilities. This is an abstract base class. **Atributes**: - `input_shape`: The shape of samples. **Methods**: - `sample_and_log_prob(key) -> (x,log_px)`: Sample from the distribution and compute the log probability. - `sample(key) -> x`: Pull a single sample from the model - `log_prob(x) -> log_px`: Compute the log probability of a point under the model \"\"\" input_shape : int = eqx . field ( static = True ) def __init__ ( self , * , input_shape : Union [ int , Tuple [ int ]], ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The dimension of the space. This can be either an integer or a tuple of integers to represent images \"\"\" assert isinstance ( input_shape , tuple ) or isinstance ( input_shape , list ) self . input_shape = input_shape @abstractmethod def sample_and_log_prob ( self , key : PRNGKeyArray ) -> Array : \"\"\"**Arguments**: - `key`: The random number generator key. **Returns**: A single sample from the model with its log probability. Use eqx.filter_vmap to get more samples! For example, ```python keys = random.split(key, n_samples) x, log_px = eqx.filter_vmap(self.sample_and_log_prob)(keys) ``` \"\"\" pass def sample ( self , key : PRNGKeyArray , y : Optional [ Array ] = None ) -> Array : \"\"\" **Arguments**: - `key`: The random number generator key. **Returns**: Samples from the model Use eqx.filter_vmap to get more samples! For example, ```python keys = random.split(key, n_samples) samples = eqx.filter_vmap(self.sample)(keys) ``` \"\"\" return self . sample_and_log_prob ( key , y )[ 0 ] @abstractmethod def log_prob ( self , x : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array : \"\"\"**Arguments**: - `x`: The point we want to compute logp(x) at. - `y`: The (optional) conditioning information. - `key`: The random number generator key. Can be passed in the event that we're getting a stochastic estimate of the log prob. **Returns**: The log likelihood of x under the model. \"\"\" pass def score ( self , x : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array : \"\"\"**Arguments**: - `x`: The point we want to compute grad logp(x) at. - `y`: The (optional) conditioning information. - `key`: The random number generator key. Can be passed in the event that we're getting a stochastic estimate of the log prob. **Returns**: The log likelihood of x under the model. \"\"\" return eqx . filter_grad ( self . log_prob )( x , y = y , key = key )","title":"ProbabilityDistribution"},{"location":"api/distributions/distributions/#generax.distributions.base.ProbabilityPath","text":"A time dependent probability distribution. Atributes : input_shape : The dimension of the sampling space. Methods : sample_and_log_prob(key) -> (x,log_px) : Sample from the distribution and compute the log probability. sample(key) -> x : Pull a single sample from the model log_prob(x) -> log_px : Compute the log probability of a point under the model Source code in generax/distributions/base.py class ProbabilityPath ( ProbabilityDistribution ): \"\"\"A time dependent probability distribution. **Atributes**: - `input_shape`: The dimension of the sampling space. **Methods**: - `sample_and_log_prob(key) -> (x,log_px)`: Sample from the distribution and compute the log probability. - `sample(key) -> x`: Pull a single sample from the model - `log_prob(x) -> log_px`: Compute the log probability of a point under the model \"\"\" input_shape : int = eqx . field ( static = True ) def __init__ ( self , * , input_shape : Union [ int , Tuple [ int ]], ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The dimension of the space. This can be either an integer or a tuple of integers to represent images \"\"\" assert isinstance ( input_shape , tuple ) or isinstance ( input_shape , list ) self . input_shape = input_shape @abstractmethod def sample_and_log_prob ( self , t : Array , key : PRNGKeyArray ) -> Array : \"\"\"**Arguments**: - `t`: The time at which we want to sample. - `key`: The random number generator key. **Returns**: A single sample from the model with its log probability. \"\"\" pass @abstractmethod def log_prob ( self , t : Array , xt : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array : \"\"\"**Arguments**: - `t`: The time at which we want to sample. - `xt`: The point we want to compute logp(x) at. - `y`: The (optional) conditioning information. - `key`: The random number generator key. Can be passed in the event that we're getting a stochastic estimate of the log prob. **Returns**: The log likelihood of x under the model. \"\"\" pass def sample ( self , t : Array , key : PRNGKeyArray , y : Optional [ Array ] = None ) -> Array : \"\"\" Use eqx.filter_vmap to get more samples! For example, keys = random.split(key, n_samples) samples = eqx.filter_vmap(self.sample, in_axes=(None, 0))(t, keys) **Arguments**: - `t`: The time at which we want to sample. - `key`: The random number generator key. - `n_samples`: The number of samples to draw. If `None`, then we just draw a single sample. **Returns**: Samples from the model \"\"\" return self . sample_and_log_prob ( t , key , y )[ 0 ] def score ( self , t : Array , xt : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array : \"\"\"**Arguments**: - `x`: The point we want to compute grad logp(x) at. - `y`: The (optional) conditioning information. - `key`: The random number generator key. Can be passed in the event that we're getting a stochastic estimate of the log prob. **Returns**: The log likelihood of x under the model. \"\"\" def log_prob ( xt ): return self . log_prob ( t , xt , y = y , key = key ) return eqx . filter_grad ( log_prob )( xt ) @abstractmethod def transform_and_vector_field ( self , t : Array , x0 : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"The vector field that samples evolve on as t changes **Arguments**: - `t`: Time. - `x0`: A point in the base space. - `y`: The (optional) conditioning information. **Returns**: (xt, dxt/dt) \"\"\" pass @abstractmethod def vector_field ( self , t : Array , xt : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"The vector field that samples evolve on as t changes **Arguments**: - `t`: Time. - `xt`: A point in the base space. - `y`: The (optional) conditioning information. **Returns**: dxt/dt \"\"\" pass","title":"ProbabilityPath"},{"location":"api/distributions/distributions/#generax.distributions.base.Gaussian","text":"This represents a Gaussian distribution Source code in generax/distributions/base.py class Gaussian ( ProbabilityDistribution ): \"\"\"This represents a Gaussian distribution\"\"\" def sample ( self , key : PRNGKeyArray , y : Optional [ Array ] = None ) -> Array : \"\"\"**Arguments**: - `key`: The random number generator key. **Returns**: A single sample from the model. Use eqx.filter_vmap to get more samples. \"\"\" return random . normal ( key , shape = self . input_shape ) def log_prob ( self , x : Array ) -> Array : \"\"\"**Arguments**: - `x`: The point we want to compute logp(x) at. **Returns**: The log likelihood of x under the model. \"\"\" return jax . scipy . stats . norm . logpdf ( x ) . sum () def sample_and_log_prob ( self , key : PRNGKeyArray ) -> Array : \"\"\"**Arguments**: - `key`: The random number generator key. **Returns**: A single sample from the model with its log probability. \"\"\" x = self . sample ( key ) log_px = self . log_prob ( x ) return x , log_px","title":"Gaussian"},{"location":"api/distributions/flow_models/","text":"Flows \u00a4 generax.distributions.flow_models.NormalizingFlow ( ProbabilityDistribution ) \u00a4 A normalizing flow is a model that we use to represent probability distributions. See this for an overview. Atributes : transform : A BijectiveTransform object that transforms a variable from the base space to the data space and also computes the change is log pdf. transform(x) -> (z,log_det) : Apply the transformation to the input. prior : The prior probability distribution. Methods : to_base_space(x) -> z : Transform a point from the data space to the base space. sample_and_log_prob(key) -> (x,log_px) : Sample from the distribution and compute the log probability. sample(key) -> x : Pull a single sample from the model log_prob(x) -> log_px : Compute the log probability of a point under the model Source code in generax/distributions/flow_models.py class NormalizingFlow ( ProbabilityDistribution , ABC ): \"\"\"A normalizing flow is a model that we use to represent probability distributions. See [this](https://arxiv.org/pdf/1912.02762.pdf) for an overview. **Atributes**: - `transform`: A `BijectiveTransform` object that transforms a variable from the base space to the data space and also computes the change is log pdf. - `transform(x) -> (z,log_det)`: Apply the transformation to the input. - `prior`: The prior probability distribution. **Methods**: - `to_base_space(x) -> z`: Transform a point from the data space to the base space. - `sample_and_log_prob(key) -> (x,log_px)`: Sample from the distribution and compute the log probability. - `sample(key) -> x`: Pull a single sample from the model - `log_prob(x) -> log_px`: Compute the log probability of a point under the model \"\"\" transform : BijectiveTransform prior : ProbabilityDistribution def __init__ ( self , transform : BijectiveTransform , prior : ProbabilityDistribution , ** kwargs ): \"\"\"**Arguments**: - `transform`: A bijective transformation - `prior`: The prior distribution \"\"\" self . transform = transform self . prior = prior input_shape = self . transform . input_shape super () . __init__ ( input_shape = input_shape , ** kwargs ) def to_base_space ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: A JAX array with shape `(dim,)`. - `y`: The conditioning information **Returns**: A JAX array with shape `(dim,)`. \"\"\" return self . transform ( x , y = y , ** kwargs )[ 0 ] def to_data_space ( self , z : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"**Arguments**: - `z`: A JAX array with shape `(dim,)`. - `y`: The conditioning information **Returns**: A JAX array with shape `(dim,)`. \"\"\" return self . transform ( z , y = y , inverse = True , ** kwargs )[ 0 ] def sample_and_log_prob ( self , key : PRNGKeyArray , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"**Arguments**: - `key`: The random number generator key. - `y`: The conditioning information **Returns**: A single sample from the model. Use vmap to get more samples. \"\"\" z , log_pz = self . prior . sample_and_log_prob ( key ) x , log_det = self . transform ( z , y = y , inverse = True , ** kwargs ) # The log determinant of the inverse transform has a negative sign! return x , log_pz - log_det def log_prob ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The point we want to compute logp(x) at. - `y`: The conditioning information **Returns**: The log likelihood of x under the model. \"\"\" z , log_det = self . transform ( x , y = y , ** kwargs ) log_pz = self . prior . log_prob ( z ) return log_pz + log_det def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on a batch of data. This is one of the few times that $x$ is expected to be batched. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new flow with the parameters initialized. \"\"\" new_layer = self . transform . data_dependent_init ( x , y = y , key = key ) # Turn the new parameters into a new module get_transform = lambda tree : tree . transform return eqx . tree_at ( get_transform , self , new_layer ) __init__ ( self , transform : BijectiveTransform , prior : ProbabilityDistribution , ** kwargs ) \u00a4 Arguments : transform : A bijective transformation prior : The prior distribution Source code in generax/distributions/flow_models.py def __init__ ( self , transform : BijectiveTransform , prior : ProbabilityDistribution , ** kwargs ): \"\"\"**Arguments**: - `transform`: A bijective transformation - `prior`: The prior distribution \"\"\" self . transform = transform self . prior = prior input_shape = self . transform . input_shape super () . __init__ ( input_shape = input_shape , ** kwargs ) sample ( self , key : PRNGKeyArray , y : Optional [ Array ] = None ) -> Array \u00a4 Inherited from generax.distributions.base.ProbabilityDistribution.sample . Source code in generax/distributions/flow_models.py def sample ( self , key : PRNGKeyArray , y : Optional [ Array ] = None ) -> Array : \"\"\" **Arguments**: - `key`: The random number generator key. **Returns**: Samples from the model Use eqx.filter_vmap to get more samples! For example, ```python keys = random.split(key, n_samples) samples = eqx.filter_vmap(self.sample)(keys) ``` \"\"\" return self . sample_and_log_prob ( key , y )[ 0 ] sample_and_log_prob ( self , key : PRNGKeyArray , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Arguments : key : The random number generator key. y : The conditioning information Returns : A single sample from the model. Use vmap to get more samples. Source code in generax/distributions/flow_models.py def sample_and_log_prob ( self , key : PRNGKeyArray , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"**Arguments**: - `key`: The random number generator key. - `y`: The conditioning information **Returns**: A single sample from the model. Use vmap to get more samples. \"\"\" z , log_pz = self . prior . sample_and_log_prob ( key ) x , log_det = self . transform ( z , y = y , inverse = True , ** kwargs ) # The log determinant of the inverse transform has a negative sign! return x , log_pz - log_det score ( self , x : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array \u00a4 Inherited from generax.distributions.base.ProbabilityDistribution.score . Source code in generax/distributions/flow_models.py def score ( self , x : Array , y : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None ) -> Array : \"\"\"**Arguments**: - `x`: The point we want to compute grad logp(x) at. - `y`: The (optional) conditioning information. - `key`: The random number generator key. Can be passed in the event that we're getting a stochastic estimate of the log prob. **Returns**: The log likelihood of x under the model. \"\"\" return eqx . filter_grad ( self . log_prob )( x , y = y , key = key ) log_prob ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Arguments : x : The point we want to compute logp(x) at. y : The conditioning information Returns : The log likelihood of x under the model. Source code in generax/distributions/flow_models.py def log_prob ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The point we want to compute logp(x) at. - `y`: The conditioning information **Returns**: The log likelihood of x under the model. \"\"\" z , log_det = self . transform ( x , y = y , ** kwargs ) log_pz = self . prior . log_prob ( z ) return log_pz + log_det data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform \u00a4 Initialize the parameters of the layer based on a batch of data. This is one of the few times that \\(x\\) is expected to be batched. Arguments : x : The data to initialize the parameters with. y : The conditioning information key : A jax.random.PRNGKey for initialization Returns : A new flow with the parameters initialized. Source code in generax/distributions/flow_models.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on a batch of data. This is one of the few times that $x$ is expected to be batched. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new flow with the parameters initialized. \"\"\" new_layer = self . transform . data_dependent_init ( x , y = y , key = key ) # Turn the new parameters into a new module get_transform = lambda tree : tree . transform return eqx . tree_at ( get_transform , self , new_layer ) generax.distributions.flow_models.RealNVP ( NormalizingFlow ) \u00a4 RealNVP( args, *kwargs) Source code in generax/distributions/flow_models.py class RealNVP ( NormalizingFlow ): def __init__ ( self , input_shape : Tuple [ int ], n_flow_layers : int = 3 , working_size : int = 16 , hidden_size : int = 32 , n_blocks : int = 4 , cond_shape : Optional [ Tuple [ int ]] = None , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The shape of the input data. - `n_flow_layers`: The number of layers in the flow. - `working_size`: The size of the working space. - `hidden_size`: The size of the hidden layers. - `n_blocks`: The number of blocks in the coupling layers. - `cond_shape`: The shape of the conditioning information. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" transform = RealNVPTransform ( input_shape = input_shape , n_flow_layers = n_flow_layers , working_size = working_size , hidden_size = hidden_size , n_blocks = n_blocks , cond_shape = cond_shape , key = key ) prior = Gaussian ( input_shape = input_shape ) super () . __init__ ( transform = transform , prior = prior , ** kwargs ) __init__ ( self , input_shape : Tuple [ int ], n_flow_layers : int = 3 , working_size : int = 16 , hidden_size : int = 32 , n_blocks : int = 4 , cond_shape : Optional [ Tuple [ int ]] = None , * , key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : input_shape : The shape of the input data. n_flow_layers : The number of layers in the flow. working_size : The size of the working space. hidden_size : The size of the hidden layers. n_blocks : The number of blocks in the coupling layers. cond_shape : The shape of the conditioning information. key : A jax.random.PRNGKey for initialization Source code in generax/distributions/flow_models.py def __init__ ( self , input_shape : Tuple [ int ], n_flow_layers : int = 3 , working_size : int = 16 , hidden_size : int = 32 , n_blocks : int = 4 , cond_shape : Optional [ Tuple [ int ]] = None , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The shape of the input data. - `n_flow_layers`: The number of layers in the flow. - `working_size`: The size of the working space. - `hidden_size`: The size of the hidden layers. - `n_blocks`: The number of blocks in the coupling layers. - `cond_shape`: The shape of the conditioning information. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" transform = RealNVPTransform ( input_shape = input_shape , n_flow_layers = n_flow_layers , working_size = working_size , hidden_size = hidden_size , n_blocks = n_blocks , cond_shape = cond_shape , key = key ) prior = Gaussian ( input_shape = input_shape ) super () . __init__ ( transform = transform , prior = prior , ** kwargs ) generax.distributions.flow_models.NeuralSpline ( NormalizingFlow ) \u00a4 NeuralSpline( args, *kwargs) Source code in generax/distributions/flow_models.py class NeuralSpline ( NormalizingFlow ): def __init__ ( self , input_shape : Tuple [ int ], n_flow_layers : int = 3 , working_size : int = 16 , hidden_size : int = 32 , n_blocks : int = 4 , n_spline_knots : int = 8 , cond_shape : Optional [ Tuple [ int ]] = None , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The shape of the input data. - `n_flow_layers`: The number of layers in the flow. - `working_size`: The size of the working space. - `hidden_size`: The size of the hidden layers. - `n_blocks`: The number of blocks in the coupling layers. - `cond_shape`: The shape of the conditioning information. - `n_splice_knots`: The number of knots in the spline. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" transform = NeuralSplineTransform ( input_shape = input_shape , n_flow_layers = n_flow_layers , working_size = working_size , hidden_size = hidden_size , n_blocks = n_blocks , n_spline_knots = n_spline_knots , cond_shape = cond_shape , key = key ) prior = Gaussian ( input_shape = input_shape ) super () . __init__ ( transform = transform , prior = prior , ** kwargs ) __init__ ( self , input_shape : Tuple [ int ], n_flow_layers : int = 3 , working_size : int = 16 , hidden_size : int = 32 , n_blocks : int = 4 , n_spline_knots : int = 8 , cond_shape : Optional [ Tuple [ int ]] = None , * , key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : input_shape : The shape of the input data. n_flow_layers : The number of layers in the flow. working_size : The size of the working space. hidden_size : The size of the hidden layers. n_blocks : The number of blocks in the coupling layers. cond_shape : The shape of the conditioning information. n_splice_knots : The number of knots in the spline. key : A jax.random.PRNGKey for initialization Source code in generax/distributions/flow_models.py def __init__ ( self , input_shape : Tuple [ int ], n_flow_layers : int = 3 , working_size : int = 16 , hidden_size : int = 32 , n_blocks : int = 4 , n_spline_knots : int = 8 , cond_shape : Optional [ Tuple [ int ]] = None , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The shape of the input data. - `n_flow_layers`: The number of layers in the flow. - `working_size`: The size of the working space. - `hidden_size`: The size of the hidden layers. - `n_blocks`: The number of blocks in the coupling layers. - `cond_shape`: The shape of the conditioning information. - `n_splice_knots`: The number of knots in the spline. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" transform = NeuralSplineTransform ( input_shape = input_shape , n_flow_layers = n_flow_layers , working_size = working_size , hidden_size = hidden_size , n_blocks = n_blocks , n_spline_knots = n_spline_knots , cond_shape = cond_shape , key = key ) prior = Gaussian ( input_shape = input_shape ) super () . __init__ ( transform = transform , prior = prior , ** kwargs ) generax.distributions.flow_models.ContinuousNormalizingFlow ( NormalizingFlow ) \u00a4 This is FFJORD . Source code in generax/distributions/flow_models.py class ContinuousNormalizingFlow ( NormalizingFlow ): \"\"\"This is [FFJORD](https://arxiv.org/pdf/1810.01367.pdf). \"\"\" def __init__ ( self , input_shape : Tuple [ int ], net : eqx . Module = None , cond_shape : Optional [ Tuple [ int ]] = None , * , controller_rtol : Optional [ float ] = 1e-3 , controller_atol : Optional [ float ] = 1e-5 , trace_estimate_likelihood : Optional [ bool ] = False , adjoint = 'recursive_checkpoint' , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The shape of the input data. - `net`: The neural network to use for the vector field. If None, a default network will be used. `net` should accept `net(t, x, y=y)` - `cond_shape`: The shape of the conditioning information. - `key`: A `jax.random.PRNGKey` for initialization - `controller_rtol`: The relative tolerance for the controller. - `controller_atol`: The absolute tolerance for the controller. - `trace_estimate_likelihood`: Whether or not to use trace estimation for the likelihood. - `adjoint`: The adjoint method to use. See [this](https://docs.kidger.site/diffrax/api/adjoints/) \"\"\" transform = FFJORDTransform ( input_shape = input_shape , net = net , cond_shape = cond_shape , key = key , controller_rtol = controller_rtol , controller_atol = controller_atol , trace_estimate_likelihood = trace_estimate_likelihood , adjoint = adjoint , ** kwargs ) prior = Gaussian ( input_shape = input_shape ) super () . __init__ ( transform = transform , prior = prior , ** kwargs ) @property def vector_field ( self ): \"\"\"Get the vector field function that samples evolve on as t changes. This is an `eqx.Module` that with the signature `vector_field(t, x, y=y) -> dx/dt`.\"\"\" return self . transform . vector_field @property def net ( self ): \"\"\"Same as `vector_field`\"\"\" return self . vector_field def sample ( self , key : PRNGKeyArray , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"**Arguments**: - `key`: The random number generator key. **Returns**: Samples from the model \"\"\" z = self . prior . sample ( key ) x , _ = self . transform ( z , y = y , inverse = True , log_likelihood = False , ** kwargs ) return x vector_field property readonly \u00a4 Get the vector field function that samples evolve on as t changes. This is an eqx.Module that with the signature vector_field(t, x, y=y) -> dx/dt . net property readonly \u00a4 Same as vector_field __init__ ( self , input_shape : Tuple [ int ], net : Module = None , cond_shape : Optional [ Tuple [ int ]] = None , * , controller_rtol : Optional [ float ] = 0.001 , controller_atol : Optional [ float ] = 1e-05 , trace_estimate_likelihood : Optional [ bool ] = False , adjoint = 'recursive_checkpoint' , key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : input_shape : The shape of the input data. net : The neural network to use for the vector field. If None, a default network will be used. net should accept net(t, x, y=y) cond_shape : The shape of the conditioning information. key : A jax.random.PRNGKey for initialization controller_rtol : The relative tolerance for the controller. controller_atol : The absolute tolerance for the controller. trace_estimate_likelihood : Whether or not to use trace estimation for the likelihood. adjoint : The adjoint method to use. See this Source code in generax/distributions/flow_models.py def __init__ ( self , input_shape : Tuple [ int ], net : eqx . Module = None , cond_shape : Optional [ Tuple [ int ]] = None , * , controller_rtol : Optional [ float ] = 1e-3 , controller_atol : Optional [ float ] = 1e-5 , trace_estimate_likelihood : Optional [ bool ] = False , adjoint = 'recursive_checkpoint' , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The shape of the input data. - `net`: The neural network to use for the vector field. If None, a default network will be used. `net` should accept `net(t, x, y=y)` - `cond_shape`: The shape of the conditioning information. - `key`: A `jax.random.PRNGKey` for initialization - `controller_rtol`: The relative tolerance for the controller. - `controller_atol`: The absolute tolerance for the controller. - `trace_estimate_likelihood`: Whether or not to use trace estimation for the likelihood. - `adjoint`: The adjoint method to use. See [this](https://docs.kidger.site/diffrax/api/adjoints/) \"\"\" transform = FFJORDTransform ( input_shape = input_shape , net = net , cond_shape = cond_shape , key = key , controller_rtol = controller_rtol , controller_atol = controller_atol , trace_estimate_likelihood = trace_estimate_likelihood , adjoint = adjoint , ** kwargs ) prior = Gaussian ( input_shape = input_shape ) super () . __init__ ( transform = transform , prior = prior , ** kwargs )","title":"Flows"},{"location":"api/distributions/flow_models/#flows","text":"","title":"Flows"},{"location":"api/distributions/flow_models/#generax.distributions.flow_models.NormalizingFlow","text":"A normalizing flow is a model that we use to represent probability distributions. See this for an overview. Atributes : transform : A BijectiveTransform object that transforms a variable from the base space to the data space and also computes the change is log pdf. transform(x) -> (z,log_det) : Apply the transformation to the input. prior : The prior probability distribution. Methods : to_base_space(x) -> z : Transform a point from the data space to the base space. sample_and_log_prob(key) -> (x,log_px) : Sample from the distribution and compute the log probability. sample(key) -> x : Pull a single sample from the model log_prob(x) -> log_px : Compute the log probability of a point under the model Source code in generax/distributions/flow_models.py class NormalizingFlow ( ProbabilityDistribution , ABC ): \"\"\"A normalizing flow is a model that we use to represent probability distributions. See [this](https://arxiv.org/pdf/1912.02762.pdf) for an overview. **Atributes**: - `transform`: A `BijectiveTransform` object that transforms a variable from the base space to the data space and also computes the change is log pdf. - `transform(x) -> (z,log_det)`: Apply the transformation to the input. - `prior`: The prior probability distribution. **Methods**: - `to_base_space(x) -> z`: Transform a point from the data space to the base space. - `sample_and_log_prob(key) -> (x,log_px)`: Sample from the distribution and compute the log probability. - `sample(key) -> x`: Pull a single sample from the model - `log_prob(x) -> log_px`: Compute the log probability of a point under the model \"\"\" transform : BijectiveTransform prior : ProbabilityDistribution def __init__ ( self , transform : BijectiveTransform , prior : ProbabilityDistribution , ** kwargs ): \"\"\"**Arguments**: - `transform`: A bijective transformation - `prior`: The prior distribution \"\"\" self . transform = transform self . prior = prior input_shape = self . transform . input_shape super () . __init__ ( input_shape = input_shape , ** kwargs ) def to_base_space ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: A JAX array with shape `(dim,)`. - `y`: The conditioning information **Returns**: A JAX array with shape `(dim,)`. \"\"\" return self . transform ( x , y = y , ** kwargs )[ 0 ] def to_data_space ( self , z : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"**Arguments**: - `z`: A JAX array with shape `(dim,)`. - `y`: The conditioning information **Returns**: A JAX array with shape `(dim,)`. \"\"\" return self . transform ( z , y = y , inverse = True , ** kwargs )[ 0 ] def sample_and_log_prob ( self , key : PRNGKeyArray , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"**Arguments**: - `key`: The random number generator key. - `y`: The conditioning information **Returns**: A single sample from the model. Use vmap to get more samples. \"\"\" z , log_pz = self . prior . sample_and_log_prob ( key ) x , log_det = self . transform ( z , y = y , inverse = True , ** kwargs ) # The log determinant of the inverse transform has a negative sign! return x , log_pz - log_det def log_prob ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The point we want to compute logp(x) at. - `y`: The conditioning information **Returns**: The log likelihood of x under the model. \"\"\" z , log_det = self . transform ( x , y = y , ** kwargs ) log_pz = self . prior . log_prob ( z ) return log_pz + log_det def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on a batch of data. This is one of the few times that $x$ is expected to be batched. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new flow with the parameters initialized. \"\"\" new_layer = self . transform . data_dependent_init ( x , y = y , key = key ) # Turn the new parameters into a new module get_transform = lambda tree : tree . transform return eqx . tree_at ( get_transform , self , new_layer )","title":"NormalizingFlow"},{"location":"api/distributions/flow_models/#generax.distributions.flow_models.RealNVP","text":"RealNVP( args, *kwargs) Source code in generax/distributions/flow_models.py class RealNVP ( NormalizingFlow ): def __init__ ( self , input_shape : Tuple [ int ], n_flow_layers : int = 3 , working_size : int = 16 , hidden_size : int = 32 , n_blocks : int = 4 , cond_shape : Optional [ Tuple [ int ]] = None , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The shape of the input data. - `n_flow_layers`: The number of layers in the flow. - `working_size`: The size of the working space. - `hidden_size`: The size of the hidden layers. - `n_blocks`: The number of blocks in the coupling layers. - `cond_shape`: The shape of the conditioning information. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" transform = RealNVPTransform ( input_shape = input_shape , n_flow_layers = n_flow_layers , working_size = working_size , hidden_size = hidden_size , n_blocks = n_blocks , cond_shape = cond_shape , key = key ) prior = Gaussian ( input_shape = input_shape ) super () . __init__ ( transform = transform , prior = prior , ** kwargs )","title":"RealNVP"},{"location":"api/distributions/flow_models/#generax.distributions.flow_models.NeuralSpline","text":"NeuralSpline( args, *kwargs) Source code in generax/distributions/flow_models.py class NeuralSpline ( NormalizingFlow ): def __init__ ( self , input_shape : Tuple [ int ], n_flow_layers : int = 3 , working_size : int = 16 , hidden_size : int = 32 , n_blocks : int = 4 , n_spline_knots : int = 8 , cond_shape : Optional [ Tuple [ int ]] = None , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The shape of the input data. - `n_flow_layers`: The number of layers in the flow. - `working_size`: The size of the working space. - `hidden_size`: The size of the hidden layers. - `n_blocks`: The number of blocks in the coupling layers. - `cond_shape`: The shape of the conditioning information. - `n_splice_knots`: The number of knots in the spline. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" transform = NeuralSplineTransform ( input_shape = input_shape , n_flow_layers = n_flow_layers , working_size = working_size , hidden_size = hidden_size , n_blocks = n_blocks , n_spline_knots = n_spline_knots , cond_shape = cond_shape , key = key ) prior = Gaussian ( input_shape = input_shape ) super () . __init__ ( transform = transform , prior = prior , ** kwargs )","title":"NeuralSpline"},{"location":"api/distributions/flow_models/#generax.distributions.flow_models.ContinuousNormalizingFlow","text":"This is FFJORD . Source code in generax/distributions/flow_models.py class ContinuousNormalizingFlow ( NormalizingFlow ): \"\"\"This is [FFJORD](https://arxiv.org/pdf/1810.01367.pdf). \"\"\" def __init__ ( self , input_shape : Tuple [ int ], net : eqx . Module = None , cond_shape : Optional [ Tuple [ int ]] = None , * , controller_rtol : Optional [ float ] = 1e-3 , controller_atol : Optional [ float ] = 1e-5 , trace_estimate_likelihood : Optional [ bool ] = False , adjoint = 'recursive_checkpoint' , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The shape of the input data. - `net`: The neural network to use for the vector field. If None, a default network will be used. `net` should accept `net(t, x, y=y)` - `cond_shape`: The shape of the conditioning information. - `key`: A `jax.random.PRNGKey` for initialization - `controller_rtol`: The relative tolerance for the controller. - `controller_atol`: The absolute tolerance for the controller. - `trace_estimate_likelihood`: Whether or not to use trace estimation for the likelihood. - `adjoint`: The adjoint method to use. See [this](https://docs.kidger.site/diffrax/api/adjoints/) \"\"\" transform = FFJORDTransform ( input_shape = input_shape , net = net , cond_shape = cond_shape , key = key , controller_rtol = controller_rtol , controller_atol = controller_atol , trace_estimate_likelihood = trace_estimate_likelihood , adjoint = adjoint , ** kwargs ) prior = Gaussian ( input_shape = input_shape ) super () . __init__ ( transform = transform , prior = prior , ** kwargs ) @property def vector_field ( self ): \"\"\"Get the vector field function that samples evolve on as t changes. This is an `eqx.Module` that with the signature `vector_field(t, x, y=y) -> dx/dt`.\"\"\" return self . transform . vector_field @property def net ( self ): \"\"\"Same as `vector_field`\"\"\" return self . vector_field def sample ( self , key : PRNGKeyArray , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"**Arguments**: - `key`: The random number generator key. **Returns**: Samples from the model \"\"\" z = self . prior . sample ( key ) x , _ = self . transform ( z , y = y , inverse = True , log_likelihood = False , ** kwargs ) return x","title":"ContinuousNormalizingFlow"},{"location":"api/flows/affine/","text":"Affine \u00a4 generax.flows.affine.ShiftScale ( BijectiveTransform ) \u00a4 This represents a shift and scale transformation. This is RealNVP https://arxiv.org/pdf/1605.08803.pdf when used in a coupling layer. Attributes : - s_unbounded : The unbounded scaling parameter. - b : The shift parameter. Source code in generax/flows/affine.py class ShiftScale ( BijectiveTransform ): \"\"\"This represents a shift and scale transformation. This is RealNVP https://arxiv.org/pdf/1605.08803.pdf when used in a coupling layer. **Attributes**: - `s_unbounded`: The unbounded scaling parameter. - `b`: The shift parameter. \"\"\" s_unbounded : Array b : Array def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) # Initialize the parameters randomly self . s_unbounded , self . b = random . normal ( key , shape = ( 2 ,) + input_shape ) def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'x must be batched' mean , std = misc . mean_and_std ( x , axis = 0 ) std += 1e-4 # Initialize the parameters so that z will have # zero mean and unit variance b = mean s_unbounded = std - 1 / std # Turn the new parameters into a new module get_b = lambda tree : tree . b get_s_unbounded = lambda tree : tree . s_unbounded updated_layer = eqx . tree_at ( get_b , self , b ) updated_layer = eqx . tree_at ( get_s_unbounded , updated_layer , s_unbounded ) return updated_layer def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' # s must be strictly positive s = misc . square_plus ( self . s_unbounded , gamma = 1.0 ) + 1e-4 log_s = jnp . log ( s ) if inverse == False : z = ( x - self . b ) / s else : z = x * s + self . b if inverse == False : log_det = - log_s . sum () else : log_det = log_s . sum () return z , log_det __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. key : A jax.random.PRNGKey for initialization Source code in generax/flows/affine.py def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) # Initialize the parameters randomly self . s_unbounded , self . b = random . normal ( key , shape = ( 2 ,) + input_shape ) data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform \u00a4 Initialize the parameters of the layer based on the data. Arguments : x : The data to initialize the parameters with. y : The conditioning information key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/flows/affine.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'x must be batched' mean , std = misc . mean_and_std ( x , axis = 0 ) std += 1e-4 # Initialize the parameters so that z will have # zero mean and unit variance b = mean s_unbounded = std - 1 / std # Turn the new parameters into a new module get_b = lambda tree : tree . b get_s_unbounded = lambda tree : tree . s_unbounded updated_layer = eqx . tree_at ( get_b , self , b ) updated_layer = eqx . tree_at ( get_s_unbounded , updated_layer , s_unbounded ) return updated_layer __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation Returns : (z, log_det) Source code in generax/flows/affine.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' # s must be strictly positive s = misc . square_plus ( self . s_unbounded , gamma = 1.0 ) + 1e-4 log_s = jnp . log ( s ) if inverse == False : z = ( x - self . b ) / s else : z = x * s + self . b if inverse == False : log_det = - log_s . sum () else : log_det = log_s . sum () return z , log_det inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Inherited from generax.flows.base.BijectiveTransform.inverse . Source code in generax/flows/affine.py def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs ) generax.flows.affine.DenseLinear ( BijectiveTransform ) \u00a4 Multiply the last axis by a dense matrix. When applied to images, this is GLOW https://arxiv.org/pdf/1807.03039.pdf Attributes : - W : The weight matrix Source code in generax/flows/affine.py class DenseLinear ( BijectiveTransform ): \"\"\"Multiply the last axis by a dense matrix. When applied to images, this is GLOW https://arxiv.org/pdf/1807.03039.pdf **Attributes**: - `W`: The weight matrix \"\"\" W : Array def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) dim = self . input_shape [ - 1 ] self . W = random . normal ( key , shape = ( dim , dim )) self . W = misc . whiten ( self . W ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : z = jnp . einsum ( 'ij,...j->...i' , self . W , x ) else : W_inv = jnp . linalg . inv ( self . W ) z = jnp . einsum ( 'ij,...j->...i' , W_inv , x ) # Need to multiply the log determinant by the number of times # that we're applying the transformation. if len ( self . input_shape ) > 1 : dim_mult = np . prod ( self . input_shape [: - 1 ]) else : dim_mult = 1 log_det = jnp . linalg . slogdet ( self . W )[ 1 ] * dim_mult if inverse : log_det *= - 1 return z , log_det data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) \u00a4 Inherited from generax.flows.base.BijectiveTransform.data_dependent_init . Source code in generax/flows/affine.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ): \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" return self inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Inherited from generax.flows.base.BijectiveTransform.inverse . Source code in generax/flows/affine.py def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs ) __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. key : A jax.random.PRNGKey for initialization Source code in generax/flows/affine.py def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) dim = self . input_shape [ - 1 ] self . W = random . normal ( key , shape = ( dim , dim )) self . W = misc . whiten ( self . W ) __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Implements generax.flows.base.BijectiveTransform.__call__ . Source code in generax/flows/affine.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : z = jnp . einsum ( 'ij,...j->...i' , self . W , x ) else : W_inv = jnp . linalg . inv ( self . W ) z = jnp . einsum ( 'ij,...j->...i' , W_inv , x ) # Need to multiply the log determinant by the number of times # that we're applying the transformation. if len ( self . input_shape ) > 1 : dim_mult = np . prod ( self . input_shape [: - 1 ]) else : dim_mult = 1 log_det = jnp . linalg . slogdet ( self . W )[ 1 ] * dim_mult if inverse : log_det *= - 1 return z , log_det generax.flows.affine.DenseAffine ( BijectiveTransform ) \u00a4 Multiply the last axis by a dense matrix. When applied to images, this is GLOW https://arxiv.org/pdf/1807.03039.pdf Attributes : - W : The weight matrix - b : The bias vector Source code in generax/flows/affine.py class DenseAffine ( BijectiveTransform ): \"\"\"Multiply the last axis by a dense matrix. When applied to images, this is GLOW https://arxiv.org/pdf/1807.03039.pdf **Attributes**: - `W`: The weight matrix - `b`: The bias vector \"\"\" W : DenseLinear b : Array def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) self . W = DenseLinear ( input_shape = input_shape , key = key , ** kwargs ) self . b = jnp . zeros ( input_shape ) def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'x must be batched' b = - jnp . mean ( x , axis = 0 ) return eqx . tree_at ( lambda tree : tree . b , self , b ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : x = x + self . b z , log_det = self . W ( x , y = y , inverse = False ) else : z , log_det = self . W ( x , y = y , inverse = True ) z = z - self . b return z , log_det inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Inherited from generax.flows.base.BijectiveTransform.inverse . Source code in generax/flows/affine.py def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs ) __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. key : A jax.random.PRNGKey for initialization Source code in generax/flows/affine.py def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) self . W = DenseLinear ( input_shape = input_shape , key = key , ** kwargs ) self . b = jnp . zeros ( input_shape ) data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform \u00a4 Initialize the parameters of the layer based on the data. Arguments : x : The data to initialize the parameters with. y : The conditioning information key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/flows/affine.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'x must be batched' b = - jnp . mean ( x , axis = 0 ) return eqx . tree_at ( lambda tree : tree . b , self , b ) __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Implements generax.flows.base.BijectiveTransform.__call__ . Source code in generax/flows/affine.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : x = x + self . b z , log_det = self . W ( x , y = y , inverse = False ) else : z , log_det = self . W ( x , y = y , inverse = True ) z = z - self . b return z , log_det generax.flows.affine.CaleyOrthogonalMVP ( BijectiveTransform ) \u00a4 Caley transform parametrization of an orthogonal matrix. This performs a matrix vector product with an orthogonal matrix. Attributes : - W : The weight matrix - b : The bias vector Source code in generax/flows/affine.py class CaleyOrthogonalMVP ( BijectiveTransform ): \"\"\"Caley transform parametrization of an orthogonal matrix. This performs a matrix vector product with an orthogonal matrix. **Attributes**: - `W`: The weight matrix - `b`: The bias vector \"\"\" W : Array b : Array def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) dim = self . input_shape [ - 1 ] self . W = random . normal ( key , shape = ( dim , dim )) self . b = jnp . zeros ( input_shape ) def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : assert x . shape [ 1 :] == self . input_shape , 'x must be batched' b = - jnp . mean ( x , axis = 0 ) return eqx . tree_at ( lambda tree : tree . b , self , b ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' A = self . W - self . W . T dim = self . input_shape [ - 1 ] # So that we can multiply with channel dim of images @partial ( jnp . vectorize , signature = '(i,j),(j)->(i)' ) def matmul ( A , x ): return A @x if inverse == False : x += self . b IpA_inv = jnp . linalg . inv ( jnp . eye ( dim ) + A ) y = matmul ( IpA_inv , x ) z = y - matmul ( A , y ) else : ImA_inv = jnp . linalg . inv ( jnp . eye ( dim ) - A ) y = matmul ( ImA_inv , x ) z = y + matmul ( A , y ) z -= self . b log_det = jnp . zeros ( 1 ) return z , log_det inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Inherited from generax.flows.base.BijectiveTransform.inverse . Source code in generax/flows/affine.py def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs ) __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. key : A jax.random.PRNGKey for initialization Source code in generax/flows/affine.py def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) dim = self . input_shape [ - 1 ] self . W = random . normal ( key , shape = ( dim , dim )) self . b = jnp . zeros ( input_shape ) data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform \u00a4 Initialize the parameters of the layer based on the data. Arguments : x : The data to initialize the parameters with. y : The conditioning information key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/flows/affine.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : assert x . shape [ 1 :] == self . input_shape , 'x must be batched' b = - jnp . mean ( x , axis = 0 ) return eqx . tree_at ( lambda tree : tree . b , self , b ) __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Implements generax.flows.base.BijectiveTransform.__call__ . Source code in generax/flows/affine.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' A = self . W - self . W . T dim = self . input_shape [ - 1 ] # So that we can multiply with channel dim of images @partial ( jnp . vectorize , signature = '(i,j),(j)->(i)' ) def matmul ( A , x ): return A @x if inverse == False : x += self . b IpA_inv = jnp . linalg . inv ( jnp . eye ( dim ) + A ) y = matmul ( IpA_inv , x ) z = y - matmul ( A , y ) else : ImA_inv = jnp . linalg . inv ( jnp . eye ( dim ) - A ) y = matmul ( ImA_inv , x ) z = y + matmul ( A , y ) z -= self . b log_det = jnp . zeros ( 1 ) return z , log_det generax.flows.affine.PLUAffine ( BijectiveTransform ) \u00a4 Multiply the last axis by a matrix that is parametrized using the LU decomposition. This is more efficient than the dense parametrization Attributes : - A : The weight matrix components. The top half is the upper triangular matrix, and the bottom half is the lower triangular matrix and the diagonal is ignored. - b : The bias vector Source code in generax/flows/affine.py class PLUAffine ( BijectiveTransform ): \"\"\"Multiply the last axis by a matrix that is parametrized using the LU decomposition. This is more efficient than the dense parametrization **Attributes**: - `A`: The weight matrix components. The top half is the upper triangular matrix, and the bottom half is the lower triangular matrix and the diagonal is ignored. - `b`: The bias vector \"\"\" A : Array b : Array def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) # Initialize so that this will be approximately the identity matrix dim = input_shape [ - 1 ] self . A = random . normal ( key , shape = ( dim , dim )) * 0.01 self . A = self . A . at [ jnp . arange ( dim ), jnp . arange ( dim )] . set ( 1.0 ) self . b = jnp . zeros ( input_shape ) def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'x must be batched' b = - jnp . mean ( x , axis = 0 ) return eqx . tree_at ( lambda tree : tree . b , self , b ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' dim = x . shape [ - 1 ] mask = jnp . ones (( dim , dim ), dtype = bool ) upper_mask = jnp . triu ( mask ) lower_mask = jnp . tril ( mask , k =- 1 ) if inverse == False : x += self . b z = jnp . einsum ( \"ij,...j->...i\" , self . A * upper_mask , x ) z = jnp . einsum ( \"ij,...j->...i\" , self . A * lower_mask , z ) + z else : # vmap in order to handle images L_solve_vmap = L_solve U_solve_vmap = U_solve_with_diag for _ in x . shape [: - 1 ]: L_solve_vmap = jax . vmap ( L_solve_vmap , in_axes = ( None , 0 )) U_solve_vmap = jax . vmap ( U_solve_vmap , in_axes = ( None , 0 )) z = L_solve_vmap ( self . A * lower_mask , x ) z = U_solve_vmap ( self . A * upper_mask , z ) z -= self . b log_det = jnp . log ( jnp . abs ( jnp . diag ( self . A ))) . sum () * misc . list_prod ( x . shape [: - 1 ]) if inverse : log_det *= - 1 return z , log_det inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Inherited from generax.flows.base.BijectiveTransform.inverse . Source code in generax/flows/affine.py def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs ) __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. key : A jax.random.PRNGKey for initialization Source code in generax/flows/affine.py def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) # Initialize so that this will be approximately the identity matrix dim = input_shape [ - 1 ] self . A = random . normal ( key , shape = ( dim , dim )) * 0.01 self . A = self . A . at [ jnp . arange ( dim ), jnp . arange ( dim )] . set ( 1.0 ) self . b = jnp . zeros ( input_shape ) data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform \u00a4 Initialize the parameters of the layer based on the data. Arguments : x : The data to initialize the parameters with. y : The conditioning information key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/flows/affine.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'x must be batched' b = - jnp . mean ( x , axis = 0 ) return eqx . tree_at ( lambda tree : tree . b , self , b ) __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Implements generax.flows.base.BijectiveTransform.__call__ . Source code in generax/flows/affine.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' dim = x . shape [ - 1 ] mask = jnp . ones (( dim , dim ), dtype = bool ) upper_mask = jnp . triu ( mask ) lower_mask = jnp . tril ( mask , k =- 1 ) if inverse == False : x += self . b z = jnp . einsum ( \"ij,...j->...i\" , self . A * upper_mask , x ) z = jnp . einsum ( \"ij,...j->...i\" , self . A * lower_mask , z ) + z else : # vmap in order to handle images L_solve_vmap = L_solve U_solve_vmap = U_solve_with_diag for _ in x . shape [: - 1 ]: L_solve_vmap = jax . vmap ( L_solve_vmap , in_axes = ( None , 0 )) U_solve_vmap = jax . vmap ( U_solve_vmap , in_axes = ( None , 0 )) z = L_solve_vmap ( self . A * lower_mask , x ) z = U_solve_vmap ( self . A * upper_mask , z ) z -= self . b log_det = jnp . log ( jnp . abs ( jnp . diag ( self . A ))) . sum () * misc . list_prod ( x . shape [: - 1 ]) if inverse : log_det *= - 1 return z , log_det generax.flows.affine.ConditionalOptionalTransport ( TimeDependentBijectiveTransform ) \u00a4 Given x1, compute f(t, x0) = t x1 + (1-t) x0. This is the optimal transport map between the two points. Used in flow matching https://arxiv.org/pdf/2210.02747.pdf Non-inverse mode goes t -> 0 while inverse mode goes t -> 1. Attributes : Source code in generax/flows/affine.py class ConditionalOptionalTransport ( TimeDependentBijectiveTransform ): \"\"\"Given x1, compute f(t, x0) = t*x1 + (1-t)*x0. This is the optimal transport map between the two points. Used in flow matching https://arxiv.org/pdf/2210.02747.pdf Non-inverse mode goes t -> 0 while inverse mode goes t -> 1. **Attributes**: \"\"\" def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) def __call__ ( self , t : Array , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `t`: The time point. - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to invert the transformation (0 -> t) **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if y is None : raise ValueError ( f 'Expected a conditional input' ) if y . shape != x . shape : raise ValueError ( f 'Expected y.shape ( { y . shape } ) to match x.shape ( { x . shape } )' ) x1 = y if inverse : x0 = x xt = ( 1 - t ) * x0 + t * x1 log_det = jnp . log ( 1 - t ) return xt , log_det else : xt = x x0 = ( xt - t * x1 ) / ( 1 - t ) log_det = - jnp . log ( 1 - t ) return x0 , log_det def vector_field ( self , t : Array , xt : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"The vector field that samples evolve on as t changes **Arguments**: - `t`: Time. - `x0`: A point in the base space. - `y`: The conditioning information. **Returns**: The vector field that samples evolve on at (t, x). \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if y is None : raise ValueError ( f 'Expected a conditional input' ) if y . shape != x . shape : raise ValueError ( f 'Expected y.shape ( { y . shape } ) to match x.shape ( { x . shape } )' ) return y - x data_dependent_init ( self , t : Array , xt : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) \u00a4 Inherited from generax.flows.base.TimeDependentBijectiveTransform.data_dependent_init . Source code in generax/flows/affine.py def data_dependent_init ( self , t : Array , xt : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ): \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `t`: Time. - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" return self inverse ( self , t : Array , x0 : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Inherited from generax.flows.base.TimeDependentBijectiveTransform.inverse . Source code in generax/flows/affine.py def inverse ( self , t : Array , x0 : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (xt, log_det) \"\"\" return self ( t , x0 , y = y , inverse = True , ** kwargs ) __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. key : A jax.random.PRNGKey for initialization Source code in generax/flows/affine.py def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) __call__ ( self , t : Array , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Arguments : t : The time point. x : The input to the transformation y : The conditioning information inverse : Whether to invert the transformation (0 -> t) Returns : (z, log_det) Source code in generax/flows/affine.py def __call__ ( self , t : Array , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `t`: The time point. - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to invert the transformation (0 -> t) **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if y is None : raise ValueError ( f 'Expected a conditional input' ) if y . shape != x . shape : raise ValueError ( f 'Expected y.shape ( { y . shape } ) to match x.shape ( { x . shape } )' ) x1 = y if inverse : x0 = x xt = ( 1 - t ) * x0 + t * x1 log_det = jnp . log ( 1 - t ) return xt , log_det else : xt = x x0 = ( xt - t * x1 ) / ( 1 - t ) log_det = - jnp . log ( 1 - t ) return x0 , log_det vector_field ( self , t : Array , xt : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 The vector field that samples evolve on as t changes Arguments : t : Time. x0 : A point in the base space. y : The conditioning information. Returns : The vector field that samples evolve on at (t, x). Source code in generax/flows/affine.py def vector_field ( self , t : Array , xt : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"The vector field that samples evolve on as t changes **Arguments**: - `t`: Time. - `x0`: A point in the base space. - `y`: The conditioning information. **Returns**: The vector field that samples evolve on at (t, x). \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if y is None : raise ValueError ( f 'Expected a conditional input' ) if y . shape != x . shape : raise ValueError ( f 'Expected y.shape ( { y . shape } ) to match x.shape ( { x . shape } )' ) return y - x","title":"Affine"},{"location":"api/flows/affine/#affine","text":"","title":"Affine"},{"location":"api/flows/affine/#generax.flows.affine.ShiftScale","text":"This represents a shift and scale transformation. This is RealNVP https://arxiv.org/pdf/1605.08803.pdf when used in a coupling layer. Attributes : - s_unbounded : The unbounded scaling parameter. - b : The shift parameter. Source code in generax/flows/affine.py class ShiftScale ( BijectiveTransform ): \"\"\"This represents a shift and scale transformation. This is RealNVP https://arxiv.org/pdf/1605.08803.pdf when used in a coupling layer. **Attributes**: - `s_unbounded`: The unbounded scaling parameter. - `b`: The shift parameter. \"\"\" s_unbounded : Array b : Array def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) # Initialize the parameters randomly self . s_unbounded , self . b = random . normal ( key , shape = ( 2 ,) + input_shape ) def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'x must be batched' mean , std = misc . mean_and_std ( x , axis = 0 ) std += 1e-4 # Initialize the parameters so that z will have # zero mean and unit variance b = mean s_unbounded = std - 1 / std # Turn the new parameters into a new module get_b = lambda tree : tree . b get_s_unbounded = lambda tree : tree . s_unbounded updated_layer = eqx . tree_at ( get_b , self , b ) updated_layer = eqx . tree_at ( get_s_unbounded , updated_layer , s_unbounded ) return updated_layer def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' # s must be strictly positive s = misc . square_plus ( self . s_unbounded , gamma = 1.0 ) + 1e-4 log_s = jnp . log ( s ) if inverse == False : z = ( x - self . b ) / s else : z = x * s + self . b if inverse == False : log_det = - log_s . sum () else : log_det = log_s . sum () return z , log_det","title":"ShiftScale"},{"location":"api/flows/affine/#generax.flows.affine.DenseLinear","text":"Multiply the last axis by a dense matrix. When applied to images, this is GLOW https://arxiv.org/pdf/1807.03039.pdf Attributes : - W : The weight matrix Source code in generax/flows/affine.py class DenseLinear ( BijectiveTransform ): \"\"\"Multiply the last axis by a dense matrix. When applied to images, this is GLOW https://arxiv.org/pdf/1807.03039.pdf **Attributes**: - `W`: The weight matrix \"\"\" W : Array def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) dim = self . input_shape [ - 1 ] self . W = random . normal ( key , shape = ( dim , dim )) self . W = misc . whiten ( self . W ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : z = jnp . einsum ( 'ij,...j->...i' , self . W , x ) else : W_inv = jnp . linalg . inv ( self . W ) z = jnp . einsum ( 'ij,...j->...i' , W_inv , x ) # Need to multiply the log determinant by the number of times # that we're applying the transformation. if len ( self . input_shape ) > 1 : dim_mult = np . prod ( self . input_shape [: - 1 ]) else : dim_mult = 1 log_det = jnp . linalg . slogdet ( self . W )[ 1 ] * dim_mult if inverse : log_det *= - 1 return z , log_det","title":"DenseLinear"},{"location":"api/flows/affine/#generax.flows.affine.DenseAffine","text":"Multiply the last axis by a dense matrix. When applied to images, this is GLOW https://arxiv.org/pdf/1807.03039.pdf Attributes : - W : The weight matrix - b : The bias vector Source code in generax/flows/affine.py class DenseAffine ( BijectiveTransform ): \"\"\"Multiply the last axis by a dense matrix. When applied to images, this is GLOW https://arxiv.org/pdf/1807.03039.pdf **Attributes**: - `W`: The weight matrix - `b`: The bias vector \"\"\" W : DenseLinear b : Array def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) self . W = DenseLinear ( input_shape = input_shape , key = key , ** kwargs ) self . b = jnp . zeros ( input_shape ) def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'x must be batched' b = - jnp . mean ( x , axis = 0 ) return eqx . tree_at ( lambda tree : tree . b , self , b ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : x = x + self . b z , log_det = self . W ( x , y = y , inverse = False ) else : z , log_det = self . W ( x , y = y , inverse = True ) z = z - self . b return z , log_det","title":"DenseAffine"},{"location":"api/flows/affine/#generax.flows.affine.CaleyOrthogonalMVP","text":"Caley transform parametrization of an orthogonal matrix. This performs a matrix vector product with an orthogonal matrix. Attributes : - W : The weight matrix - b : The bias vector Source code in generax/flows/affine.py class CaleyOrthogonalMVP ( BijectiveTransform ): \"\"\"Caley transform parametrization of an orthogonal matrix. This performs a matrix vector product with an orthogonal matrix. **Attributes**: - `W`: The weight matrix - `b`: The bias vector \"\"\" W : Array b : Array def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) dim = self . input_shape [ - 1 ] self . W = random . normal ( key , shape = ( dim , dim )) self . b = jnp . zeros ( input_shape ) def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : assert x . shape [ 1 :] == self . input_shape , 'x must be batched' b = - jnp . mean ( x , axis = 0 ) return eqx . tree_at ( lambda tree : tree . b , self , b ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' A = self . W - self . W . T dim = self . input_shape [ - 1 ] # So that we can multiply with channel dim of images @partial ( jnp . vectorize , signature = '(i,j),(j)->(i)' ) def matmul ( A , x ): return A @x if inverse == False : x += self . b IpA_inv = jnp . linalg . inv ( jnp . eye ( dim ) + A ) y = matmul ( IpA_inv , x ) z = y - matmul ( A , y ) else : ImA_inv = jnp . linalg . inv ( jnp . eye ( dim ) - A ) y = matmul ( ImA_inv , x ) z = y + matmul ( A , y ) z -= self . b log_det = jnp . zeros ( 1 ) return z , log_det","title":"CaleyOrthogonalMVP"},{"location":"api/flows/affine/#generax.flows.affine.PLUAffine","text":"Multiply the last axis by a matrix that is parametrized using the LU decomposition. This is more efficient than the dense parametrization Attributes : - A : The weight matrix components. The top half is the upper triangular matrix, and the bottom half is the lower triangular matrix and the diagonal is ignored. - b : The bias vector Source code in generax/flows/affine.py class PLUAffine ( BijectiveTransform ): \"\"\"Multiply the last axis by a matrix that is parametrized using the LU decomposition. This is more efficient than the dense parametrization **Attributes**: - `A`: The weight matrix components. The top half is the upper triangular matrix, and the bottom half is the lower triangular matrix and the diagonal is ignored. - `b`: The bias vector \"\"\" A : Array b : Array def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) # Initialize so that this will be approximately the identity matrix dim = input_shape [ - 1 ] self . A = random . normal ( key , shape = ( dim , dim )) * 0.01 self . A = self . A . at [ jnp . arange ( dim ), jnp . arange ( dim )] . set ( 1.0 ) self . b = jnp . zeros ( input_shape ) def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'x must be batched' b = - jnp . mean ( x , axis = 0 ) return eqx . tree_at ( lambda tree : tree . b , self , b ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' dim = x . shape [ - 1 ] mask = jnp . ones (( dim , dim ), dtype = bool ) upper_mask = jnp . triu ( mask ) lower_mask = jnp . tril ( mask , k =- 1 ) if inverse == False : x += self . b z = jnp . einsum ( \"ij,...j->...i\" , self . A * upper_mask , x ) z = jnp . einsum ( \"ij,...j->...i\" , self . A * lower_mask , z ) + z else : # vmap in order to handle images L_solve_vmap = L_solve U_solve_vmap = U_solve_with_diag for _ in x . shape [: - 1 ]: L_solve_vmap = jax . vmap ( L_solve_vmap , in_axes = ( None , 0 )) U_solve_vmap = jax . vmap ( U_solve_vmap , in_axes = ( None , 0 )) z = L_solve_vmap ( self . A * lower_mask , x ) z = U_solve_vmap ( self . A * upper_mask , z ) z -= self . b log_det = jnp . log ( jnp . abs ( jnp . diag ( self . A ))) . sum () * misc . list_prod ( x . shape [: - 1 ]) if inverse : log_det *= - 1 return z , log_det","title":"PLUAffine"},{"location":"api/flows/affine/#generax.flows.affine.ConditionalOptionalTransport","text":"Given x1, compute f(t, x0) = t x1 + (1-t) x0. This is the optimal transport map between the two points. Used in flow matching https://arxiv.org/pdf/2210.02747.pdf Non-inverse mode goes t -> 0 while inverse mode goes t -> 1. Attributes : Source code in generax/flows/affine.py class ConditionalOptionalTransport ( TimeDependentBijectiveTransform ): \"\"\"Given x1, compute f(t, x0) = t*x1 + (1-t)*x0. This is the optimal transport map between the two points. Used in flow matching https://arxiv.org/pdf/2210.02747.pdf Non-inverse mode goes t -> 0 while inverse mode goes t -> 1. **Attributes**: \"\"\" def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) def __call__ ( self , t : Array , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `t`: The time point. - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to invert the transformation (0 -> t) **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if y is None : raise ValueError ( f 'Expected a conditional input' ) if y . shape != x . shape : raise ValueError ( f 'Expected y.shape ( { y . shape } ) to match x.shape ( { x . shape } )' ) x1 = y if inverse : x0 = x xt = ( 1 - t ) * x0 + t * x1 log_det = jnp . log ( 1 - t ) return xt , log_det else : xt = x x0 = ( xt - t * x1 ) / ( 1 - t ) log_det = - jnp . log ( 1 - t ) return x0 , log_det def vector_field ( self , t : Array , xt : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"The vector field that samples evolve on as t changes **Arguments**: - `t`: Time. - `x0`: A point in the base space. - `y`: The conditioning information. **Returns**: The vector field that samples evolve on at (t, x). \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if y is None : raise ValueError ( f 'Expected a conditional input' ) if y . shape != x . shape : raise ValueError ( f 'Expected y.shape ( { y . shape } ) to match x.shape ( { x . shape } )' ) return y - x","title":"ConditionalOptionalTransport"},{"location":"api/flows/base/","text":"Base \u00a4 generax.flows.base.BijectiveTransform \u00a4 This represents a bijective transformation. Every bijective transformation only (x, key) on initialization in order to be able to do data dependent initialization. *** x MUST be a batched array in order for the layers to work correctly. *** Atributes : input_shape : The input shape. Output shape will have the same dimensionality as the input. cond_shape : The shape of the conditioning information. If there is no conditioning information, this is None. Source code in generax/flows/base.py class BijectiveTransform ( eqx . Module , ABC ): \"\"\"This represents a bijective transformation. Every bijective transformation only `(x, key)` on initialization in order to be able to do data dependent initialization. *** x MUST be a batched array in order for the layers to work correctly. *** **Atributes**: - `input_shape`: The input shape. Output shape will have the same dimensionality as the input. - `cond_shape`: The shape of the conditioning information. If there is no conditioning information, this is None. \"\"\" input_shape : Tuple [ int ] = eqx . field ( static = True ) cond_shape : Union [ None , Tuple [ int ]] = eqx . field ( static = True ) def __init__ ( self , * _ , input_shape : Tuple [ int ], cond_shape : Union [ None , Tuple [ int ]] = None , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. \"\"\" super () . __init__ ( ** kwargs ) assert isinstance ( input_shape , tuple ) or isinstance ( input_shape , list ) self . input_shape = tuple ( input_shape ) if cond_shape is not None : assert isinstance ( cond_shape , tuple ) or isinstance ( cond_shape , list ) self . cond_shape = tuple ( cond_shape ) else : self . cond_shape = None def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ): \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" return self @abstractmethod def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" pass def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs ) __init__ ( self , * _ , * , input_shape : Tuple [ int ], cond_shape : Optional [ Tuple [ int ]] = None , ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. Source code in generax/flows/base.py def __init__ ( self , * _ , input_shape : Tuple [ int ], cond_shape : Union [ None , Tuple [ int ]] = None , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. \"\"\" super () . __init__ ( ** kwargs ) assert isinstance ( input_shape , tuple ) or isinstance ( input_shape , list ) self . input_shape = tuple ( input_shape ) if cond_shape is not None : assert isinstance ( cond_shape , tuple ) or isinstance ( cond_shape , list ) self . cond_shape = tuple ( cond_shape ) else : self . cond_shape = None data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) \u00a4 Initialize the parameters of the layer based on the data. Arguments : x : The data to initialize the parameters with. y : The conditioning information key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/flows/base.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ): \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" return self __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array abstractmethod \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation Returns : (z, log_det) Source code in generax/flows/base.py @abstractmethod def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" pass inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Apply the inverse transformation. Arguments : x : The input to the transformation y : The conditioning information Returns : (z, log_det) Source code in generax/flows/base.py def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs ) generax.flows.base.TimeDependentBijectiveTransform ( BijectiveTransform ) \u00a4 Time dependent bijective transform. This will help us build simple probability paths. Non-inverse mode goes t -> 0 while inverse mode goes t -> 1. Atributes : input_shape : The input shape. Output shape will have the same dimensionality as the input. cond_shape : The shape of the conditioning information. If there is no conditioning information, this is None. Source code in generax/flows/base.py class TimeDependentBijectiveTransform ( BijectiveTransform ): \"\"\"Time dependent bijective transform. This will help us build simple probability paths. Non-inverse mode goes t -> 0 while inverse mode goes t -> 1. **Atributes**: - `input_shape`: The input shape. Output shape will have the same dimensionality as the input. - `cond_shape`: The shape of the conditioning information. If there is no conditioning information, this is None. \"\"\" def data_dependent_init ( self , t : Array , xt : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ): \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `t`: Time. - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" return self @abstractmethod def __call__ ( self , t : Array , xt : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `xt`: The input to the transformation. If inverse=True, then should be x0 - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (x0, log_det) \"\"\" pass def inverse ( self , t : Array , x0 : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (xt, log_det) \"\"\" return self ( t , x0 , y = y , inverse = True , ** kwargs ) def vector_field ( self , t : Array , xt : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"The vector field that samples evolve on as t changes **Arguments**: - `t`: Time. - `x0`: A point in the base space. - `y`: The conditioning information. **Returns**: The vector field that samples evolve on at (t, x). \"\"\" raise NotImplementedError __init__ ( self , * _ , * , input_shape : Tuple [ int ], cond_shape : Optional [ Tuple [ int ]] = None , ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. Source code in generax/flows/base.py def __init__ ( self , * _ , input_shape : Tuple [ int ], cond_shape : Union [ None , Tuple [ int ]] = None , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. \"\"\" super () . __init__ ( ** kwargs ) assert isinstance ( input_shape , tuple ) or isinstance ( input_shape , list ) self . input_shape = tuple ( input_shape ) if cond_shape is not None : assert isinstance ( cond_shape , tuple ) or isinstance ( cond_shape , list ) self . cond_shape = tuple ( cond_shape ) else : self . cond_shape = None data_dependent_init ( self , t : Array , xt : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) \u00a4 Initialize the parameters of the layer based on the data. Arguments : t : Time. x : The data to initialize the parameters with. y : The conditioning information key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/flows/base.py def data_dependent_init ( self , t : Array , xt : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ): \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `t`: Time. - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" return self __call__ ( self , t : Array , xt : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array abstractmethod \u00a4 Arguments : xt : The input to the transformation. If inverse=True, then should be x0 y : The conditioning information inverse : Whether to inverse the transformation Returns : (x0, log_det) Source code in generax/flows/base.py @abstractmethod def __call__ ( self , t : Array , xt : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `xt`: The input to the transformation. If inverse=True, then should be x0 - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (x0, log_det) \"\"\" pass inverse ( self , t : Array , x0 : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Apply the inverse transformation. Arguments : x : The input to the transformation y : The conditioning information Returns : (xt, log_det) Source code in generax/flows/base.py def inverse ( self , t : Array , x0 : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (xt, log_det) \"\"\" return self ( t , x0 , y = y , inverse = True , ** kwargs ) vector_field ( self , t : Array , xt : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 The vector field that samples evolve on as t changes Arguments : t : Time. x0 : A point in the base space. y : The conditioning information. Returns : The vector field that samples evolve on at (t, x). Source code in generax/flows/base.py def vector_field ( self , t : Array , xt : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"The vector field that samples evolve on as t changes **Arguments**: - `t`: Time. - `x0`: A point in the base space. - `y`: The conditioning information. **Returns**: The vector field that samples evolve on at (t, x). \"\"\" raise NotImplementedError generax.flows.base.Sequential ( BijectiveTransform ) \u00a4 A sequence of bijective transformations. Accepts a sequence of BijectiveTransform initializers. # Intented usage: layer1 = MyTransform ( ... ) layer2 = MyTransform ( ... ) transform = Sequential ( layer1 , layer2 ) Attributes : - n_layers : The number of layers in the composition - layers : A tuple of the layers in the composition Source code in generax/flows/base.py class Sequential ( BijectiveTransform ): \"\"\"A sequence of bijective transformations. Accepts a sequence of `BijectiveTransform` initializers. ```python # Intented usage: layer1 = MyTransform(...) layer2 = MyTransform(...) transform = Sequential(layer1, layer2) ``` **Attributes**: - `n_layers`: The number of layers in the composition - `layers`: A tuple of the layers in the composition \"\"\" n_layers : int = eqx . field ( static = True ) layers : Tuple [ BijectiveTransform ] def __init__ ( self , * layers : Sequence [ BijectiveTransform ], ** kwargs ): \"\"\"**Arguments**: - `layers`: A sequence of `BijectiveTransform`. \"\"\" input_shape = layers [ 0 ] . input_shape cond_shape = layers [ 0 ] . cond_shape # Check that all of the layers have the same cond shape for layer in layers : assert layer . cond_shape == cond_shape super () . __init__ ( input_shape = input_shape , cond_shape = cond_shape , ** kwargs ) self . layers = tuple ( layers ) self . n_layers = len ( layers ) def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' # We need to initialize each of the layers keys = random . split ( key , self . n_layers ) new_layers = [] for i , ( layer , key ) in enumerate ( zip ( self . layers , keys )): new_layer = layer . data_dependent_init ( x = x , y = y , key = key ) new_layers . append ( new_layer ) x , _ = eqx . filter_vmap ( new_layer )( x ) new_layers = tuple ( new_layers ) # Turn the new parameters into a new module get_layers = lambda tree : tree . layers updated_layer = eqx . tree_at ( get_layers , self , new_layers ) return updated_layer def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" delta_logpx = 0.0 layers = reversed ( self . layers ) if inverse else self . layers for layer in layers : x , log_det = layer ( x , y = y , inverse = inverse , ** kwargs ) delta_logpx += log_det return x , delta_logpx def __getitem__ ( self , i : Union [ int , slice ]) -> Callable : if isinstance ( i , int ): return self . layers [ i ] elif isinstance ( i , slice ): return Sequential ( self . layers [ i ]) else : raise TypeError ( f \"Indexing with type { type ( i ) } is not supported\" ) def __iter__ ( self ): yield from self . layers def __len__ ( self ): return len ( self . layers ) inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Inherited from generax.flows.base.BijectiveTransform.inverse . Source code in generax/flows/base.py def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs ) __init__ ( self , * layers : Sequence [ BijectiveTransform ], ** kwargs ) \u00a4 Arguments : layers : A sequence of BijectiveTransform . Source code in generax/flows/base.py def __init__ ( self , * layers : Sequence [ BijectiveTransform ], ** kwargs ): \"\"\"**Arguments**: - `layers`: A sequence of `BijectiveTransform`. \"\"\" input_shape = layers [ 0 ] . input_shape cond_shape = layers [ 0 ] . cond_shape # Check that all of the layers have the same cond shape for layer in layers : assert layer . cond_shape == cond_shape super () . __init__ ( input_shape = input_shape , cond_shape = cond_shape , ** kwargs ) self . layers = tuple ( layers ) self . n_layers = len ( layers ) data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform \u00a4 Initialize the parameters of the layer based on the data. Arguments : x : The data to initialize the parameters with. y : The conditioning information key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/flows/base.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' # We need to initialize each of the layers keys = random . split ( key , self . n_layers ) new_layers = [] for i , ( layer , key ) in enumerate ( zip ( self . layers , keys )): new_layer = layer . data_dependent_init ( x = x , y = y , key = key ) new_layers . append ( new_layer ) x , _ = eqx . filter_vmap ( new_layer )( x ) new_layers = tuple ( new_layers ) # Turn the new parameters into a new module get_layers = lambda tree : tree . layers updated_layer = eqx . tree_at ( get_layers , self , new_layers ) return updated_layer __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Implements generax.flows.base.BijectiveTransform.__call__ . Source code in generax/flows/base.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" delta_logpx = 0.0 layers = reversed ( self . layers ) if inverse else self . layers for layer in layers : x , log_det = layer ( x , y = y , inverse = inverse , ** kwargs ) delta_logpx += log_det return x , delta_logpx generax.flows.reshape.Reverse ( BijectiveTransform ) \u00a4 Reverse an input Source code in generax/flows/reshape.py class Reverse ( BijectiveTransform ): \"\"\"Reverse an input \"\"\" def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: The transformed input and 0 \"\"\" assert x . shape == self . input_shape z = x [ ... , :: - 1 ] log_det = jnp . array ( 0.0 ) return z , log_det __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. key : A jax.random.PRNGKey for initialization Source code in generax/flows/reshape.py def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation Returns : The transformed input and 0 Source code in generax/flows/reshape.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: The transformed input and 0 \"\"\" assert x . shape == self . input_shape z = x [ ... , :: - 1 ] log_det = jnp . array ( 0.0 ) return z , log_det data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) \u00a4 Inherited from generax.flows.base.BijectiveTransform.data_dependent_init . Source code in generax/flows/reshape.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ): \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" return self inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Inherited from generax.flows.base.BijectiveTransform.inverse . Source code in generax/flows/reshape.py def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs )","title":"Base"},{"location":"api/flows/base/#base","text":"","title":"Base"},{"location":"api/flows/base/#generax.flows.base.BijectiveTransform","text":"This represents a bijective transformation. Every bijective transformation only (x, key) on initialization in order to be able to do data dependent initialization. *** x MUST be a batched array in order for the layers to work correctly. *** Atributes : input_shape : The input shape. Output shape will have the same dimensionality as the input. cond_shape : The shape of the conditioning information. If there is no conditioning information, this is None. Source code in generax/flows/base.py class BijectiveTransform ( eqx . Module , ABC ): \"\"\"This represents a bijective transformation. Every bijective transformation only `(x, key)` on initialization in order to be able to do data dependent initialization. *** x MUST be a batched array in order for the layers to work correctly. *** **Atributes**: - `input_shape`: The input shape. Output shape will have the same dimensionality as the input. - `cond_shape`: The shape of the conditioning information. If there is no conditioning information, this is None. \"\"\" input_shape : Tuple [ int ] = eqx . field ( static = True ) cond_shape : Union [ None , Tuple [ int ]] = eqx . field ( static = True ) def __init__ ( self , * _ , input_shape : Tuple [ int ], cond_shape : Union [ None , Tuple [ int ]] = None , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. \"\"\" super () . __init__ ( ** kwargs ) assert isinstance ( input_shape , tuple ) or isinstance ( input_shape , list ) self . input_shape = tuple ( input_shape ) if cond_shape is not None : assert isinstance ( cond_shape , tuple ) or isinstance ( cond_shape , list ) self . cond_shape = tuple ( cond_shape ) else : self . cond_shape = None def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ): \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" return self @abstractmethod def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" pass def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs )","title":"BijectiveTransform"},{"location":"api/flows/base/#generax.flows.base.TimeDependentBijectiveTransform","text":"Time dependent bijective transform. This will help us build simple probability paths. Non-inverse mode goes t -> 0 while inverse mode goes t -> 1. Atributes : input_shape : The input shape. Output shape will have the same dimensionality as the input. cond_shape : The shape of the conditioning information. If there is no conditioning information, this is None. Source code in generax/flows/base.py class TimeDependentBijectiveTransform ( BijectiveTransform ): \"\"\"Time dependent bijective transform. This will help us build simple probability paths. Non-inverse mode goes t -> 0 while inverse mode goes t -> 1. **Atributes**: - `input_shape`: The input shape. Output shape will have the same dimensionality as the input. - `cond_shape`: The shape of the conditioning information. If there is no conditioning information, this is None. \"\"\" def data_dependent_init ( self , t : Array , xt : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ): \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `t`: Time. - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" return self @abstractmethod def __call__ ( self , t : Array , xt : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `xt`: The input to the transformation. If inverse=True, then should be x0 - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (x0, log_det) \"\"\" pass def inverse ( self , t : Array , x0 : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (xt, log_det) \"\"\" return self ( t , x0 , y = y , inverse = True , ** kwargs ) def vector_field ( self , t : Array , xt : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"The vector field that samples evolve on as t changes **Arguments**: - `t`: Time. - `x0`: A point in the base space. - `y`: The conditioning information. **Returns**: The vector field that samples evolve on at (t, x). \"\"\" raise NotImplementedError","title":"TimeDependentBijectiveTransform"},{"location":"api/flows/base/#generax.flows.base.Sequential","text":"A sequence of bijective transformations. Accepts a sequence of BijectiveTransform initializers. # Intented usage: layer1 = MyTransform ( ... ) layer2 = MyTransform ( ... ) transform = Sequential ( layer1 , layer2 ) Attributes : - n_layers : The number of layers in the composition - layers : A tuple of the layers in the composition Source code in generax/flows/base.py class Sequential ( BijectiveTransform ): \"\"\"A sequence of bijective transformations. Accepts a sequence of `BijectiveTransform` initializers. ```python # Intented usage: layer1 = MyTransform(...) layer2 = MyTransform(...) transform = Sequential(layer1, layer2) ``` **Attributes**: - `n_layers`: The number of layers in the composition - `layers`: A tuple of the layers in the composition \"\"\" n_layers : int = eqx . field ( static = True ) layers : Tuple [ BijectiveTransform ] def __init__ ( self , * layers : Sequence [ BijectiveTransform ], ** kwargs ): \"\"\"**Arguments**: - `layers`: A sequence of `BijectiveTransform`. \"\"\" input_shape = layers [ 0 ] . input_shape cond_shape = layers [ 0 ] . cond_shape # Check that all of the layers have the same cond shape for layer in layers : assert layer . cond_shape == cond_shape super () . __init__ ( input_shape = input_shape , cond_shape = cond_shape , ** kwargs ) self . layers = tuple ( layers ) self . n_layers = len ( layers ) def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' # We need to initialize each of the layers keys = random . split ( key , self . n_layers ) new_layers = [] for i , ( layer , key ) in enumerate ( zip ( self . layers , keys )): new_layer = layer . data_dependent_init ( x = x , y = y , key = key ) new_layers . append ( new_layer ) x , _ = eqx . filter_vmap ( new_layer )( x ) new_layers = tuple ( new_layers ) # Turn the new parameters into a new module get_layers = lambda tree : tree . layers updated_layer = eqx . tree_at ( get_layers , self , new_layers ) return updated_layer def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" delta_logpx = 0.0 layers = reversed ( self . layers ) if inverse else self . layers for layer in layers : x , log_det = layer ( x , y = y , inverse = inverse , ** kwargs ) delta_logpx += log_det return x , delta_logpx def __getitem__ ( self , i : Union [ int , slice ]) -> Callable : if isinstance ( i , int ): return self . layers [ i ] elif isinstance ( i , slice ): return Sequential ( self . layers [ i ]) else : raise TypeError ( f \"Indexing with type { type ( i ) } is not supported\" ) def __iter__ ( self ): yield from self . layers def __len__ ( self ): return len ( self . layers )","title":"Sequential"},{"location":"api/flows/base/#generax.flows.reshape.Reverse","text":"Reverse an input Source code in generax/flows/reshape.py class Reverse ( BijectiveTransform ): \"\"\"Reverse an input \"\"\" def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: The transformed input and 0 \"\"\" assert x . shape == self . input_shape z = x [ ... , :: - 1 ] log_det = jnp . array ( 0.0 ) return z , log_det","title":"Reverse"},{"location":"api/flows/compositions/","text":"Models \u00a4 generax.flows.models.GeneralTransform ( Sequential ) \u00a4 GeneralTransform( args, *kwargs) Source code in generax/flows/models.py class GeneralTransform ( Sequential ): def __init__ ( self , TransformType : type , input_shape : Tuple [ int ], n_flow_layers : int = 3 , working_size : int = 16 , hidden_size : int = 32 , n_blocks : int = 4 , filter_shape : Optional [ Tuple [ int ]] = ( 3 , 3 ), cond_shape : Optional [ Tuple [ int ]] = None , * , key : PRNGKeyArray , ** kwargs ): # Build a dummy transfom to get the input and output shapes transform_input_shape , net_input_shape = Coupling . get_split_shapes ( input_shape ) transform = TransformType ( input_shape = transform_input_shape , key = key ) net_output_size = Coupling . get_net_output_shapes ( input_shape , transform ) if len ( net_input_shape ) == 3 : H , W , C = net_input_shape assert net_output_size % ( H * W ) == 0 net_output_size = net_output_size // ( H * W ) def create_net ( key ): return ResNet ( input_shape = net_input_shape , working_size = working_size , hidden_size = hidden_size , out_size = net_output_size , n_blocks = n_blocks , filter_shape = filter_shape , cond_shape = cond_shape , key = key ) layers = [] keys = random . split ( key , n_flow_layers ) for i , k in enumerate ( keys ): k1 , k2 , k3 , k4 = random . split ( k , 4 ) transform = TransformType ( input_shape = transform_input_shape , cond_shape = cond_shape , key = k1 ) layer = Coupling ( transform , create_net ( k2 ), input_shape = input_shape , cond_shape = cond_shape , key = k2 ) layers . append ( layer ) layers . append ( PLUAffine ( input_shape = input_shape , cond_shape = cond_shape , key = k3 )) layers . append ( ShiftScale ( input_shape = input_shape , cond_shape = cond_shape , key = k4 )) super () . __init__ ( * layers , ** kwargs ) __init__ ( self , TransformType : type , input_shape : Tuple [ int ], n_flow_layers : int = 3 , working_size : int = 16 , hidden_size : int = 32 , n_blocks : int = 4 , filter_shape : Optional [ Tuple [ int ]] = ( 3 , 3 ), cond_shape : Optional [ Tuple [ int ]] = None , * , key : PRNGKeyArray , ** kwargs ) \u00a4 Source code in generax/flows/models.py def __init__ ( self , TransformType : type , input_shape : Tuple [ int ], n_flow_layers : int = 3 , working_size : int = 16 , hidden_size : int = 32 , n_blocks : int = 4 , filter_shape : Optional [ Tuple [ int ]] = ( 3 , 3 ), cond_shape : Optional [ Tuple [ int ]] = None , * , key : PRNGKeyArray , ** kwargs ): # Build a dummy transfom to get the input and output shapes transform_input_shape , net_input_shape = Coupling . get_split_shapes ( input_shape ) transform = TransformType ( input_shape = transform_input_shape , key = key ) net_output_size = Coupling . get_net_output_shapes ( input_shape , transform ) if len ( net_input_shape ) == 3 : H , W , C = net_input_shape assert net_output_size % ( H * W ) == 0 net_output_size = net_output_size // ( H * W ) def create_net ( key ): return ResNet ( input_shape = net_input_shape , working_size = working_size , hidden_size = hidden_size , out_size = net_output_size , n_blocks = n_blocks , filter_shape = filter_shape , cond_shape = cond_shape , key = key ) layers = [] keys = random . split ( key , n_flow_layers ) for i , k in enumerate ( keys ): k1 , k2 , k3 , k4 = random . split ( k , 4 ) transform = TransformType ( input_shape = transform_input_shape , cond_shape = cond_shape , key = k1 ) layer = Coupling ( transform , create_net ( k2 ), input_shape = input_shape , cond_shape = cond_shape , key = k2 ) layers . append ( layer ) layers . append ( PLUAffine ( input_shape = input_shape , cond_shape = cond_shape , key = k3 )) layers . append ( ShiftScale ( input_shape = input_shape , cond_shape = cond_shape , key = k4 )) super () . __init__ ( * layers , ** kwargs ) inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Apply the inverse transformation. Arguments : x : The input to the transformation y : The conditioning information Returns : (z, log_det) Source code in generax/flows/models.py def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs ) data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform \u00a4 Inherited from generax.flows.base.Sequential.data_dependent_init . Source code in generax/flows/models.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' # We need to initialize each of the layers keys = random . split ( key , self . n_layers ) new_layers = [] for i , ( layer , key ) in enumerate ( zip ( self . layers , keys )): new_layer = layer . data_dependent_init ( x = x , y = y , key = key ) new_layers . append ( new_layer ) x , _ = eqx . filter_vmap ( new_layer )( x ) new_layers = tuple ( new_layers ) # Turn the new parameters into a new module get_layers = lambda tree : tree . layers updated_layer = eqx . tree_at ( get_layers , self , new_layers ) return updated_layer __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Inherited from generax.flows.base.Sequential.__call__ . Source code in generax/flows/models.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" delta_logpx = 0.0 layers = reversed ( self . layers ) if inverse else self . layers for layer in layers : x , log_det = layer ( x , y = y , inverse = inverse , ** kwargs ) delta_logpx += log_det return x , delta_logpx generax.flows.models.RealNVPTransform ( GeneralTransform ) \u00a4 RealNVPTransform( args, *kwargs) Source code in generax/flows/models.py class RealNVPTransform ( GeneralTransform ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( TransformType = ShiftScale , * args , ** kwargs ) __init__ ( self , * args , ** kwargs ) \u00a4 Source code in generax/flows/models.py def __init__ ( self , * args , ** kwargs ): super () . __init__ ( TransformType = ShiftScale , * args , ** kwargs ) inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Apply the inverse transformation. Arguments : x : The input to the transformation y : The conditioning information Returns : (z, log_det) Source code in generax/flows/models.py def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs ) data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform \u00a4 Initialize the parameters of the layer based on the data. Arguments : x : The data to initialize the parameters with. y : The conditioning information key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/flows/models.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' # We need to initialize each of the layers keys = random . split ( key , self . n_layers ) new_layers = [] for i , ( layer , key ) in enumerate ( zip ( self . layers , keys )): new_layer = layer . data_dependent_init ( x = x , y = y , key = key ) new_layers . append ( new_layer ) x , _ = eqx . filter_vmap ( new_layer )( x ) new_layers = tuple ( new_layers ) # Turn the new parameters into a new module get_layers = lambda tree : tree . layers updated_layer = eqx . tree_at ( get_layers , self , new_layers ) return updated_layer __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation Returns : (z, log_det) Source code in generax/flows/models.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" delta_logpx = 0.0 layers = reversed ( self . layers ) if inverse else self . layers for layer in layers : x , log_det = layer ( x , y = y , inverse = inverse , ** kwargs ) delta_logpx += log_det return x , delta_logpx generax.flows.models.NeuralSplineTransform ( GeneralTransform ) \u00a4 NeuralSplineTransform( args, *kwargs) Source code in generax/flows/models.py class NeuralSplineTransform ( GeneralTransform ): def __init__ ( self , * args , n_spline_knots : int = 8 , ** kwargs ): super () . __init__ ( TransformType = partial ( RationalQuadraticSpline , K = n_spline_knots ), * args , ** kwargs ) inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Apply the inverse transformation. Arguments : x : The input to the transformation y : The conditioning information Returns : (z, log_det) Source code in generax/flows/models.py def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs ) __init__ ( self , * args , * , n_spline_knots : int = 8 , ** kwargs ) \u00a4 Source code in generax/flows/models.py def __init__ ( self , * args , n_spline_knots : int = 8 , ** kwargs ): super () . __init__ ( TransformType = partial ( RationalQuadraticSpline , K = n_spline_knots ), * args , ** kwargs ) data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform \u00a4 Initialize the parameters of the layer based on the data. Arguments : x : The data to initialize the parameters with. y : The conditioning information key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/flows/models.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' # We need to initialize each of the layers keys = random . split ( key , self . n_layers ) new_layers = [] for i , ( layer , key ) in enumerate ( zip ( self . layers , keys )): new_layer = layer . data_dependent_init ( x = x , y = y , key = key ) new_layers . append ( new_layer ) x , _ = eqx . filter_vmap ( new_layer )( x ) new_layers = tuple ( new_layers ) # Turn the new parameters into a new module get_layers = lambda tree : tree . layers updated_layer = eqx . tree_at ( get_layers , self , new_layers ) return updated_layer __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation Returns : (z, log_det) Source code in generax/flows/models.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" delta_logpx = 0.0 layers = reversed ( self . layers ) if inverse else self . layers for layer in layers : x , log_det = layer ( x , y = y , inverse = inverse , ** kwargs ) delta_logpx += log_det return x , delta_logpx","title":"Models"},{"location":"api/flows/compositions/#models","text":"","title":"Models"},{"location":"api/flows/compositions/#generax.flows.models.GeneralTransform","text":"GeneralTransform( args, *kwargs) Source code in generax/flows/models.py class GeneralTransform ( Sequential ): def __init__ ( self , TransformType : type , input_shape : Tuple [ int ], n_flow_layers : int = 3 , working_size : int = 16 , hidden_size : int = 32 , n_blocks : int = 4 , filter_shape : Optional [ Tuple [ int ]] = ( 3 , 3 ), cond_shape : Optional [ Tuple [ int ]] = None , * , key : PRNGKeyArray , ** kwargs ): # Build a dummy transfom to get the input and output shapes transform_input_shape , net_input_shape = Coupling . get_split_shapes ( input_shape ) transform = TransformType ( input_shape = transform_input_shape , key = key ) net_output_size = Coupling . get_net_output_shapes ( input_shape , transform ) if len ( net_input_shape ) == 3 : H , W , C = net_input_shape assert net_output_size % ( H * W ) == 0 net_output_size = net_output_size // ( H * W ) def create_net ( key ): return ResNet ( input_shape = net_input_shape , working_size = working_size , hidden_size = hidden_size , out_size = net_output_size , n_blocks = n_blocks , filter_shape = filter_shape , cond_shape = cond_shape , key = key ) layers = [] keys = random . split ( key , n_flow_layers ) for i , k in enumerate ( keys ): k1 , k2 , k3 , k4 = random . split ( k , 4 ) transform = TransformType ( input_shape = transform_input_shape , cond_shape = cond_shape , key = k1 ) layer = Coupling ( transform , create_net ( k2 ), input_shape = input_shape , cond_shape = cond_shape , key = k2 ) layers . append ( layer ) layers . append ( PLUAffine ( input_shape = input_shape , cond_shape = cond_shape , key = k3 )) layers . append ( ShiftScale ( input_shape = input_shape , cond_shape = cond_shape , key = k4 )) super () . __init__ ( * layers , ** kwargs )","title":"GeneralTransform"},{"location":"api/flows/compositions/#generax.flows.models.RealNVPTransform","text":"RealNVPTransform( args, *kwargs) Source code in generax/flows/models.py class RealNVPTransform ( GeneralTransform ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( TransformType = ShiftScale , * args , ** kwargs )","title":"RealNVPTransform"},{"location":"api/flows/compositions/#generax.flows.models.NeuralSplineTransform","text":"NeuralSplineTransform( args, *kwargs) Source code in generax/flows/models.py class NeuralSplineTransform ( GeneralTransform ): def __init__ ( self , * args , n_spline_knots : int = 8 , ** kwargs ): super () . __init__ ( TransformType = partial ( RationalQuadraticSpline , K = n_spline_knots ), * args , ** kwargs )","title":"NeuralSplineTransform"},{"location":"api/flows/coupling/","text":"Coupling \u00a4 generax.flows.coupling.RavelParameters \u00a4 Flatten and concatenate the parameters of a eqx.Module Source code in generax/flows/coupling.py class RavelParameters ( eqx . Module ): \"\"\"Flatten and concatenate the parameters of a eqx.Module \"\"\" shapes_and_sizes : Sequence [ Tuple [ Tuple [ int ], int ]] = eqx . field ( static = True ) flat_params_size : Tuple [ int ] = eqx . field ( static = True ) static : Any = eqx . field ( static = True ) treedef : Any = eqx . field ( static = True ) indices : np . ndarray = eqx . field ( static = True ) def __init__ ( self , module ): # Split the parameters into dynamic and static params , self . static = eqx . partition ( module , eqx . is_array ) # Flatten the parameters so that we can extract its sizes leaves , self . treedef = jax . tree_util . tree_flatten ( params ) # Get the shape and size of each leaf self . shapes_and_sizes = [( leaf . shape , leaf . size ) for leaf in leaves ] # Flatten the parameters flat_params = jnp . concatenate ([ leaf . ravel () for leaf in leaves ]) # Keep track of the size of the flattened parameters self . flat_params_size = flat_params . size # Keep track of the split points for each paramter in the flattened array self . indices = np . cumsum ( np . array ([ 0 ] + [ size for _ , size in self . shapes_and_sizes ])) def __call__ ( self , flat_params : Array ) -> eqx . Module : flat_params = flat_params . ravel () # Flatten the parameters completely leaves = [] for i , ( shape , size ) in enumerate ( self . shapes_and_sizes ): # Extract each leaf from the flattened parameters and reshape it buffer = flat_params [ self . indices [ i ]: self . indices [ i + 1 ]] if buffer . size != misc . list_prod ( shape ): raise ValueError ( f 'Expected total size of { misc . list_prod ( shape ) } but got { buffer . size } ' ) leaf = buffer . reshape ( shape ) leaves . append ( leaf ) # Turn the leaves back into a tree params = jax . tree_util . tree_unflatten ( self . treedef , leaves ) return eqx . combine ( params , self . static ) __init__ ( self , module ) \u00a4 Source code in generax/flows/coupling.py def __init__ ( self , module ): # Split the parameters into dynamic and static params , self . static = eqx . partition ( module , eqx . is_array ) # Flatten the parameters so that we can extract its sizes leaves , self . treedef = jax . tree_util . tree_flatten ( params ) # Get the shape and size of each leaf self . shapes_and_sizes = [( leaf . shape , leaf . size ) for leaf in leaves ] # Flatten the parameters flat_params = jnp . concatenate ([ leaf . ravel () for leaf in leaves ]) # Keep track of the size of the flattened parameters self . flat_params_size = flat_params . size # Keep track of the split points for each paramter in the flattened array self . indices = np . cumsum ( np . array ([ 0 ] + [ size for _ , size in self . shapes_and_sizes ])) __call__ ( self , flat_params : Array ) -> Module \u00a4 Call self as a function. Source code in generax/flows/coupling.py def __call__ ( self , flat_params : Array ) -> eqx . Module : flat_params = flat_params . ravel () # Flatten the parameters completely leaves = [] for i , ( shape , size ) in enumerate ( self . shapes_and_sizes ): # Extract each leaf from the flattened parameters and reshape it buffer = flat_params [ self . indices [ i ]: self . indices [ i + 1 ]] if buffer . size != misc . list_prod ( shape ): raise ValueError ( f 'Expected total size of { misc . list_prod ( shape ) } but got { buffer . size } ' ) leaf = buffer . reshape ( shape ) leaves . append ( leaf ) # Turn the leaves back into a tree params = jax . tree_util . tree_unflatten ( self . treedef , leaves ) return eqx . combine ( params , self . static ) generax.flows.coupling.Coupling ( BijectiveTransform ) \u00a4 Parametrize a flow over half of the inputs using the other half. The conditioning network will be fixed # Intended usage: layer = Coupling ( BijectiveTransform , eqx . Module ) z , log_det = layer ( x , y ) Attributes : - transform : The bijective transformation to use. - scale : A scalar that we'll use to start with small parameter values - net : The neural network to use. Source code in generax/flows/coupling.py class Coupling ( BijectiveTransform ): \"\"\"Parametrize a flow over half of the inputs using the other half. The conditioning network will be fixed ```python # Intended usage: layer = Coupling(BijectiveTransform, eqx.Module) z, log_det = layer(x, y) ``` **Attributes**: - `transform`: The bijective transformation to use. - `scale`: A scalar that we'll use to start with small parameter values - `net`: The neural network to use. \"\"\" net : eqx . Module scale : Array params_to_transform : RavelParameters def __init__ ( self , transform : BijectiveTransform , net : eqx . Module , input_shape : Tuple [ int ], cond_shape : Optional [ Tuple [ int ]] = None , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `transform`: The bijective transformation to use. - `net`: The neural network to generate the transform parameters. - `input_shape`: The shape of the input - `cond_shape`: The shape of the conditioning information - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , cond_shape = cond_shape , ** kwargs ) # Check the input shapes of the transform and network x1_shape , x2_shape = self . get_split_shapes ( input_shape ) assert transform . input_shape == x1_shape self . net = net # Use this to turn an eqx module into an array and vice-versa self . params_to_transform = RavelParameters ( transform ) # Also initialize the parameters to be close to 0 self . scale = random . normal ( key , ( 1 ,)) * 0.01 def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' x1 , x2 = self . split ( x ) net = self . net . data_dependent_init ( x2 , y = y , key = key ) # Turn the new parameters into a new module get_net = lambda tree : tree . net updated_layer = eqx . tree_at ( get_net , self , net ) return updated_layer def split ( self , x : Array ) -> Tuple [ Array , Array ]: \"\"\"Split the input into two halves.\"\"\" split_dim = x . shape [ - 1 ] // 2 x1 , x2 = x [ ... , : split_dim ], x [ ... , split_dim :] return x1 , x2 @classmethod def get_split_shapes ( cls , input_shape : Tuple [ int ]) -> Tuple [ Tuple [ int ]]: split_dim = input_shape [ - 1 ] // 2 x1_dim , x2_dim = split_dim , input_shape [ - 1 ] - split_dim x1_shape = input_shape [: - 1 ] + ( x1_dim ,) x2_shape = input_shape [: - 1 ] + ( x2_dim ,) return x1_shape , x2_shape @classmethod def get_net_output_shapes ( cls , input_shape : Tuple [ int ], transform : BijectiveTransform ) -> Tuple [ Tuple [ int ], int ]: \"\"\" **Arguments**: - `input_shape`: The shape of the input - `transform`: The bijective transformation to use. **Returns**: - `net_output_size`: The size of the output of the neural network. This is a single integer because the network is expected to produce a single vector. \"\"\" x1_shape , x2_shape = cls . get_split_shapes ( input_shape ) if x1_shape != transform . input_shape : raise ValueError ( f 'The transform { transform } needs to have an input shape equal to { x1_shape } . Use Coupling.get_input_shapes to get this shape.' ) params_to_transform = RavelParameters ( transform ) net_output_size = params_to_transform . flat_params_size return net_output_size @classmethod def get_net_input_and_output_shapes ( cls , input_shape : Tuple [ int ], transform : BijectiveTransform ) -> Tuple [ Tuple [ int ], int ]: \"\"\" **Arguments**: - `input_shape`: The shape of the input - `transform`: The bijective transformation to use. **Returns**: - `net_input_shape`: The shape of the input to the neural network. This is a tuple of ints - `net_output_size`: The size of the output of the neural network. This is a single integer because the network is expected to produce a single vector. \"\"\" x1_shape , x2_shape = cls . get_split_shapes ( input_shape ) net_input_shape = x2_shape params_to_transform = RavelParameters ( transform ) net_output_size = params_to_transform . flat_params_size return x1_shape , net_input_shape , net_output_size def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' # Split the input into two halves x1 , x2 = self . split ( x ) params = self . net ( x2 , y = y , ** kwargs ) params *= self . scale assert params . size == self . params_to_transform . flat_params_size # Apply the transformation to x1 given x2 transform = self . params_to_transform ( params ) z1 , log_det = transform ( x1 , y = y , inverse = inverse , ** kwargs ) z = jnp . concatenate ([ z1 , x2 ], axis =- 1 ) return z , log_det __init__ ( self , transform : BijectiveTransform , net : Module , input_shape : Tuple [ int ], cond_shape : Optional [ Tuple [ int ]] = None , * , key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : transform : The bijective transformation to use. net : The neural network to generate the transform parameters. input_shape : The shape of the input cond_shape : The shape of the conditioning information key : A jax.random.PRNGKey for initialization Source code in generax/flows/coupling.py def __init__ ( self , transform : BijectiveTransform , net : eqx . Module , input_shape : Tuple [ int ], cond_shape : Optional [ Tuple [ int ]] = None , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `transform`: The bijective transformation to use. - `net`: The neural network to generate the transform parameters. - `input_shape`: The shape of the input - `cond_shape`: The shape of the conditioning information - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , cond_shape = cond_shape , ** kwargs ) # Check the input shapes of the transform and network x1_shape , x2_shape = self . get_split_shapes ( input_shape ) assert transform . input_shape == x1_shape self . net = net # Use this to turn an eqx module into an array and vice-versa self . params_to_transform = RavelParameters ( transform ) # Also initialize the parameters to be close to 0 self . scale = random . normal ( key , ( 1 ,)) * 0.01 inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Inherited from generax.flows.base.BijectiveTransform.inverse . Source code in generax/flows/coupling.py def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs ) data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform \u00a4 Initialize the parameters of the layer based on the data. Arguments : x : The data to initialize the parameters with. y : The conditioning information key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/flows/coupling.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' x1 , x2 = self . split ( x ) net = self . net . data_dependent_init ( x2 , y = y , key = key ) # Turn the new parameters into a new module get_net = lambda tree : tree . net updated_layer = eqx . tree_at ( get_net , self , net ) return updated_layer split ( self , x : Array ) -> Tuple [ Array , Array ] \u00a4 Split the input into two halves. Source code in generax/flows/coupling.py def split ( self , x : Array ) -> Tuple [ Array , Array ]: \"\"\"Split the input into two halves.\"\"\" split_dim = x . shape [ - 1 ] // 2 x1 , x2 = x [ ... , : split_dim ], x [ ... , split_dim :] return x1 , x2 get_split_shapes ( input_shape : Tuple [ int ]) -> Tuple [ Tuple [ int ]] classmethod \u00a4 Source code in generax/flows/coupling.py @classmethod def get_split_shapes ( cls , input_shape : Tuple [ int ]) -> Tuple [ Tuple [ int ]]: split_dim = input_shape [ - 1 ] // 2 x1_dim , x2_dim = split_dim , input_shape [ - 1 ] - split_dim x1_shape = input_shape [: - 1 ] + ( x1_dim ,) x2_shape = input_shape [: - 1 ] + ( x2_dim ,) return x1_shape , x2_shape get_net_output_shapes ( input_shape : Tuple [ int ], transform : BijectiveTransform ) -> Tuple [ Tuple [ int ], int ] classmethod \u00a4 Arguments : - input_shape : The shape of the input - transform : The bijective transformation to use. Returns : - net_output_size : The size of the output of the neural network. This is a single integer because the network is expected to produce a single vector. Source code in generax/flows/coupling.py @classmethod def get_net_output_shapes ( cls , input_shape : Tuple [ int ], transform : BijectiveTransform ) -> Tuple [ Tuple [ int ], int ]: \"\"\" **Arguments**: - `input_shape`: The shape of the input - `transform`: The bijective transformation to use. **Returns**: - `net_output_size`: The size of the output of the neural network. This is a single integer because the network is expected to produce a single vector. \"\"\" x1_shape , x2_shape = cls . get_split_shapes ( input_shape ) if x1_shape != transform . input_shape : raise ValueError ( f 'The transform { transform } needs to have an input shape equal to { x1_shape } . Use Coupling.get_input_shapes to get this shape.' ) params_to_transform = RavelParameters ( transform ) net_output_size = params_to_transform . flat_params_size return net_output_size get_net_input_and_output_shapes ( input_shape : Tuple [ int ], transform : BijectiveTransform ) -> Tuple [ Tuple [ int ], int ] classmethod \u00a4 Arguments : - input_shape : The shape of the input - transform : The bijective transformation to use. Returns : - net_input_shape : The shape of the input to the neural network. This is a tuple of ints - net_output_size : The size of the output of the neural network. This is a single integer because the network is expected to produce a single vector. Source code in generax/flows/coupling.py @classmethod def get_net_input_and_output_shapes ( cls , input_shape : Tuple [ int ], transform : BijectiveTransform ) -> Tuple [ Tuple [ int ], int ]: \"\"\" **Arguments**: - `input_shape`: The shape of the input - `transform`: The bijective transformation to use. **Returns**: - `net_input_shape`: The shape of the input to the neural network. This is a tuple of ints - `net_output_size`: The size of the output of the neural network. This is a single integer because the network is expected to produce a single vector. \"\"\" x1_shape , x2_shape = cls . get_split_shapes ( input_shape ) net_input_shape = x2_shape params_to_transform = RavelParameters ( transform ) net_output_size = params_to_transform . flat_params_size return x1_shape , net_input_shape , net_output_size __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Implements generax.flows.base.BijectiveTransform.__call__ . Source code in generax/flows/coupling.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' # Split the input into two halves x1 , x2 = self . split ( x ) params = self . net ( x2 , y = y , ** kwargs ) params *= self . scale assert params . size == self . params_to_transform . flat_params_size # Apply the transformation to x1 given x2 transform = self . params_to_transform ( params ) z1 , log_det = transform ( x1 , y = y , inverse = inverse , ** kwargs ) z = jnp . concatenate ([ z1 , x2 ], axis =- 1 ) return z , log_det","title":"Coupling"},{"location":"api/flows/coupling/#coupling","text":"","title":"Coupling"},{"location":"api/flows/coupling/#generax.flows.coupling.RavelParameters","text":"Flatten and concatenate the parameters of a eqx.Module Source code in generax/flows/coupling.py class RavelParameters ( eqx . Module ): \"\"\"Flatten and concatenate the parameters of a eqx.Module \"\"\" shapes_and_sizes : Sequence [ Tuple [ Tuple [ int ], int ]] = eqx . field ( static = True ) flat_params_size : Tuple [ int ] = eqx . field ( static = True ) static : Any = eqx . field ( static = True ) treedef : Any = eqx . field ( static = True ) indices : np . ndarray = eqx . field ( static = True ) def __init__ ( self , module ): # Split the parameters into dynamic and static params , self . static = eqx . partition ( module , eqx . is_array ) # Flatten the parameters so that we can extract its sizes leaves , self . treedef = jax . tree_util . tree_flatten ( params ) # Get the shape and size of each leaf self . shapes_and_sizes = [( leaf . shape , leaf . size ) for leaf in leaves ] # Flatten the parameters flat_params = jnp . concatenate ([ leaf . ravel () for leaf in leaves ]) # Keep track of the size of the flattened parameters self . flat_params_size = flat_params . size # Keep track of the split points for each paramter in the flattened array self . indices = np . cumsum ( np . array ([ 0 ] + [ size for _ , size in self . shapes_and_sizes ])) def __call__ ( self , flat_params : Array ) -> eqx . Module : flat_params = flat_params . ravel () # Flatten the parameters completely leaves = [] for i , ( shape , size ) in enumerate ( self . shapes_and_sizes ): # Extract each leaf from the flattened parameters and reshape it buffer = flat_params [ self . indices [ i ]: self . indices [ i + 1 ]] if buffer . size != misc . list_prod ( shape ): raise ValueError ( f 'Expected total size of { misc . list_prod ( shape ) } but got { buffer . size } ' ) leaf = buffer . reshape ( shape ) leaves . append ( leaf ) # Turn the leaves back into a tree params = jax . tree_util . tree_unflatten ( self . treedef , leaves ) return eqx . combine ( params , self . static )","title":"RavelParameters"},{"location":"api/flows/coupling/#generax.flows.coupling.Coupling","text":"Parametrize a flow over half of the inputs using the other half. The conditioning network will be fixed # Intended usage: layer = Coupling ( BijectiveTransform , eqx . Module ) z , log_det = layer ( x , y ) Attributes : - transform : The bijective transformation to use. - scale : A scalar that we'll use to start with small parameter values - net : The neural network to use. Source code in generax/flows/coupling.py class Coupling ( BijectiveTransform ): \"\"\"Parametrize a flow over half of the inputs using the other half. The conditioning network will be fixed ```python # Intended usage: layer = Coupling(BijectiveTransform, eqx.Module) z, log_det = layer(x, y) ``` **Attributes**: - `transform`: The bijective transformation to use. - `scale`: A scalar that we'll use to start with small parameter values - `net`: The neural network to use. \"\"\" net : eqx . Module scale : Array params_to_transform : RavelParameters def __init__ ( self , transform : BijectiveTransform , net : eqx . Module , input_shape : Tuple [ int ], cond_shape : Optional [ Tuple [ int ]] = None , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `transform`: The bijective transformation to use. - `net`: The neural network to generate the transform parameters. - `input_shape`: The shape of the input - `cond_shape`: The shape of the conditioning information - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , cond_shape = cond_shape , ** kwargs ) # Check the input shapes of the transform and network x1_shape , x2_shape = self . get_split_shapes ( input_shape ) assert transform . input_shape == x1_shape self . net = net # Use this to turn an eqx module into an array and vice-versa self . params_to_transform = RavelParameters ( transform ) # Also initialize the parameters to be close to 0 self . scale = random . normal ( key , ( 1 ,)) * 0.01 def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> BijectiveTransform : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' x1 , x2 = self . split ( x ) net = self . net . data_dependent_init ( x2 , y = y , key = key ) # Turn the new parameters into a new module get_net = lambda tree : tree . net updated_layer = eqx . tree_at ( get_net , self , net ) return updated_layer def split ( self , x : Array ) -> Tuple [ Array , Array ]: \"\"\"Split the input into two halves.\"\"\" split_dim = x . shape [ - 1 ] // 2 x1 , x2 = x [ ... , : split_dim ], x [ ... , split_dim :] return x1 , x2 @classmethod def get_split_shapes ( cls , input_shape : Tuple [ int ]) -> Tuple [ Tuple [ int ]]: split_dim = input_shape [ - 1 ] // 2 x1_dim , x2_dim = split_dim , input_shape [ - 1 ] - split_dim x1_shape = input_shape [: - 1 ] + ( x1_dim ,) x2_shape = input_shape [: - 1 ] + ( x2_dim ,) return x1_shape , x2_shape @classmethod def get_net_output_shapes ( cls , input_shape : Tuple [ int ], transform : BijectiveTransform ) -> Tuple [ Tuple [ int ], int ]: \"\"\" **Arguments**: - `input_shape`: The shape of the input - `transform`: The bijective transformation to use. **Returns**: - `net_output_size`: The size of the output of the neural network. This is a single integer because the network is expected to produce a single vector. \"\"\" x1_shape , x2_shape = cls . get_split_shapes ( input_shape ) if x1_shape != transform . input_shape : raise ValueError ( f 'The transform { transform } needs to have an input shape equal to { x1_shape } . Use Coupling.get_input_shapes to get this shape.' ) params_to_transform = RavelParameters ( transform ) net_output_size = params_to_transform . flat_params_size return net_output_size @classmethod def get_net_input_and_output_shapes ( cls , input_shape : Tuple [ int ], transform : BijectiveTransform ) -> Tuple [ Tuple [ int ], int ]: \"\"\" **Arguments**: - `input_shape`: The shape of the input - `transform`: The bijective transformation to use. **Returns**: - `net_input_shape`: The shape of the input to the neural network. This is a tuple of ints - `net_output_size`: The size of the output of the neural network. This is a single integer because the network is expected to produce a single vector. \"\"\" x1_shape , x2_shape = cls . get_split_shapes ( input_shape ) net_input_shape = x2_shape params_to_transform = RavelParameters ( transform ) net_output_size = params_to_transform . flat_params_size return x1_shape , net_input_shape , net_output_size def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: (z, log_det) \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' # Split the input into two halves x1 , x2 = self . split ( x ) params = self . net ( x2 , y = y , ** kwargs ) params *= self . scale assert params . size == self . params_to_transform . flat_params_size # Apply the transformation to x1 given x2 transform = self . params_to_transform ( params ) z1 , log_det = transform ( x1 , y = y , inverse = inverse , ** kwargs ) z = jnp . concatenate ([ z1 , x2 ], axis =- 1 ) return z , log_det","title":"Coupling"},{"location":"api/flows/ffjord/","text":"FFJORD \u00a4 generax.flows.ffjord.FFJORDTransform ( BijectiveTransform ) \u00a4 Flow parametrized by a neural ODE https://arxiv.org/pdf/1810.01367.pdf Attributes : input_shape : The input shape. Output shape will have the same dimensionality as the input. cond_shape : The shape of the conditioning information. If there is no conditioning information, this is None. neural_ode : The neural ODE trace_estimate_likelihood : Whether to use a trace estimate for the likelihood. adjoint : The adjoint method to use. Can be one of the following: key : The random key to use for initialization Source code in generax/flows/ffjord.py class FFJORDTransform ( BijectiveTransform ): \"\"\"Flow parametrized by a neural ODE https://arxiv.org/pdf/1810.01367.pdf **Attributes**: - `input_shape`: The input shape. Output shape will have the same dimensionality as the input. - `cond_shape`: The shape of the conditioning information. If there is no conditioning information, this is None. - `neural_ode`: The neural ODE - `trace_estimate_likelihood`: Whether to use a trace estimate for the likelihood. - `adjoint`: The adjoint method to use. Can be one of the following: - `key`: The random key to use for initialization \"\"\" neural_ode : NeuralODE trace_estimate_likelihood : bool def __init__ ( self , input_shape : Tuple [ int ], net : eqx . Module = None , working_size : int = 16 , hidden_size : int = 32 , n_blocks : int = 4 , time_embedding_size = 16 , n_time_features = 8 , cond_shape : Optional [ Tuple [ int ]] = None , * , controller_rtol : Optional [ float ] = 1e-8 , controller_atol : Optional [ float ] = 1e-8 , trace_estimate_likelihood : Optional [ bool ] = False , adjoint = 'recursive_checkpoint' , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The shape of the input to the transformation - `net`: The neural network to use for the vector field. If None, a default network will be used. `net` should accept `net(t, x, y=y)` - `controller_rtol`: The relative tolerance of the stepsize controller. - `controller_atol`: The absolute tolerance of the stepsize controller. - `trace_estimate_likelihood`: Whether to use a trace estimate for the likelihood. - `adjoint`: The adjoint method to use. Can be one of the following: - `\"recursive_checkpoint\"`: Use the recursive checkpoint method. Doesn't support jvp. - `\"direct\"`: Use the direct method. Supports jvps. - `\"seminorm\"`: Use the seminorm method. Does fast backprop through the solver. - `key`: The random key to use for initialization \"\"\" if net is None : net = TimeDependentResNet ( input_shape = input_shape , working_size = working_size , hidden_size = hidden_size , out_size = input_shape [ - 1 ], n_blocks = n_blocks , cond_shape = cond_shape , embedding_size = time_embedding_size , out_features = n_time_features , key = key ) self . neural_ode = NeuralODE ( vf = net , adjoint = adjoint , controller_rtol = controller_rtol , controller_atol = controller_atol ) self . trace_estimate_likelihood = trace_estimate_likelihood super () . __init__ ( input_shape = input_shape , ** kwargs ) @property def vector_field ( self ): return self . neural_ode . vector_field def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , log_likelihood : bool = True , key : Optional [ PRNGKeyArray ] = None , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation - `log_likelihood`: Whether to compute the log likelihood of the transformation - `key`: The random key to use for initialization **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if log_likelihood and ( self . trace_estimate_likelihood and ( key is None )): raise TypeError ( f 'When using trace estimation, must pass random key' ) if log_likelihood == False : trace_estimate_likelihood = False else : trace_estimate_likelihood = self . trace_estimate_likelihood z , log_det = self . neural_ode ( x , y = y , inverse = inverse , log_likelihood = log_likelihood , trace_estimate_likelihood = trace_estimate_likelihood , save_at = None , key = key , ** kwargs ) return z , log_det vector_field property readonly \u00a4 __init__ ( self , input_shape : Tuple [ int ], net : Module = None , working_size : int = 16 , hidden_size : int = 32 , n_blocks : int = 4 , time_embedding_size = 16 , n_time_features = 8 , cond_shape : Optional [ Tuple [ int ]] = None , * , controller_rtol : Optional [ float ] = 1e-08 , controller_atol : Optional [ float ] = 1e-08 , trace_estimate_likelihood : Optional [ bool ] = False , adjoint = 'recursive_checkpoint' , key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : input_shape : The shape of the input to the transformation net : The neural network to use for the vector field. If None, a default network will be used. net should accept net(t, x, y=y) controller_rtol : The relative tolerance of the stepsize controller. controller_atol : The absolute tolerance of the stepsize controller. trace_estimate_likelihood : Whether to use a trace estimate for the likelihood. adjoint : The adjoint method to use. Can be one of the following: \"recursive_checkpoint\" : Use the recursive checkpoint method. Doesn't support jvp. \"direct\" : Use the direct method. Supports jvps. \"seminorm\" : Use the seminorm method. Does fast backprop through the solver. key : The random key to use for initialization Source code in generax/flows/ffjord.py def __init__ ( self , input_shape : Tuple [ int ], net : eqx . Module = None , working_size : int = 16 , hidden_size : int = 32 , n_blocks : int = 4 , time_embedding_size = 16 , n_time_features = 8 , cond_shape : Optional [ Tuple [ int ]] = None , * , controller_rtol : Optional [ float ] = 1e-8 , controller_atol : Optional [ float ] = 1e-8 , trace_estimate_likelihood : Optional [ bool ] = False , adjoint = 'recursive_checkpoint' , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The shape of the input to the transformation - `net`: The neural network to use for the vector field. If None, a default network will be used. `net` should accept `net(t, x, y=y)` - `controller_rtol`: The relative tolerance of the stepsize controller. - `controller_atol`: The absolute tolerance of the stepsize controller. - `trace_estimate_likelihood`: Whether to use a trace estimate for the likelihood. - `adjoint`: The adjoint method to use. Can be one of the following: - `\"recursive_checkpoint\"`: Use the recursive checkpoint method. Doesn't support jvp. - `\"direct\"`: Use the direct method. Supports jvps. - `\"seminorm\"`: Use the seminorm method. Does fast backprop through the solver. - `key`: The random key to use for initialization \"\"\" if net is None : net = TimeDependentResNet ( input_shape = input_shape , working_size = working_size , hidden_size = hidden_size , out_size = input_shape [ - 1 ], n_blocks = n_blocks , cond_shape = cond_shape , embedding_size = time_embedding_size , out_features = n_time_features , key = key ) self . neural_ode = NeuralODE ( vf = net , adjoint = adjoint , controller_rtol = controller_rtol , controller_atol = controller_atol ) self . trace_estimate_likelihood = trace_estimate_likelihood super () . __init__ ( input_shape = input_shape , ** kwargs ) data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) \u00a4 Inherited from generax.flows.base.BijectiveTransform.data_dependent_init . Source code in generax/flows/ffjord.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ): \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" return self inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Inherited from generax.flows.base.BijectiveTransform.inverse . Source code in generax/flows/ffjord.py def inverse ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"Apply the inverse transformation. **Arguments**: - `x`: The input to the transformation - `y`: The conditioning information **Returns**: (z, log_det) \"\"\" return self ( x , y = y , inverse = True , ** kwargs ) __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , log_likelihood : bool = True , key : Optional [ PRNGKeyArray ] = None , ** kwargs ) -> Array \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation log_likelihood : Whether to compute the log likelihood of the transformation key : The random key to use for initialization Returns : (z, log_det) Source code in generax/flows/ffjord.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , log_likelihood : bool = True , key : Optional [ PRNGKeyArray ] = None , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation - `log_likelihood`: Whether to compute the log likelihood of the transformation - `key`: The random key to use for initialization **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if log_likelihood and ( self . trace_estimate_likelihood and ( key is None )): raise TypeError ( f 'When using trace estimation, must pass random key' ) if log_likelihood == False : trace_estimate_likelihood = False else : trace_estimate_likelihood = self . trace_estimate_likelihood z , log_det = self . neural_ode ( x , y = y , inverse = inverse , log_likelihood = log_likelihood , trace_estimate_likelihood = trace_estimate_likelihood , save_at = None , key = key , ** kwargs ) return z , log_det generax.nn.neural_ode.NeuralODE \u00a4 Neural ODE Source code in generax/nn/neural_ode.py class NeuralODE ( eqx . Module ): \"\"\"Neural ODE\"\"\" vector_field : eqx . Module adjoint : diffrax . AbstractAdjoint stepsize_controller : diffrax . AbstractAdaptiveStepSizeController def __init__ ( self , vf : eqx . Module , adjoint : Optional [ str ] = 'recursive_checkpoint' , controller_rtol : Optional [ float ] = 1e-3 , controller_atol : Optional [ float ] = 1e-5 , ): \"\"\"**Arguments**: - `vf`: A function that computes the vector field. It must output a vector of the same shape as its input. - `adjoint`: The adjoint method to use. Can be one of the following: - `\"recursive_checkpoint\"`: Use the recursive checkpoint method. Doesn't support jvp. - `\"direct\"`: Use the direct method. Supports jvps. - `\"seminorm\"`: Use the seminorm method. Does fast backprop through the solver. - `controller_rtol`: The relative tolerance of the stepsize controller. - `controller_atol`: The absolute tolerance of the stepsize controller. \"\"\" self . vector_field = vf if adjoint == 'recursive_checkpoint' : self . adjoint = diffrax . RecursiveCheckpointAdjoint () elif adjoint == 'direct' : self . adjoint = diffrax . DirectAdjoint () elif adjoint == 'seminorm' : assert 0 , \"There is a tracer leak somewhere \" # TODO: Make a bug report adjoint_controller = diffrax . PIDController ( rtol = 1e-3 , atol = 1e-6 , norm = diffrax . adjoint_rms_seminorm ) self . adjoint = diffrax . BacksolveAdjoint ( stepsize_controller = adjoint_controller ) self . stepsize_controller = diffrax . PIDController ( rtol = controller_rtol , atol = controller_atol ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , * , inverse : Optional [ bool ] = False , log_likelihood : Optional [ bool ] = False , trace_estimate_likelihood : Optional [ bool ] = False , save_at : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None , t0 : Optional [ float ] = 0.0 , t1 : Optional [ float ] = 1.0 ) -> Array : \"\"\"**Arguemnts**: - `x`: The input to the neural ODE. Must be a rank 1 array. - `key`: The random number generator key. - `inverse`: Whether to compute the inverse of the neural ODE. `inverse=True` corresponds to going from the base space to the data space. - `log_likelihood`: Whether to compute the log likelihood of the neural ODE. - `trace_estimate_likelihood`: Whether to compute a trace estimate of the likelihood of the neural ODE. - `save_at`: The times to save the neural ODE at. - `key`: The random key to use for initialization - `t0`: The initial time. - `t1`: The final time. **Returns**: - `z`: The output of the neural ODE. - `log_likelihood`: The log likelihood of the neural ODE if `log_likelihood=True`. \"\"\" assert x . shape == self . vector_field . input_shape if trace_estimate_likelihood : # Get a random vector for hutchinsons trace estimator k1 , _ = random . split ( key , 2 ) v = random . normal ( k1 , x . shape ) # Split the model into its static and dynamic parts so that backprop # through the ode solver can be faster. params , static = eqx . partition ( self . vector_field , eqx . is_array ) def f ( t , x_and_logpx , params ): x , log_px = x_and_logpx if inverse == False : # If we're inverting the flow, we need to adjust the time t = 1.0 - t # Recombine the model model = eqx . combine ( params , static ) # Fill the model with the current time def apply_vf ( x ): return model ( t , x , y = y ) if log_likelihood : if trace_estimate_likelihood : # Hutchinsons trace estimator. See ContinuousNormalizingFlow https://arxiv.org/pdf/1810.01367.pdf dxdt , dudxv = jax . jvp ( apply_vf , ( x ,), ( v ,)) dlogpxdt = - jnp . sum ( dudxv * v ) else : # Brute force dlogpx/dt. See NeuralODE https://arxiv.org/pdf/1806.07366.pdf x_flat = x . ravel () eye = jnp . eye ( x_flat . shape [ - 1 ]) x_shape = x . shape def jvp_flat ( x_flat , dx_flat ): x = x_flat . reshape ( x_shape ) dx = dx_flat . reshape ( x_shape ) dxdt , d2dx_dtdx = jax . jvp ( apply_vf , ( x ,), ( dx ,)) return dxdt , d2dx_dtdx . ravel () dxdt , d2dx_dtdx_flat = jax . vmap ( jvp_flat , in_axes = ( None , 0 ))( x_flat , eye ) dxdt = dxdt [ 0 ] dlogpxdt = - jnp . trace ( d2dx_dtdx_flat ) else : # Don't worry about the log likelihood dxdt = apply_vf ( x ) dlogpxdt = jnp . zeros_like ( log_px ) if inverse == False : # If we're inverting the flow, we need to flip the sign of dxdt dxdt = - dxdt return dxdt , dlogpxdt term = diffrax . ODETerm ( f ) solver = diffrax . Dopri5 () # Determine which times we want to save the neural ODE at. if save_at is None : saveat = diffrax . SaveAt ( ts = [ t1 ]) else : saveat = diffrax . SaveAt ( ts = save_at ) # Run the ODE solver solution = diffrax . diffeqsolve ( term , solver , saveat = saveat , t0 = t0 , t1 = t1 , dt0 = 0.0001 , y0 = ( x , jnp . array ( 0.0 )), args = params , adjoint = self . adjoint , stepsize_controller = self . stepsize_controller , throw = True ) outs = solution . ys if save_at is None : # Only take the first time outs = jax . tree_util . tree_map ( lambda x : x [ 0 ], outs ) z , log_px = outs if inverse : log_px = - log_px return z , log_px __init__ ( self , vf : Module , adjoint : Optional [ str ] = 'recursive_checkpoint' , controller_rtol : Optional [ float ] = 0.001 , controller_atol : Optional [ float ] = 1e-05 ) \u00a4 Arguments : vf : A function that computes the vector field. It must output a vector of the same shape as its input. adjoint : The adjoint method to use. Can be one of the following: \"recursive_checkpoint\" : Use the recursive checkpoint method. Doesn't support jvp. \"direct\" : Use the direct method. Supports jvps. \"seminorm\" : Use the seminorm method. Does fast backprop through the solver. controller_rtol : The relative tolerance of the stepsize controller. controller_atol : The absolute tolerance of the stepsize controller. Source code in generax/nn/neural_ode.py def __init__ ( self , vf : eqx . Module , adjoint : Optional [ str ] = 'recursive_checkpoint' , controller_rtol : Optional [ float ] = 1e-3 , controller_atol : Optional [ float ] = 1e-5 , ): \"\"\"**Arguments**: - `vf`: A function that computes the vector field. It must output a vector of the same shape as its input. - `adjoint`: The adjoint method to use. Can be one of the following: - `\"recursive_checkpoint\"`: Use the recursive checkpoint method. Doesn't support jvp. - `\"direct\"`: Use the direct method. Supports jvps. - `\"seminorm\"`: Use the seminorm method. Does fast backprop through the solver. - `controller_rtol`: The relative tolerance of the stepsize controller. - `controller_atol`: The absolute tolerance of the stepsize controller. \"\"\" self . vector_field = vf if adjoint == 'recursive_checkpoint' : self . adjoint = diffrax . RecursiveCheckpointAdjoint () elif adjoint == 'direct' : self . adjoint = diffrax . DirectAdjoint () elif adjoint == 'seminorm' : assert 0 , \"There is a tracer leak somewhere \" # TODO: Make a bug report adjoint_controller = diffrax . PIDController ( rtol = 1e-3 , atol = 1e-6 , norm = diffrax . adjoint_rms_seminorm ) self . adjoint = diffrax . BacksolveAdjoint ( stepsize_controller = adjoint_controller ) self . stepsize_controller = diffrax . PIDController ( rtol = controller_rtol , atol = controller_atol ) __call__ ( self , x : Array , y : Optional [ Array ] = None , * , inverse : Optional [ bool ] = False , log_likelihood : Optional [ bool ] = False , trace_estimate_likelihood : Optional [ bool ] = False , save_at : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None , t0 : Optional [ float ] = 0.0 , t1 : Optional [ float ] = 1.0 ) -> Array \u00a4 Arguemnts : x : The input to the neural ODE. Must be a rank 1 array. key : The random number generator key. inverse : Whether to compute the inverse of the neural ODE. inverse=True corresponds to going from the base space to the data space. log_likelihood : Whether to compute the log likelihood of the neural ODE. trace_estimate_likelihood : Whether to compute a trace estimate of the likelihood of the neural ODE. save_at : The times to save the neural ODE at. key : The random key to use for initialization t0 : The initial time. t1 : The final time. Returns : - z : The output of the neural ODE. - log_likelihood : The log likelihood of the neural ODE if log_likelihood=True . Source code in generax/nn/neural_ode.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , * , inverse : Optional [ bool ] = False , log_likelihood : Optional [ bool ] = False , trace_estimate_likelihood : Optional [ bool ] = False , save_at : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None , t0 : Optional [ float ] = 0.0 , t1 : Optional [ float ] = 1.0 ) -> Array : \"\"\"**Arguemnts**: - `x`: The input to the neural ODE. Must be a rank 1 array. - `key`: The random number generator key. - `inverse`: Whether to compute the inverse of the neural ODE. `inverse=True` corresponds to going from the base space to the data space. - `log_likelihood`: Whether to compute the log likelihood of the neural ODE. - `trace_estimate_likelihood`: Whether to compute a trace estimate of the likelihood of the neural ODE. - `save_at`: The times to save the neural ODE at. - `key`: The random key to use for initialization - `t0`: The initial time. - `t1`: The final time. **Returns**: - `z`: The output of the neural ODE. - `log_likelihood`: The log likelihood of the neural ODE if `log_likelihood=True`. \"\"\" assert x . shape == self . vector_field . input_shape if trace_estimate_likelihood : # Get a random vector for hutchinsons trace estimator k1 , _ = random . split ( key , 2 ) v = random . normal ( k1 , x . shape ) # Split the model into its static and dynamic parts so that backprop # through the ode solver can be faster. params , static = eqx . partition ( self . vector_field , eqx . is_array ) def f ( t , x_and_logpx , params ): x , log_px = x_and_logpx if inverse == False : # If we're inverting the flow, we need to adjust the time t = 1.0 - t # Recombine the model model = eqx . combine ( params , static ) # Fill the model with the current time def apply_vf ( x ): return model ( t , x , y = y ) if log_likelihood : if trace_estimate_likelihood : # Hutchinsons trace estimator. See ContinuousNormalizingFlow https://arxiv.org/pdf/1810.01367.pdf dxdt , dudxv = jax . jvp ( apply_vf , ( x ,), ( v ,)) dlogpxdt = - jnp . sum ( dudxv * v ) else : # Brute force dlogpx/dt. See NeuralODE https://arxiv.org/pdf/1806.07366.pdf x_flat = x . ravel () eye = jnp . eye ( x_flat . shape [ - 1 ]) x_shape = x . shape def jvp_flat ( x_flat , dx_flat ): x = x_flat . reshape ( x_shape ) dx = dx_flat . reshape ( x_shape ) dxdt , d2dx_dtdx = jax . jvp ( apply_vf , ( x ,), ( dx ,)) return dxdt , d2dx_dtdx . ravel () dxdt , d2dx_dtdx_flat = jax . vmap ( jvp_flat , in_axes = ( None , 0 ))( x_flat , eye ) dxdt = dxdt [ 0 ] dlogpxdt = - jnp . trace ( d2dx_dtdx_flat ) else : # Don't worry about the log likelihood dxdt = apply_vf ( x ) dlogpxdt = jnp . zeros_like ( log_px ) if inverse == False : # If we're inverting the flow, we need to flip the sign of dxdt dxdt = - dxdt return dxdt , dlogpxdt term = diffrax . ODETerm ( f ) solver = diffrax . Dopri5 () # Determine which times we want to save the neural ODE at. if save_at is None : saveat = diffrax . SaveAt ( ts = [ t1 ]) else : saveat = diffrax . SaveAt ( ts = save_at ) # Run the ODE solver solution = diffrax . diffeqsolve ( term , solver , saveat = saveat , t0 = t0 , t1 = t1 , dt0 = 0.0001 , y0 = ( x , jnp . array ( 0.0 )), args = params , adjoint = self . adjoint , stepsize_controller = self . stepsize_controller , throw = True ) outs = solution . ys if save_at is None : # Only take the first time outs = jax . tree_util . tree_map ( lambda x : x [ 0 ], outs ) z , log_px = outs if inverse : log_px = - log_px return z , log_px","title":"FFJORD"},{"location":"api/flows/ffjord/#ffjord","text":"","title":"FFJORD"},{"location":"api/flows/ffjord/#generax.flows.ffjord.FFJORDTransform","text":"Flow parametrized by a neural ODE https://arxiv.org/pdf/1810.01367.pdf Attributes : input_shape : The input shape. Output shape will have the same dimensionality as the input. cond_shape : The shape of the conditioning information. If there is no conditioning information, this is None. neural_ode : The neural ODE trace_estimate_likelihood : Whether to use a trace estimate for the likelihood. adjoint : The adjoint method to use. Can be one of the following: key : The random key to use for initialization Source code in generax/flows/ffjord.py class FFJORDTransform ( BijectiveTransform ): \"\"\"Flow parametrized by a neural ODE https://arxiv.org/pdf/1810.01367.pdf **Attributes**: - `input_shape`: The input shape. Output shape will have the same dimensionality as the input. - `cond_shape`: The shape of the conditioning information. If there is no conditioning information, this is None. - `neural_ode`: The neural ODE - `trace_estimate_likelihood`: Whether to use a trace estimate for the likelihood. - `adjoint`: The adjoint method to use. Can be one of the following: - `key`: The random key to use for initialization \"\"\" neural_ode : NeuralODE trace_estimate_likelihood : bool def __init__ ( self , input_shape : Tuple [ int ], net : eqx . Module = None , working_size : int = 16 , hidden_size : int = 32 , n_blocks : int = 4 , time_embedding_size = 16 , n_time_features = 8 , cond_shape : Optional [ Tuple [ int ]] = None , * , controller_rtol : Optional [ float ] = 1e-8 , controller_atol : Optional [ float ] = 1e-8 , trace_estimate_likelihood : Optional [ bool ] = False , adjoint = 'recursive_checkpoint' , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The shape of the input to the transformation - `net`: The neural network to use for the vector field. If None, a default network will be used. `net` should accept `net(t, x, y=y)` - `controller_rtol`: The relative tolerance of the stepsize controller. - `controller_atol`: The absolute tolerance of the stepsize controller. - `trace_estimate_likelihood`: Whether to use a trace estimate for the likelihood. - `adjoint`: The adjoint method to use. Can be one of the following: - `\"recursive_checkpoint\"`: Use the recursive checkpoint method. Doesn't support jvp. - `\"direct\"`: Use the direct method. Supports jvps. - `\"seminorm\"`: Use the seminorm method. Does fast backprop through the solver. - `key`: The random key to use for initialization \"\"\" if net is None : net = TimeDependentResNet ( input_shape = input_shape , working_size = working_size , hidden_size = hidden_size , out_size = input_shape [ - 1 ], n_blocks = n_blocks , cond_shape = cond_shape , embedding_size = time_embedding_size , out_features = n_time_features , key = key ) self . neural_ode = NeuralODE ( vf = net , adjoint = adjoint , controller_rtol = controller_rtol , controller_atol = controller_atol ) self . trace_estimate_likelihood = trace_estimate_likelihood super () . __init__ ( input_shape = input_shape , ** kwargs ) @property def vector_field ( self ): return self . neural_ode . vector_field def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , log_likelihood : bool = True , key : Optional [ PRNGKeyArray ] = None , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation - `log_likelihood`: Whether to compute the log likelihood of the transformation - `key`: The random key to use for initialization **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if log_likelihood and ( self . trace_estimate_likelihood and ( key is None )): raise TypeError ( f 'When using trace estimation, must pass random key' ) if log_likelihood == False : trace_estimate_likelihood = False else : trace_estimate_likelihood = self . trace_estimate_likelihood z , log_det = self . neural_ode ( x , y = y , inverse = inverse , log_likelihood = log_likelihood , trace_estimate_likelihood = trace_estimate_likelihood , save_at = None , key = key , ** kwargs ) return z , log_det","title":"FFJORDTransform"},{"location":"api/flows/ffjord/#generax.nn.neural_ode.NeuralODE","text":"Neural ODE Source code in generax/nn/neural_ode.py class NeuralODE ( eqx . Module ): \"\"\"Neural ODE\"\"\" vector_field : eqx . Module adjoint : diffrax . AbstractAdjoint stepsize_controller : diffrax . AbstractAdaptiveStepSizeController def __init__ ( self , vf : eqx . Module , adjoint : Optional [ str ] = 'recursive_checkpoint' , controller_rtol : Optional [ float ] = 1e-3 , controller_atol : Optional [ float ] = 1e-5 , ): \"\"\"**Arguments**: - `vf`: A function that computes the vector field. It must output a vector of the same shape as its input. - `adjoint`: The adjoint method to use. Can be one of the following: - `\"recursive_checkpoint\"`: Use the recursive checkpoint method. Doesn't support jvp. - `\"direct\"`: Use the direct method. Supports jvps. - `\"seminorm\"`: Use the seminorm method. Does fast backprop through the solver. - `controller_rtol`: The relative tolerance of the stepsize controller. - `controller_atol`: The absolute tolerance of the stepsize controller. \"\"\" self . vector_field = vf if adjoint == 'recursive_checkpoint' : self . adjoint = diffrax . RecursiveCheckpointAdjoint () elif adjoint == 'direct' : self . adjoint = diffrax . DirectAdjoint () elif adjoint == 'seminorm' : assert 0 , \"There is a tracer leak somewhere \" # TODO: Make a bug report adjoint_controller = diffrax . PIDController ( rtol = 1e-3 , atol = 1e-6 , norm = diffrax . adjoint_rms_seminorm ) self . adjoint = diffrax . BacksolveAdjoint ( stepsize_controller = adjoint_controller ) self . stepsize_controller = diffrax . PIDController ( rtol = controller_rtol , atol = controller_atol ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , * , inverse : Optional [ bool ] = False , log_likelihood : Optional [ bool ] = False , trace_estimate_likelihood : Optional [ bool ] = False , save_at : Optional [ Array ] = None , key : Optional [ PRNGKeyArray ] = None , t0 : Optional [ float ] = 0.0 , t1 : Optional [ float ] = 1.0 ) -> Array : \"\"\"**Arguemnts**: - `x`: The input to the neural ODE. Must be a rank 1 array. - `key`: The random number generator key. - `inverse`: Whether to compute the inverse of the neural ODE. `inverse=True` corresponds to going from the base space to the data space. - `log_likelihood`: Whether to compute the log likelihood of the neural ODE. - `trace_estimate_likelihood`: Whether to compute a trace estimate of the likelihood of the neural ODE. - `save_at`: The times to save the neural ODE at. - `key`: The random key to use for initialization - `t0`: The initial time. - `t1`: The final time. **Returns**: - `z`: The output of the neural ODE. - `log_likelihood`: The log likelihood of the neural ODE if `log_likelihood=True`. \"\"\" assert x . shape == self . vector_field . input_shape if trace_estimate_likelihood : # Get a random vector for hutchinsons trace estimator k1 , _ = random . split ( key , 2 ) v = random . normal ( k1 , x . shape ) # Split the model into its static and dynamic parts so that backprop # through the ode solver can be faster. params , static = eqx . partition ( self . vector_field , eqx . is_array ) def f ( t , x_and_logpx , params ): x , log_px = x_and_logpx if inverse == False : # If we're inverting the flow, we need to adjust the time t = 1.0 - t # Recombine the model model = eqx . combine ( params , static ) # Fill the model with the current time def apply_vf ( x ): return model ( t , x , y = y ) if log_likelihood : if trace_estimate_likelihood : # Hutchinsons trace estimator. See ContinuousNormalizingFlow https://arxiv.org/pdf/1810.01367.pdf dxdt , dudxv = jax . jvp ( apply_vf , ( x ,), ( v ,)) dlogpxdt = - jnp . sum ( dudxv * v ) else : # Brute force dlogpx/dt. See NeuralODE https://arxiv.org/pdf/1806.07366.pdf x_flat = x . ravel () eye = jnp . eye ( x_flat . shape [ - 1 ]) x_shape = x . shape def jvp_flat ( x_flat , dx_flat ): x = x_flat . reshape ( x_shape ) dx = dx_flat . reshape ( x_shape ) dxdt , d2dx_dtdx = jax . jvp ( apply_vf , ( x ,), ( dx ,)) return dxdt , d2dx_dtdx . ravel () dxdt , d2dx_dtdx_flat = jax . vmap ( jvp_flat , in_axes = ( None , 0 ))( x_flat , eye ) dxdt = dxdt [ 0 ] dlogpxdt = - jnp . trace ( d2dx_dtdx_flat ) else : # Don't worry about the log likelihood dxdt = apply_vf ( x ) dlogpxdt = jnp . zeros_like ( log_px ) if inverse == False : # If we're inverting the flow, we need to flip the sign of dxdt dxdt = - dxdt return dxdt , dlogpxdt term = diffrax . ODETerm ( f ) solver = diffrax . Dopri5 () # Determine which times we want to save the neural ODE at. if save_at is None : saveat = diffrax . SaveAt ( ts = [ t1 ]) else : saveat = diffrax . SaveAt ( ts = save_at ) # Run the ODE solver solution = diffrax . diffeqsolve ( term , solver , saveat = saveat , t0 = t0 , t1 = t1 , dt0 = 0.0001 , y0 = ( x , jnp . array ( 0.0 )), args = params , adjoint = self . adjoint , stepsize_controller = self . stepsize_controller , throw = True ) outs = solution . ys if save_at is None : # Only take the first time outs = jax . tree_util . tree_map ( lambda x : x [ 0 ], outs ) z , log_px = outs if inverse : log_px = - log_px return z , log_px","title":"NeuralODE"},{"location":"api/flows/nonlinearities/","text":"Nonlinearities \u00a4 generax.flows.nonlinearities.Softplus ( BijectiveTransform ) \u00a4 Softplus( args, *kwargs) Source code in generax/flows/nonlinearities.py class Softplus ( BijectiveTransform ): def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == True : x = jnp . where ( x < 0.0 , 1e-5 , x ) dx = jnp . log1p ( - jnp . exp ( - x )) z = x + dx log_det = dx . sum () else : z = jax . nn . softplus ( x ) log_det = jnp . log1p ( - jnp . exp ( - z )) . sum () if inverse : log_det = - log_det return z , log_det __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. key : A jax.random.PRNGKey for initialization Source code in generax/flows/nonlinearities.py def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation Returns : (z, log_det) Source code in generax/flows/nonlinearities.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == True : x = jnp . where ( x < 0.0 , 1e-5 , x ) dx = jnp . log1p ( - jnp . exp ( - x )) z = x + dx log_det = dx . sum () else : z = jax . nn . softplus ( x ) log_det = jnp . log1p ( - jnp . exp ( - z )) . sum () if inverse : log_det = - log_det return z , log_det generax.flows.nonlinearities.GaussianCDF ( BijectiveTransform ) \u00a4 GaussianCDF( args, *kwargs) Source code in generax/flows/nonlinearities.py class GaussianCDF ( BijectiveTransform ): def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : z = jax . scipy . stats . norm . cdf ( x ) log_det = jax . scipy . stats . norm . logpdf ( x ) . sum () else : z = jax . scipy . stats . norm . ppf ( x ) log_det = jax . scipy . stats . norm . logpdf ( z ) . sum () if inverse : log_det = - log_det return z , log_det __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. key : A jax.random.PRNGKey for initialization Source code in generax/flows/nonlinearities.py def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation Returns : (z, log_det) Source code in generax/flows/nonlinearities.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : z = jax . scipy . stats . norm . cdf ( x ) log_det = jax . scipy . stats . norm . logpdf ( x ) . sum () else : z = jax . scipy . stats . norm . ppf ( x ) log_det = jax . scipy . stats . norm . logpdf ( z ) . sum () if inverse : log_det = - log_det return z , log_det generax.flows.nonlinearities.LogisticCDF ( BijectiveTransform ) \u00a4 LogisticCDF( args, *kwargs) Source code in generax/flows/nonlinearities.py class LogisticCDF ( BijectiveTransform ): def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : z = jax . scipy . stats . logistic . cdf ( x ) log_det = jax . scipy . stats . logistic . logpdf ( x ) . sum () else : z = jax . scipy . stats . logistic . ppf ( x ) log_det = jax . scipy . stats . logistic . logpdf ( z ) . sum () if inverse : log_det = - log_det return z , log_det __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. key : A jax.random.PRNGKey for initialization Source code in generax/flows/nonlinearities.py def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation Returns : (z, log_det) Source code in generax/flows/nonlinearities.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : z = jax . scipy . stats . logistic . cdf ( x ) log_det = jax . scipy . stats . logistic . logpdf ( x ) . sum () else : z = jax . scipy . stats . logistic . ppf ( x ) log_det = jax . scipy . stats . logistic . logpdf ( z ) . sum () if inverse : log_det = - log_det return z , log_det generax.flows.nonlinearities.LeakyReLU ( BijectiveTransform ) \u00a4 LeakyReLU( args, *kwargs) Source code in generax/flows/nonlinearities.py class LeakyReLU ( BijectiveTransform ): alpha : float def __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray , alpha : Optional [ float ] = 0.01 , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) self . alpha = alpha def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : z = jnp . where ( x > 0 , x , self . alpha * x ) else : z = jnp . where ( x > 0 , x , x / self . alpha ) log_dx_dz = jnp . where ( x > 0 , 0 , jnp . log ( self . alpha )) log_det = log_dx_dz . sum () if inverse : log_det = - log_det return z , log_det __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray , alpha : Optional [ float ] = 0.01 , ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. key : A jax.random.PRNGKey for initialization Source code in generax/flows/nonlinearities.py def __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray , alpha : Optional [ float ] = 0.01 , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) self . alpha = alpha __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation Returns : (z, log_det) Source code in generax/flows/nonlinearities.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : z = jnp . where ( x > 0 , x , self . alpha * x ) else : z = jnp . where ( x > 0 , x , x / self . alpha ) log_dx_dz = jnp . where ( x > 0 , 0 , jnp . log ( self . alpha )) log_det = log_dx_dz . sum () if inverse : log_det = - log_det return z , log_det generax.flows.nonlinearities.SneakyReLU ( BijectiveTransform ) \u00a4 Originally from https://invertibleworkshop.github.io/INNF_2019/accepted_papers/pdfs/INNF_2019_paper_26.pdf Source code in generax/flows/nonlinearities.py class SneakyReLU ( BijectiveTransform ): \"\"\" Originally from https://invertibleworkshop.github.io/INNF_2019/accepted_papers/pdfs/INNF_2019_paper_26.pdf \"\"\" alpha : float def __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray , alpha : Optional [ float ] = 0.01 , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) # Sneaky ReLU uses a different convention self . alpha = ( 1.0 - alpha ) / ( 1.0 + alpha ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : sqrt_1px2 = jnp . sqrt ( 1 + x ** 2 ) z = ( x + self . alpha * ( sqrt_1px2 - 1 )) / ( 1 + self . alpha ) log_det = jnp . log ( 1 + self . alpha * x / sqrt_1px2 ) - jnp . log ( 1 + self . alpha ) else : alpha_sq = self . alpha ** 2 b = ( 1 + self . alpha ) * x + self . alpha z = ( jnp . sqrt ( alpha_sq * ( 1 + b ** 2 - alpha_sq )) - b ) / ( alpha_sq - 1 ) sqrt_1px2 = jnp . sqrt ( 1 + z ** 2 ) log_det = jnp . log ( 1 + self . alpha * z / sqrt_1px2 ) - jnp . log ( 1 + self . alpha ) log_det = log_det . sum () if inverse : log_det = - log_det return z , log_det __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray , alpha : Optional [ float ] = 0.01 , ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. key : A jax.random.PRNGKey for initialization Source code in generax/flows/nonlinearities.py def __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray , alpha : Optional [ float ] = 0.01 , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) # Sneaky ReLU uses a different convention self . alpha = ( 1.0 - alpha ) / ( 1.0 + alpha ) __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation Returns : (z, log_det) Source code in generax/flows/nonlinearities.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : sqrt_1px2 = jnp . sqrt ( 1 + x ** 2 ) z = ( x + self . alpha * ( sqrt_1px2 - 1 )) / ( 1 + self . alpha ) log_det = jnp . log ( 1 + self . alpha * x / sqrt_1px2 ) - jnp . log ( 1 + self . alpha ) else : alpha_sq = self . alpha ** 2 b = ( 1 + self . alpha ) * x + self . alpha z = ( jnp . sqrt ( alpha_sq * ( 1 + b ** 2 - alpha_sq )) - b ) / ( alpha_sq - 1 ) sqrt_1px2 = jnp . sqrt ( 1 + z ** 2 ) log_det = jnp . log ( 1 + self . alpha * z / sqrt_1px2 ) - jnp . log ( 1 + self . alpha ) log_det = log_det . sum () if inverse : log_det = - log_det return z , log_det generax.flows.nonlinearities.SquarePlus ( BijectiveTransform ) \u00a4 SquarePlus( args, *kwargs) Source code in generax/flows/nonlinearities.py class SquarePlus ( BijectiveTransform ): gamma : float def __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray , gamma : Optional [ float ] = 0.5 , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) self . gamma = gamma def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : sqrt_arg = x ** 2 + 4 * self . gamma z = 0.5 * ( x + jnp . sqrt ( sqrt_arg )) z = jnp . maximum ( z , 0.0 ) dzdx = 0.5 * ( 1 + x * jax . lax . rsqrt ( sqrt_arg )) # Always positive dzdx = jnp . maximum ( dzdx , 1e-5 ) else : z = x - self . gamma / x dzdx = 0.5 * ( 1 + z * jax . lax . rsqrt ( z ** 2 + 4 * self . gamma )) log_det = jnp . log ( dzdx ) . sum () if inverse : log_det = - log_det return z , log_det __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray , gamma : Optional [ float ] = 0.5 , ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. key : A jax.random.PRNGKey for initialization Source code in generax/flows/nonlinearities.py def __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray , gamma : Optional [ float ] = 0.5 , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) self . gamma = gamma __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation Returns : (z, log_det) Source code in generax/flows/nonlinearities.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : sqrt_arg = x ** 2 + 4 * self . gamma z = 0.5 * ( x + jnp . sqrt ( sqrt_arg )) z = jnp . maximum ( z , 0.0 ) dzdx = 0.5 * ( 1 + x * jax . lax . rsqrt ( sqrt_arg )) # Always positive dzdx = jnp . maximum ( dzdx , 1e-5 ) else : z = x - self . gamma / x dzdx = 0.5 * ( 1 + z * jax . lax . rsqrt ( z ** 2 + 4 * self . gamma )) log_det = jnp . log ( dzdx ) . sum () if inverse : log_det = - log_det return z , log_det generax.flows.nonlinearities.SquareSigmoid ( BijectiveTransform ) \u00a4 SquareSigmoid( args, *kwargs) Source code in generax/flows/nonlinearities.py class SquareSigmoid ( BijectiveTransform ): gamma : float def __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray , gamma : Optional [ float ] = 0.5 , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) self . gamma = gamma def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : rsqrt = jax . lax . rsqrt ( x ** 2 + 4 * self . gamma ) z = 0.5 * ( 1 + x * rsqrt ) else : arg = 2 * x - 1 z = 2 * jnp . sqrt ( self . gamma ) * arg * jax . lax . rsqrt ( 1 - arg ** 2 ) rsqrt = jax . lax . rsqrt ( z ** 2 + 4 * self . gamma ) dzdx = 2 * self . gamma * rsqrt ** 3 log_det = jnp . log ( dzdx ) . sum () if inverse : log_det = - log_det return z , log_det __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray , gamma : Optional [ float ] = 0.5 , ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. key : A jax.random.PRNGKey for initialization Source code in generax/flows/nonlinearities.py def __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray , gamma : Optional [ float ] = 0.5 , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) self . gamma = gamma __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation Returns : (z, log_det) Source code in generax/flows/nonlinearities.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : rsqrt = jax . lax . rsqrt ( x ** 2 + 4 * self . gamma ) z = 0.5 * ( 1 + x * rsqrt ) else : arg = 2 * x - 1 z = 2 * jnp . sqrt ( self . gamma ) * arg * jax . lax . rsqrt ( 1 - arg ** 2 ) rsqrt = jax . lax . rsqrt ( z ** 2 + 4 * self . gamma ) dzdx = 2 * self . gamma * rsqrt ** 3 log_det = jnp . log ( dzdx ) . sum () if inverse : log_det = - log_det return z , log_det generax.flows.nonlinearities.SquareLogit ( SquareSigmoid ) \u00a4 SquareLogit( args, *kwargs) Source code in generax/flows/nonlinearities.py class SquareLogit ( SquareSigmoid ): def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : return super () . __call__ ( x , y = y , inverse = not inverse , ** kwargs ) __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray , gamma : Optional [ float ] = 0.5 , ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. key : A jax.random.PRNGKey for initialization Source code in generax/flows/nonlinearities.py def __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray , gamma : Optional [ float ] = 0.5 , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) self . gamma = gamma __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation Returns : (z, log_det) Source code in generax/flows/nonlinearities.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : return super () . __call__ ( x , y = y , inverse = not inverse , ** kwargs ) generax.flows.nonlinearities.SLog ( BijectiveTransform ) \u00a4 https://papers.nips.cc/paper/2019/file/b1f62fa99de9f27a048344d55c5ef7a6-Paper.pdf Source code in generax/flows/nonlinearities.py class SLog ( BijectiveTransform ): \"\"\" https://papers.nips.cc/paper/2019/file/b1f62fa99de9f27a048344d55c5ef7a6-Paper.pdf \"\"\" alpha : Union [ float , None ] def __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray , alpha : Optional [ float ] = 0.0 , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) self . alpha = alpha def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' # Bound alpha to be positive alpha = misc . square_plus ( self . alpha ) + 1e-4 if inverse == False : log_det = jnp . log1p ( alpha * jnp . abs ( x )) z = jnp . sign ( x ) / alpha * log_det else : z = jnp . sign ( x ) / alpha * ( jnp . exp ( alpha * jnp . abs ( x )) - 1 ) log_det = jnp . log1p ( alpha * jnp . abs ( z )) log_det = - log_det . sum () if inverse : log_det = - log_det return z , log_det __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray , alpha : Optional [ float ] = 0.0 , ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. key : A jax.random.PRNGKey for initialization Source code in generax/flows/nonlinearities.py def __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray , alpha : Optional [ float ] = 0.0 , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) self . alpha = alpha __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation Returns : (z, log_det) Source code in generax/flows/nonlinearities.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' # Bound alpha to be positive alpha = misc . square_plus ( self . alpha ) + 1e-4 if inverse == False : log_det = jnp . log1p ( alpha * jnp . abs ( x )) z = jnp . sign ( x ) / alpha * log_det else : z = jnp . sign ( x ) / alpha * ( jnp . exp ( alpha * jnp . abs ( x )) - 1 ) log_det = jnp . log1p ( alpha * jnp . abs ( z )) log_det = - log_det . sum () if inverse : log_det = - log_det return z , log_det generax.flows.logistic_cdf_mixture_logit.LogisticCDFMixtureLogit ( BijectiveTransform ) \u00a4 Used in Flow++ https://arxiv.org/pdf/1902.00275.pdf This is a logistic CDF mixture model followed by a logit. Attributes : - theta : The parameters of the transformation. Source code in generax/flows/logistic_cdf_mixture_logit.py class LogisticCDFMixtureLogit ( BijectiveTransform ): \"\"\"Used in Flow++ https://arxiv.org/pdf/1902.00275.pdf This is a logistic CDF mixture model followed by a logit. **Attributes**: - `theta`: The parameters of the transformation. \"\"\" theta : Array K : int = eqx . field ( static = True ) def __init__ ( self , input_shape : Tuple [ int ], K : int = 8 , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization - `K`: The number of knots to use. \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) self . K = K x_dim = util . list_prod ( input_shape ) self . theta = random . normal ( key , shape = ( x_dim * ( 3 * self . K ),)) * 0.1 def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' # Flatten x x = x . ravel () theta = self . theta . reshape ( x . shape + ( 3 * self . K ,)) # Split the parameters weight_logits , means , scales = theta [ ... ,: self . K ], theta [ ... , self . K : 2 * self . K ], theta [ ... , 2 * self . K :] scales = misc . square_plus ( scales , gamma = 1.0 ) + 1e-4 # Create the jvp function that we'll need def f_and_df ( x , * args ): primals = weight_logits , means , scales , x tangents = jax . tree_util . tree_map ( jnp . zeros_like , primals [: - 1 ]) + ( jnp . ones_like ( x ),) return jax . jvp ( logistic_cdf_mixture_logit , primals , tangents ) if inverse == False : # Only need a single pass z , dzdx = f_and_df ( x ) else : # Invert with bisection method. f = lambda x , * args : f_and_df ( x , * args )[ 0 ] lower , upper = - 1000.0 , 1000.0 lower , upper = jnp . broadcast_to ( lower , x . shape ), jnp . broadcast_to ( upper , x . shape ) z = util . bisection ( f , lower , upper , x ) reconstr , dzdx = f_and_df ( z ) ew_log_det = jnp . log ( dzdx ) log_det = ew_log_det . sum () if inverse : log_det *= - 1 # Unflatten the output z = z . reshape ( self . input_shape ) return z , log_det __init__ ( self , input_shape : Tuple [ int ], K : int = 8 , * , key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. key : A jax.random.PRNGKey for initialization K : The number of knots to use. Source code in generax/flows/logistic_cdf_mixture_logit.py def __init__ ( self , input_shape : Tuple [ int ], K : int = 8 , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization - `K`: The number of knots to use. \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) self . K = K x_dim = util . list_prod ( input_shape ) self . theta = random . normal ( key , shape = ( x_dim * ( 3 * self . K ),)) * 0.1 __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation Returns : (z, log_det) Source code in generax/flows/logistic_cdf_mixture_logit.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' # Flatten x x = x . ravel () theta = self . theta . reshape ( x . shape + ( 3 * self . K ,)) # Split the parameters weight_logits , means , scales = theta [ ... ,: self . K ], theta [ ... , self . K : 2 * self . K ], theta [ ... , 2 * self . K :] scales = misc . square_plus ( scales , gamma = 1.0 ) + 1e-4 # Create the jvp function that we'll need def f_and_df ( x , * args ): primals = weight_logits , means , scales , x tangents = jax . tree_util . tree_map ( jnp . zeros_like , primals [: - 1 ]) + ( jnp . ones_like ( x ),) return jax . jvp ( logistic_cdf_mixture_logit , primals , tangents ) if inverse == False : # Only need a single pass z , dzdx = f_and_df ( x ) else : # Invert with bisection method. f = lambda x , * args : f_and_df ( x , * args )[ 0 ] lower , upper = - 1000.0 , 1000.0 lower , upper = jnp . broadcast_to ( lower , x . shape ), jnp . broadcast_to ( upper , x . shape ) z = util . bisection ( f , lower , upper , x ) reconstr , dzdx = f_and_df ( z ) ew_log_det = jnp . log ( dzdx ) log_det = ew_log_det . sum () if inverse : log_det *= - 1 # Unflatten the output z = z . reshape ( self . input_shape ) return z , log_det generax.flows.spline.RationalQuadraticSpline ( BijectiveTransform ) \u00a4 Splines from https://arxiv.org/pdf/1906.04032.pdf. This is the best overall choice to use in flows. Attributes : - theta : The parameters of the spline. Source code in generax/flows/spline.py class RationalQuadraticSpline ( BijectiveTransform ): \"\"\"Splines from https://arxiv.org/pdf/1906.04032.pdf. This is the best overall choice to use in flows. **Attributes**: - `theta`: The parameters of the spline. \"\"\" theta : Array K : int = eqx . field ( static = True ) min_width : float = eqx . field ( static = True ) min_height : float = eqx . field ( static = True ) min_derivative : float = eqx . field ( static = True ) bounds : Sequence [ float ] = eqx . field ( static = True ) def __init__ ( self , input_shape : Tuple [ int ], K : int = 8 , min_width : Optional [ float ] = 1e-3 , min_height : Optional [ float ] = 1e-3 , min_derivative : Optional [ float ] = 1e-3 , bounds : Sequence [ float ] = (( - 10.0 , 10.0 ), ( - 10.0 , 10.0 )), * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization - `K`: The number of knots to use. - `min_width`: The minimum width of the knots. - `min_height`: The minimum height of the knots. - `min_derivative`: The minimum derivative of the knots. - `bounds`: The bounds of the splines. \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) self . K = K self . min_width = min_width self . min_height = min_height self . min_derivative = min_derivative self . bounds = bounds x_dim = util . list_prod ( input_shape ) self . theta = random . normal ( key , shape = ( x_dim * ( 3 * self . K - 1 ),)) * 0.1 def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' # Flatten x x = x . ravel () # Get the parameters settings = self . K , self . min_width , self . min_height , self . min_derivative , self . bounds theta = jnp . broadcast_to ( self . theta , x . shape + self . theta . shape ) knot_x , knot_y , knot_derivs = get_knot_params ( settings , theta ) # The relevant knot depends on if we are inverting or not if inverse == False : mask = ( x > self . bounds [ 0 ][ 0 ] + 1e-5 ) & ( x < self . bounds [ 0 ][ 1 ] - 1e-5 ) apply_fun = forward_spline else : mask = ( x > self . bounds [ 1 ][ 0 ] + 1e-5 ) & ( x < self . bounds [ 1 ][ 1 ] - 1e-5 ) apply_fun = inverse_spline args = find_knots ( x , knot_x , knot_y , knot_derivs , inverse ) z , dzdx = apply_fun ( x , mask , * args ) elementwise_log_det = jnp . log ( dzdx ) log_det = elementwise_log_det . sum () if inverse : log_det = - log_det # Unflatten the output z = z . reshape ( self . input_shape ) return z , log_det __init__ ( self , input_shape : Tuple [ int ], K : int = 8 , min_width : Optional [ float ] = 0.001 , min_height : Optional [ float ] = 0.001 , min_derivative : Optional [ float ] = 0.001 , bounds : Sequence [ float ] = (( - 10.0 , 10.0 ), ( - 10.0 , 10.0 )), * , key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : input_shape : The input shape. Output size is the same as shape. key : A jax.random.PRNGKey for initialization K : The number of knots to use. min_width : The minimum width of the knots. min_height : The minimum height of the knots. min_derivative : The minimum derivative of the knots. bounds : The bounds of the splines. Source code in generax/flows/spline.py def __init__ ( self , input_shape : Tuple [ int ], K : int = 8 , min_width : Optional [ float ] = 1e-3 , min_height : Optional [ float ] = 1e-3 , min_derivative : Optional [ float ] = 1e-3 , bounds : Sequence [ float ] = (( - 10.0 , 10.0 ), ( - 10.0 , 10.0 )), * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization - `K`: The number of knots to use. - `min_width`: The minimum width of the knots. - `min_height`: The minimum height of the knots. - `min_derivative`: The minimum derivative of the knots. - `bounds`: The bounds of the splines. \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) self . K = K self . min_width = min_width self . min_height = min_height self . min_derivative = min_derivative self . bounds = bounds x_dim = util . list_prod ( input_shape ) self . theta = random . normal ( key , shape = ( x_dim * ( 3 * self . K - 1 ),)) * 0.1 __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array \u00a4 Arguments : x : The input to the transformation y : The conditioning information inverse : Whether to inverse the transformation Returns : (z, log_det) Source code in generax/flows/spline.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' # Flatten x x = x . ravel () # Get the parameters settings = self . K , self . min_width , self . min_height , self . min_derivative , self . bounds theta = jnp . broadcast_to ( self . theta , x . shape + self . theta . shape ) knot_x , knot_y , knot_derivs = get_knot_params ( settings , theta ) # The relevant knot depends on if we are inverting or not if inverse == False : mask = ( x > self . bounds [ 0 ][ 0 ] + 1e-5 ) & ( x < self . bounds [ 0 ][ 1 ] - 1e-5 ) apply_fun = forward_spline else : mask = ( x > self . bounds [ 1 ][ 0 ] + 1e-5 ) & ( x < self . bounds [ 1 ][ 1 ] - 1e-5 ) apply_fun = inverse_spline args = find_knots ( x , knot_x , knot_y , knot_derivs , inverse ) z , dzdx = apply_fun ( x , mask , * args ) elementwise_log_det = jnp . log ( dzdx ) log_det = elementwise_log_det . sum () if inverse : log_det = - log_det # Unflatten the output z = z . reshape ( self . input_shape ) return z , log_det","title":"Nonlinearities"},{"location":"api/flows/nonlinearities/#nonlinearities","text":"","title":"Nonlinearities"},{"location":"api/flows/nonlinearities/#generax.flows.nonlinearities.Softplus","text":"Softplus( args, *kwargs) Source code in generax/flows/nonlinearities.py class Softplus ( BijectiveTransform ): def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == True : x = jnp . where ( x < 0.0 , 1e-5 , x ) dx = jnp . log1p ( - jnp . exp ( - x )) z = x + dx log_det = dx . sum () else : z = jax . nn . softplus ( x ) log_det = jnp . log1p ( - jnp . exp ( - z )) . sum () if inverse : log_det = - log_det return z , log_det","title":"Softplus"},{"location":"api/flows/nonlinearities/#generax.flows.nonlinearities.GaussianCDF","text":"GaussianCDF( args, *kwargs) Source code in generax/flows/nonlinearities.py class GaussianCDF ( BijectiveTransform ): def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : z = jax . scipy . stats . norm . cdf ( x ) log_det = jax . scipy . stats . norm . logpdf ( x ) . sum () else : z = jax . scipy . stats . norm . ppf ( x ) log_det = jax . scipy . stats . norm . logpdf ( z ) . sum () if inverse : log_det = - log_det return z , log_det","title":"GaussianCDF"},{"location":"api/flows/nonlinearities/#generax.flows.nonlinearities.LogisticCDF","text":"LogisticCDF( args, *kwargs) Source code in generax/flows/nonlinearities.py class LogisticCDF ( BijectiveTransform ): def __init__ ( self , input_shape : Tuple [ int ], key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : z = jax . scipy . stats . logistic . cdf ( x ) log_det = jax . scipy . stats . logistic . logpdf ( x ) . sum () else : z = jax . scipy . stats . logistic . ppf ( x ) log_det = jax . scipy . stats . logistic . logpdf ( z ) . sum () if inverse : log_det = - log_det return z , log_det","title":"LogisticCDF"},{"location":"api/flows/nonlinearities/#generax.flows.nonlinearities.LeakyReLU","text":"LeakyReLU( args, *kwargs) Source code in generax/flows/nonlinearities.py class LeakyReLU ( BijectiveTransform ): alpha : float def __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray , alpha : Optional [ float ] = 0.01 , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) self . alpha = alpha def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : z = jnp . where ( x > 0 , x , self . alpha * x ) else : z = jnp . where ( x > 0 , x , x / self . alpha ) log_dx_dz = jnp . where ( x > 0 , 0 , jnp . log ( self . alpha )) log_det = log_dx_dz . sum () if inverse : log_det = - log_det return z , log_det","title":"LeakyReLU"},{"location":"api/flows/nonlinearities/#generax.flows.nonlinearities.SneakyReLU","text":"Originally from https://invertibleworkshop.github.io/INNF_2019/accepted_papers/pdfs/INNF_2019_paper_26.pdf Source code in generax/flows/nonlinearities.py class SneakyReLU ( BijectiveTransform ): \"\"\" Originally from https://invertibleworkshop.github.io/INNF_2019/accepted_papers/pdfs/INNF_2019_paper_26.pdf \"\"\" alpha : float def __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray , alpha : Optional [ float ] = 0.01 , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) # Sneaky ReLU uses a different convention self . alpha = ( 1.0 - alpha ) / ( 1.0 + alpha ) def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : sqrt_1px2 = jnp . sqrt ( 1 + x ** 2 ) z = ( x + self . alpha * ( sqrt_1px2 - 1 )) / ( 1 + self . alpha ) log_det = jnp . log ( 1 + self . alpha * x / sqrt_1px2 ) - jnp . log ( 1 + self . alpha ) else : alpha_sq = self . alpha ** 2 b = ( 1 + self . alpha ) * x + self . alpha z = ( jnp . sqrt ( alpha_sq * ( 1 + b ** 2 - alpha_sq )) - b ) / ( alpha_sq - 1 ) sqrt_1px2 = jnp . sqrt ( 1 + z ** 2 ) log_det = jnp . log ( 1 + self . alpha * z / sqrt_1px2 ) - jnp . log ( 1 + self . alpha ) log_det = log_det . sum () if inverse : log_det = - log_det return z , log_det","title":"SneakyReLU"},{"location":"api/flows/nonlinearities/#generax.flows.nonlinearities.SquarePlus","text":"SquarePlus( args, *kwargs) Source code in generax/flows/nonlinearities.py class SquarePlus ( BijectiveTransform ): gamma : float def __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray , gamma : Optional [ float ] = 0.5 , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) self . gamma = gamma def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : sqrt_arg = x ** 2 + 4 * self . gamma z = 0.5 * ( x + jnp . sqrt ( sqrt_arg )) z = jnp . maximum ( z , 0.0 ) dzdx = 0.5 * ( 1 + x * jax . lax . rsqrt ( sqrt_arg )) # Always positive dzdx = jnp . maximum ( dzdx , 1e-5 ) else : z = x - self . gamma / x dzdx = 0.5 * ( 1 + z * jax . lax . rsqrt ( z ** 2 + 4 * self . gamma )) log_det = jnp . log ( dzdx ) . sum () if inverse : log_det = - log_det return z , log_det","title":"SquarePlus"},{"location":"api/flows/nonlinearities/#generax.flows.nonlinearities.SquareSigmoid","text":"SquareSigmoid( args, *kwargs) Source code in generax/flows/nonlinearities.py class SquareSigmoid ( BijectiveTransform ): gamma : float def __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray , gamma : Optional [ float ] = 0.5 , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) self . gamma = gamma def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' if inverse == False : rsqrt = jax . lax . rsqrt ( x ** 2 + 4 * self . gamma ) z = 0.5 * ( 1 + x * rsqrt ) else : arg = 2 * x - 1 z = 2 * jnp . sqrt ( self . gamma ) * arg * jax . lax . rsqrt ( 1 - arg ** 2 ) rsqrt = jax . lax . rsqrt ( z ** 2 + 4 * self . gamma ) dzdx = 2 * self . gamma * rsqrt ** 3 log_det = jnp . log ( dzdx ) . sum () if inverse : log_det = - log_det return z , log_det","title":"SquareSigmoid"},{"location":"api/flows/nonlinearities/#generax.flows.nonlinearities.SquareLogit","text":"SquareLogit( args, *kwargs) Source code in generax/flows/nonlinearities.py class SquareLogit ( SquareSigmoid ): def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : return super () . __call__ ( x , y = y , inverse = not inverse , ** kwargs )","title":"SquareLogit"},{"location":"api/flows/nonlinearities/#generax.flows.nonlinearities.SLog","text":"https://papers.nips.cc/paper/2019/file/b1f62fa99de9f27a048344d55c5ef7a6-Paper.pdf Source code in generax/flows/nonlinearities.py class SLog ( BijectiveTransform ): \"\"\" https://papers.nips.cc/paper/2019/file/b1f62fa99de9f27a048344d55c5ef7a6-Paper.pdf \"\"\" alpha : Union [ float , None ] def __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray , alpha : Optional [ float ] = 0.0 , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) self . alpha = alpha def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' # Bound alpha to be positive alpha = misc . square_plus ( self . alpha ) + 1e-4 if inverse == False : log_det = jnp . log1p ( alpha * jnp . abs ( x )) z = jnp . sign ( x ) / alpha * log_det else : z = jnp . sign ( x ) / alpha * ( jnp . exp ( alpha * jnp . abs ( x )) - 1 ) log_det = jnp . log1p ( alpha * jnp . abs ( z )) log_det = - log_det . sum () if inverse : log_det = - log_det return z , log_det","title":"SLog"},{"location":"api/flows/nonlinearities/#generax.flows.logistic_cdf_mixture_logit.LogisticCDFMixtureLogit","text":"Used in Flow++ https://arxiv.org/pdf/1902.00275.pdf This is a logistic CDF mixture model followed by a logit. Attributes : - theta : The parameters of the transformation. Source code in generax/flows/logistic_cdf_mixture_logit.py class LogisticCDFMixtureLogit ( BijectiveTransform ): \"\"\"Used in Flow++ https://arxiv.org/pdf/1902.00275.pdf This is a logistic CDF mixture model followed by a logit. **Attributes**: - `theta`: The parameters of the transformation. \"\"\" theta : Array K : int = eqx . field ( static = True ) def __init__ ( self , input_shape : Tuple [ int ], K : int = 8 , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization - `K`: The number of knots to use. \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) self . K = K x_dim = util . list_prod ( input_shape ) self . theta = random . normal ( key , shape = ( x_dim * ( 3 * self . K ),)) * 0.1 def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' # Flatten x x = x . ravel () theta = self . theta . reshape ( x . shape + ( 3 * self . K ,)) # Split the parameters weight_logits , means , scales = theta [ ... ,: self . K ], theta [ ... , self . K : 2 * self . K ], theta [ ... , 2 * self . K :] scales = misc . square_plus ( scales , gamma = 1.0 ) + 1e-4 # Create the jvp function that we'll need def f_and_df ( x , * args ): primals = weight_logits , means , scales , x tangents = jax . tree_util . tree_map ( jnp . zeros_like , primals [: - 1 ]) + ( jnp . ones_like ( x ),) return jax . jvp ( logistic_cdf_mixture_logit , primals , tangents ) if inverse == False : # Only need a single pass z , dzdx = f_and_df ( x ) else : # Invert with bisection method. f = lambda x , * args : f_and_df ( x , * args )[ 0 ] lower , upper = - 1000.0 , 1000.0 lower , upper = jnp . broadcast_to ( lower , x . shape ), jnp . broadcast_to ( upper , x . shape ) z = util . bisection ( f , lower , upper , x ) reconstr , dzdx = f_and_df ( z ) ew_log_det = jnp . log ( dzdx ) log_det = ew_log_det . sum () if inverse : log_det *= - 1 # Unflatten the output z = z . reshape ( self . input_shape ) return z , log_det","title":"LogisticCDFMixtureLogit"},{"location":"api/flows/nonlinearities/#generax.flows.spline.RationalQuadraticSpline","text":"Splines from https://arxiv.org/pdf/1906.04032.pdf. This is the best overall choice to use in flows. Attributes : - theta : The parameters of the spline. Source code in generax/flows/spline.py class RationalQuadraticSpline ( BijectiveTransform ): \"\"\"Splines from https://arxiv.org/pdf/1906.04032.pdf. This is the best overall choice to use in flows. **Attributes**: - `theta`: The parameters of the spline. \"\"\" theta : Array K : int = eqx . field ( static = True ) min_width : float = eqx . field ( static = True ) min_height : float = eqx . field ( static = True ) min_derivative : float = eqx . field ( static = True ) bounds : Sequence [ float ] = eqx . field ( static = True ) def __init__ ( self , input_shape : Tuple [ int ], K : int = 8 , min_width : Optional [ float ] = 1e-3 , min_height : Optional [ float ] = 1e-3 , min_derivative : Optional [ float ] = 1e-3 , bounds : Sequence [ float ] = (( - 10.0 , 10.0 ), ( - 10.0 , 10.0 )), * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input shape. Output size is the same as shape. - `key`: A `jax.random.PRNGKey` for initialization - `K`: The number of knots to use. - `min_width`: The minimum width of the knots. - `min_height`: The minimum height of the knots. - `min_derivative`: The minimum derivative of the knots. - `bounds`: The bounds of the splines. \"\"\" super () . __init__ ( input_shape = input_shape , ** kwargs ) self . K = K self . min_width = min_width self . min_height = min_height self . min_derivative = min_derivative self . bounds = bounds x_dim = util . list_prod ( input_shape ) self . theta = random . normal ( key , shape = ( x_dim * ( 3 * self . K - 1 ),)) * 0.1 def __call__ ( self , x : Array , y : Optional [ Array ] = None , inverse : bool = False , ** kwargs ) -> Array : \"\"\"**Arguments**: - `x`: The input to the transformation - `y`: The conditioning information - `inverse`: Whether to inverse the transformation **Returns**: `(z, log_det)` \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' # Flatten x x = x . ravel () # Get the parameters settings = self . K , self . min_width , self . min_height , self . min_derivative , self . bounds theta = jnp . broadcast_to ( self . theta , x . shape + self . theta . shape ) knot_x , knot_y , knot_derivs = get_knot_params ( settings , theta ) # The relevant knot depends on if we are inverting or not if inverse == False : mask = ( x > self . bounds [ 0 ][ 0 ] + 1e-5 ) & ( x < self . bounds [ 0 ][ 1 ] - 1e-5 ) apply_fun = forward_spline else : mask = ( x > self . bounds [ 1 ][ 0 ] + 1e-5 ) & ( x < self . bounds [ 1 ][ 1 ] - 1e-5 ) apply_fun = inverse_spline args = find_knots ( x , knot_x , knot_y , knot_derivs , inverse ) z , dzdx = apply_fun ( x , mask , * args ) elementwise_log_det = jnp . log ( dzdx ) log_det = elementwise_log_det . sum () if inverse : log_det = - log_det # Unflatten the output z = z . reshape ( self . input_shape ) return z , log_det","title":"RationalQuadraticSpline"},{"location":"api/misc/trainer/","text":"Trainer \u00a4 generax.trainer.TrainingState \u00a4 TrainingState( args, *kwargs) Source code in generax/trainer.py class TrainingState ( eqx . Module ): i : float key : PRNGKeyArray model : eqx . Module opt_state : optax . OptState def __init__ ( self , i : float , # float so that it is not treated as static key : PRNGKeyArray , model : eqx . Module , opt_state : optax . OptState ): self . i = i self . key = key self . model = model self . opt_state = opt_state __init__ ( self , i : float , key : PRNGKeyArray , model : Module , opt_state : Union [ jax . Array , numpy . ndarray , numpy . bool_ , numpy . number , Iterable [ ArrayTree ], Mapping [ Any , ArrayTree ]]) \u00a4 Source code in generax/trainer.py def __init__ ( self , i : float , # float so that it is not treated as static key : PRNGKeyArray , model : eqx . Module , opt_state : optax . OptState ): self . i = i self . key = key self . model = model self . opt_state = opt_state generax.trainer.Checkpointer \u00a4 Checkpointer( args, *kwargs) Source code in generax/trainer.py class Checkpointer ( eqx . Module ): save_path : str model_folder : str def __init__ ( self , save_path : str ): self . save_path = save_path self . model_folder = os . path . join ( save_path , 'models' ) misc . ensure_path_exists ( self . model_folder ) @property def saved_model_path ( self ): return os . path . join ( self . model_folder , 'saved_model.pickle' ) def save_eqx_module ( self , model : eqx . Module ): eqx . tree_serialise_leaves ( self . saved_model_path , model ) def load_eqx_module ( self , model_example : eqx . Module ): return eqx . tree_deserialise_leaves ( self . saved_model_path , model_example ) __init__ ( self , save_path : str ) \u00a4 Source code in generax/trainer.py def __init__ ( self , save_path : str ): self . save_path = save_path self . model_folder = os . path . join ( save_path , 'models' ) misc . ensure_path_exists ( self . model_folder ) save_eqx_module ( self , model : Module ) \u00a4 Source code in generax/trainer.py def save_eqx_module ( self , model : eqx . Module ): eqx . tree_serialise_leaves ( self . saved_model_path , model ) load_eqx_module ( self , model_example : Module ) \u00a4 Source code in generax/trainer.py def load_eqx_module ( self , model_example : eqx . Module ): return eqx . tree_deserialise_leaves ( self . saved_model_path , model_example ) generax.trainer.Trainer \u00a4 Class that will monitor training and handle checkpointing. Attributes : checkpointer : Object that saves checkpoints of the model Source code in generax/trainer.py class Trainer ( eqx . Module ): \"\"\"Class that will monitor training and handle checkpointing. **Attributes**: - `checkpointer`: Object that saves checkpoints of the model \"\"\" checkpointer : Checkpointer def __init__ ( self , checkpoint_path : str ): self . checkpointer = Checkpointer ( checkpoint_path ) def train_step ( self , objective : Callable , optimizer : optax . GradientTransformation , train_state : TrainingState , data : Dict [ str , Array ]) -> Tuple [ TrainingState , Mapping [ str , Any ]]: i , model , opt_state = train_state . i , train_state . model , train_state . opt_state train_key , next_key = random . split ( train_state . key ) # Compute the gradients of the objective ( obj , aux ), grads = eqx . filter_value_and_grad ( objective , has_aux = True )( model , data , train_key ) aux [ 'objective' ] = obj # Update the model updates , new_opt_state = optimizer . update ( grads , opt_state , model ) new_model = eqx . apply_updates ( model , updates ) # Package the updated training state updated_train_state = TrainingState ( i = i + 1 , key = next_key , model = new_model , opt_state = new_opt_state ) return updated_train_state , aux def train ( self , model : eqx . Module , objective : Callable , evaluate_model : Callable , optimizer : optax . GradientTransformation , num_steps : int , data_iterator : Iterator , double_batch : int = - 1 , checkpoint_every : int = 1000 , test_every : int = 1000 , retrain : bool = False ): \"\"\"Train the model. This will load the model if the most recent checkpoint exists has completed training. **Arguments**: - `model`: The model to train - `objective`: The objective function to optimize - `evaluate_model`: A function that takes in the model and evaluates it on a test set - `optimizer`: The optimizer to use - `num_steps`: The number of training steps to take - `data_iterator`: An iterator that yields batches of data - `double_batch`: If `double_batch > 0`, then we will take `double_batch` batches of data at a time and train over them in a fast `jax.lax.scan` loop. - `checkpoint_every`: How often to checkpoint the model - `test_every`: How often to evaluate the model - `retrain`: Whether to force retraining from scratch \"\"\" key0 = random . PRNGKey ( 0 ) # Load the most recent checkpoint opt_state = optimizer . init ( eqx . filter ( model , eqx . is_inexact_array )) train_state = TrainingState ( jnp . array ( 0.0 ), key0 , model , opt_state ) # Load the most recent checkpoint if retrain == False : train_state = self . restore ( train_state ) # Fill in the training step with the objective and optimizer train_step = eqx . Partial ( self . train_step , objective , optimizer ) if double_batch == - 1 : # JIT the training update here train_step = eqx . filter_jit ( train_step ) else : # We can only pass in parameters dynamically to the scan loop, so we # need to extract the static values here (because they won't change) # and combine later inside the scan loop _ , static = eqx . partition ( train_state , eqx . is_array ) # Construct the scan loop that we'll use to process batches of data def step ( params , data ): train_state = eqx . combine ( params , static ) new_train_state , aux = train_step ( train_state , data ) new_params , _ = eqx . partition ( new_train_state , eqx . is_array ) return new_params , aux scan_step = partial ( jax . lax . scan , step ) scan_step = jax . jit ( scan_step ) # Construct the progress bar start = int ( train_state . i ) if retrain == False else 0 if double_batch <= 0 : pbar = tqdm . tqdm ( jnp . arange ( start , num_steps ), total = num_steps - start ) else : pbar = tqdm . tqdm ( jnp . arange ( start , num_steps , double_batch ), total = num_steps - start ) # Training loop for i in pbar : # Take a training step if double_batch == - 1 : data = next ( data_iterator ) train_state , aux = train_step ( train_state , data ) pbar . update ( 1 ) else : data = misc . extract_multiple_batches_from_iterator ( data_iterator , double_batch ) params , static = eqx . partition ( train_state , eqx . is_array ) params , aux = scan_step ( params , data ) train_state = eqx . combine ( params , static ) pbar . update ( double_batch ) # Update the progress bar loss = aux [ 'objective' ] . mean () pbar . set_description ( f 'loss: { loss : .4f } ' ) # Checkpoint the model if ( i and ( i % checkpoint_every == 0 )): self . checkpoint ( train_state ) print ( 'Checkpointed model' ) # Evaluate the model if ( i % test_every == 0 ) or ( i == num_steps - 1 ): evaluate_model ( train_state . model ) # Final checkpoint self . checkpoint ( train_state ) print ( 'Checkpointed model' ) return train_state . model def checkpoint ( self , train_state : TrainingState ): self . checkpointer . save_eqx_module ( train_state ) def restore ( self , train_state : TrainingState ) -> TrainingState : train_state = self . checkpointer . load_eqx_module ( train_state ) print ( f 'Restored train_state { self . checkpointer . saved_model_path } ' ) return train_state __init__ ( self , checkpoint_path : str ) \u00a4 Source code in generax/trainer.py def __init__ ( self , checkpoint_path : str ): self . checkpointer = Checkpointer ( checkpoint_path ) train_step ( self , objective : Callable , optimizer : GradientTransformation , train_state : TrainingState , data : Dict [ str , Array ]) -> Tuple [ TrainingState , Mapping [ str , Any ]] \u00a4 Source code in generax/trainer.py def train_step ( self , objective : Callable , optimizer : optax . GradientTransformation , train_state : TrainingState , data : Dict [ str , Array ]) -> Tuple [ TrainingState , Mapping [ str , Any ]]: i , model , opt_state = train_state . i , train_state . model , train_state . opt_state train_key , next_key = random . split ( train_state . key ) # Compute the gradients of the objective ( obj , aux ), grads = eqx . filter_value_and_grad ( objective , has_aux = True )( model , data , train_key ) aux [ 'objective' ] = obj # Update the model updates , new_opt_state = optimizer . update ( grads , opt_state , model ) new_model = eqx . apply_updates ( model , updates ) # Package the updated training state updated_train_state = TrainingState ( i = i + 1 , key = next_key , model = new_model , opt_state = new_opt_state ) return updated_train_state , aux train ( self , model : Module , objective : Callable , evaluate_model : Callable , optimizer : GradientTransformation , num_steps : int , data_iterator : Iterator , double_batch : int = - 1 , checkpoint_every : int = 1000 , test_every : int = 1000 , retrain : bool = False ) \u00a4 Train the model. This will load the model if the most recent checkpoint exists has completed training. Arguments : model : The model to train objective : The objective function to optimize evaluate_model : A function that takes in the model and evaluates it on a test set optimizer : The optimizer to use num_steps : The number of training steps to take data_iterator : An iterator that yields batches of data double_batch : If double_batch > 0 , then we will take double_batch batches of data at a time and train over them in a fast jax.lax.scan loop. checkpoint_every : How often to checkpoint the model test_every : How often to evaluate the model retrain : Whether to force retraining from scratch Source code in generax/trainer.py def train ( self , model : eqx . Module , objective : Callable , evaluate_model : Callable , optimizer : optax . GradientTransformation , num_steps : int , data_iterator : Iterator , double_batch : int = - 1 , checkpoint_every : int = 1000 , test_every : int = 1000 , retrain : bool = False ): \"\"\"Train the model. This will load the model if the most recent checkpoint exists has completed training. **Arguments**: - `model`: The model to train - `objective`: The objective function to optimize - `evaluate_model`: A function that takes in the model and evaluates it on a test set - `optimizer`: The optimizer to use - `num_steps`: The number of training steps to take - `data_iterator`: An iterator that yields batches of data - `double_batch`: If `double_batch > 0`, then we will take `double_batch` batches of data at a time and train over them in a fast `jax.lax.scan` loop. - `checkpoint_every`: How often to checkpoint the model - `test_every`: How often to evaluate the model - `retrain`: Whether to force retraining from scratch \"\"\" key0 = random . PRNGKey ( 0 ) # Load the most recent checkpoint opt_state = optimizer . init ( eqx . filter ( model , eqx . is_inexact_array )) train_state = TrainingState ( jnp . array ( 0.0 ), key0 , model , opt_state ) # Load the most recent checkpoint if retrain == False : train_state = self . restore ( train_state ) # Fill in the training step with the objective and optimizer train_step = eqx . Partial ( self . train_step , objective , optimizer ) if double_batch == - 1 : # JIT the training update here train_step = eqx . filter_jit ( train_step ) else : # We can only pass in parameters dynamically to the scan loop, so we # need to extract the static values here (because they won't change) # and combine later inside the scan loop _ , static = eqx . partition ( train_state , eqx . is_array ) # Construct the scan loop that we'll use to process batches of data def step ( params , data ): train_state = eqx . combine ( params , static ) new_train_state , aux = train_step ( train_state , data ) new_params , _ = eqx . partition ( new_train_state , eqx . is_array ) return new_params , aux scan_step = partial ( jax . lax . scan , step ) scan_step = jax . jit ( scan_step ) # Construct the progress bar start = int ( train_state . i ) if retrain == False else 0 if double_batch <= 0 : pbar = tqdm . tqdm ( jnp . arange ( start , num_steps ), total = num_steps - start ) else : pbar = tqdm . tqdm ( jnp . arange ( start , num_steps , double_batch ), total = num_steps - start ) # Training loop for i in pbar : # Take a training step if double_batch == - 1 : data = next ( data_iterator ) train_state , aux = train_step ( train_state , data ) pbar . update ( 1 ) else : data = misc . extract_multiple_batches_from_iterator ( data_iterator , double_batch ) params , static = eqx . partition ( train_state , eqx . is_array ) params , aux = scan_step ( params , data ) train_state = eqx . combine ( params , static ) pbar . update ( double_batch ) # Update the progress bar loss = aux [ 'objective' ] . mean () pbar . set_description ( f 'loss: { loss : .4f } ' ) # Checkpoint the model if ( i and ( i % checkpoint_every == 0 )): self . checkpoint ( train_state ) print ( 'Checkpointed model' ) # Evaluate the model if ( i % test_every == 0 ) or ( i == num_steps - 1 ): evaluate_model ( train_state . model ) # Final checkpoint self . checkpoint ( train_state ) print ( 'Checkpointed model' ) return train_state . model checkpoint ( self , train_state : TrainingState ) \u00a4 Source code in generax/trainer.py def checkpoint ( self , train_state : TrainingState ): self . checkpointer . save_eqx_module ( train_state ) restore ( self , train_state : TrainingState ) -> TrainingState \u00a4 Source code in generax/trainer.py def restore ( self , train_state : TrainingState ) -> TrainingState : train_state = self . checkpointer . load_eqx_module ( train_state ) print ( f 'Restored train_state { self . checkpointer . saved_model_path } ' ) return train_state","title":"Trainer"},{"location":"api/misc/trainer/#trainer","text":"","title":"Trainer"},{"location":"api/misc/trainer/#generax.trainer.TrainingState","text":"TrainingState( args, *kwargs) Source code in generax/trainer.py class TrainingState ( eqx . Module ): i : float key : PRNGKeyArray model : eqx . Module opt_state : optax . OptState def __init__ ( self , i : float , # float so that it is not treated as static key : PRNGKeyArray , model : eqx . Module , opt_state : optax . OptState ): self . i = i self . key = key self . model = model self . opt_state = opt_state","title":"TrainingState"},{"location":"api/misc/trainer/#generax.trainer.Checkpointer","text":"Checkpointer( args, *kwargs) Source code in generax/trainer.py class Checkpointer ( eqx . Module ): save_path : str model_folder : str def __init__ ( self , save_path : str ): self . save_path = save_path self . model_folder = os . path . join ( save_path , 'models' ) misc . ensure_path_exists ( self . model_folder ) @property def saved_model_path ( self ): return os . path . join ( self . model_folder , 'saved_model.pickle' ) def save_eqx_module ( self , model : eqx . Module ): eqx . tree_serialise_leaves ( self . saved_model_path , model ) def load_eqx_module ( self , model_example : eqx . Module ): return eqx . tree_deserialise_leaves ( self . saved_model_path , model_example )","title":"Checkpointer"},{"location":"api/misc/trainer/#generax.trainer.Trainer","text":"Class that will monitor training and handle checkpointing. Attributes : checkpointer : Object that saves checkpoints of the model Source code in generax/trainer.py class Trainer ( eqx . Module ): \"\"\"Class that will monitor training and handle checkpointing. **Attributes**: - `checkpointer`: Object that saves checkpoints of the model \"\"\" checkpointer : Checkpointer def __init__ ( self , checkpoint_path : str ): self . checkpointer = Checkpointer ( checkpoint_path ) def train_step ( self , objective : Callable , optimizer : optax . GradientTransformation , train_state : TrainingState , data : Dict [ str , Array ]) -> Tuple [ TrainingState , Mapping [ str , Any ]]: i , model , opt_state = train_state . i , train_state . model , train_state . opt_state train_key , next_key = random . split ( train_state . key ) # Compute the gradients of the objective ( obj , aux ), grads = eqx . filter_value_and_grad ( objective , has_aux = True )( model , data , train_key ) aux [ 'objective' ] = obj # Update the model updates , new_opt_state = optimizer . update ( grads , opt_state , model ) new_model = eqx . apply_updates ( model , updates ) # Package the updated training state updated_train_state = TrainingState ( i = i + 1 , key = next_key , model = new_model , opt_state = new_opt_state ) return updated_train_state , aux def train ( self , model : eqx . Module , objective : Callable , evaluate_model : Callable , optimizer : optax . GradientTransformation , num_steps : int , data_iterator : Iterator , double_batch : int = - 1 , checkpoint_every : int = 1000 , test_every : int = 1000 , retrain : bool = False ): \"\"\"Train the model. This will load the model if the most recent checkpoint exists has completed training. **Arguments**: - `model`: The model to train - `objective`: The objective function to optimize - `evaluate_model`: A function that takes in the model and evaluates it on a test set - `optimizer`: The optimizer to use - `num_steps`: The number of training steps to take - `data_iterator`: An iterator that yields batches of data - `double_batch`: If `double_batch > 0`, then we will take `double_batch` batches of data at a time and train over them in a fast `jax.lax.scan` loop. - `checkpoint_every`: How often to checkpoint the model - `test_every`: How often to evaluate the model - `retrain`: Whether to force retraining from scratch \"\"\" key0 = random . PRNGKey ( 0 ) # Load the most recent checkpoint opt_state = optimizer . init ( eqx . filter ( model , eqx . is_inexact_array )) train_state = TrainingState ( jnp . array ( 0.0 ), key0 , model , opt_state ) # Load the most recent checkpoint if retrain == False : train_state = self . restore ( train_state ) # Fill in the training step with the objective and optimizer train_step = eqx . Partial ( self . train_step , objective , optimizer ) if double_batch == - 1 : # JIT the training update here train_step = eqx . filter_jit ( train_step ) else : # We can only pass in parameters dynamically to the scan loop, so we # need to extract the static values here (because they won't change) # and combine later inside the scan loop _ , static = eqx . partition ( train_state , eqx . is_array ) # Construct the scan loop that we'll use to process batches of data def step ( params , data ): train_state = eqx . combine ( params , static ) new_train_state , aux = train_step ( train_state , data ) new_params , _ = eqx . partition ( new_train_state , eqx . is_array ) return new_params , aux scan_step = partial ( jax . lax . scan , step ) scan_step = jax . jit ( scan_step ) # Construct the progress bar start = int ( train_state . i ) if retrain == False else 0 if double_batch <= 0 : pbar = tqdm . tqdm ( jnp . arange ( start , num_steps ), total = num_steps - start ) else : pbar = tqdm . tqdm ( jnp . arange ( start , num_steps , double_batch ), total = num_steps - start ) # Training loop for i in pbar : # Take a training step if double_batch == - 1 : data = next ( data_iterator ) train_state , aux = train_step ( train_state , data ) pbar . update ( 1 ) else : data = misc . extract_multiple_batches_from_iterator ( data_iterator , double_batch ) params , static = eqx . partition ( train_state , eqx . is_array ) params , aux = scan_step ( params , data ) train_state = eqx . combine ( params , static ) pbar . update ( double_batch ) # Update the progress bar loss = aux [ 'objective' ] . mean () pbar . set_description ( f 'loss: { loss : .4f } ' ) # Checkpoint the model if ( i and ( i % checkpoint_every == 0 )): self . checkpoint ( train_state ) print ( 'Checkpointed model' ) # Evaluate the model if ( i % test_every == 0 ) or ( i == num_steps - 1 ): evaluate_model ( train_state . model ) # Final checkpoint self . checkpoint ( train_state ) print ( 'Checkpointed model' ) return train_state . model def checkpoint ( self , train_state : TrainingState ): self . checkpointer . save_eqx_module ( train_state ) def restore ( self , train_state : TrainingState ) -> TrainingState : train_state = self . checkpointer . load_eqx_module ( train_state ) print ( f 'Restored train_state { self . checkpointer . saved_model_path } ' ) return train_state","title":"Trainer"},{"location":"api/nn/layers/","text":"Neural network layers \u00a4 generax.nn.layers.WeightNormDense \u00a4 Weight normalization parametrized linear layer https://arxiv.org/pdf/1602.07868.pdf Source code in generax/nn/layers.py class WeightNormDense ( eqx . Module ): \"\"\"Weight normalization parametrized linear layer https://arxiv.org/pdf/1602.07868.pdf \"\"\" in_size : int = eqx . field ( static = True ) out_size : int = eqx . field ( static = True ) W : Array b : Array g : Array def __init__ ( self , in_size : int , out_size : int , key : PRNGKeyArray , ** kwargs ): super () . __init__ ( ** kwargs ) self . in_size = in_size self . out_size = out_size w_init = jax . nn . initializers . he_uniform ( in_axis =- 2 , out_axis =- 1 ) self . W = w_init ( key , shape = ( out_size , in_size )) self . g = jnp . array ( 1.0 ) self . b = jnp . zeros ( out_size ) def data_dependent_init ( self , x : Array , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ - 1 ] == self . in_size , 'Only works on batched data' # Initialize g and b. W = self . W * jax . lax . rsqrt (( self . W ** 2 ) . sum ( axis = 1 ))[:, None ] x = jnp . einsum ( 'ij,bj->bi' , W , x ) std = jnp . std ( x . reshape (( - 1 , x . shape [ - 1 ])), axis = 0 ) + 1e-5 g = 1 / std x *= g mean = jnp . mean ( x . reshape (( - 1 , x . shape [ - 1 ])), axis = 0 ) b = - mean # Turn the new parameters into a new module get_g = lambda tree : tree . g get_b = lambda tree : tree . b updated_layer = eqx . tree_at ( get_g , self , g ) updated_layer = eqx . tree_at ( get_b , updated_layer , b ) return updated_layer def __call__ ( self , x : Array , y : Array = None ) -> Array : W = self . W * jax . lax . rsqrt (( self . W ** 2 ) . sum ( axis = 1 ))[:, None ] x = self . g * ( W @x ) + self . b return x __init__ ( self , in_size : int , out_size : int , key : PRNGKeyArray , ** kwargs ) \u00a4 Source code in generax/nn/layers.py def __init__ ( self , in_size : int , out_size : int , key : PRNGKeyArray , ** kwargs ): super () . __init__ ( ** kwargs ) self . in_size = in_size self . out_size = out_size w_init = jax . nn . initializers . he_uniform ( in_axis =- 2 , out_axis =- 1 ) self . W = w_init ( key , shape = ( out_size , in_size )) self . g = jnp . array ( 1.0 ) self . b = jnp . zeros ( out_size ) data_dependent_init ( self , x : Array , key : PRNGKeyArray = None ) -> Module \u00a4 Initialize the parameters of the layer based on the data. Arguments : x : The data to initialize the parameters with. key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/nn/layers.py def data_dependent_init ( self , x : Array , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ - 1 ] == self . in_size , 'Only works on batched data' # Initialize g and b. W = self . W * jax . lax . rsqrt (( self . W ** 2 ) . sum ( axis = 1 ))[:, None ] x = jnp . einsum ( 'ij,bj->bi' , W , x ) std = jnp . std ( x . reshape (( - 1 , x . shape [ - 1 ])), axis = 0 ) + 1e-5 g = 1 / std x *= g mean = jnp . mean ( x . reshape (( - 1 , x . shape [ - 1 ])), axis = 0 ) b = - mean # Turn the new parameters into a new module get_g = lambda tree : tree . g get_b = lambda tree : tree . b updated_layer = eqx . tree_at ( get_g , self , g ) updated_layer = eqx . tree_at ( get_b , updated_layer , b ) return updated_layer __call__ ( self , x : Array , y : Array = None ) -> Array \u00a4 Call self as a function. Source code in generax/nn/layers.py def __call__ ( self , x : Array , y : Array = None ) -> Array : W = self . W * jax . lax . rsqrt (( self . W ** 2 ) . sum ( axis = 1 ))[:, None ] x = self . g * ( W @x ) + self . b return x generax.nn.layers.WeightNormConv \u00a4 Weight normalization parametrized convolutional layer https://arxiv.org/pdf/1602.07868.pdf Source code in generax/nn/layers.py class WeightNormConv ( eqx . Module ): \"\"\"Weight normalization parametrized convolutional layer https://arxiv.org/pdf/1602.07868.pdf \"\"\" input_shape : int = eqx . field ( static = True ) out_size : int = eqx . field ( static = True ) filter_shape : Tuple [ int ] = eqx . field ( static = True ) padding : Union [ int , str ] = eqx . field ( static = True ) stride : int = eqx . field ( static = True ) W : Array b : Array g : Array def __init__ ( self , input_shape : Tuple [ int ], # in_channels filter_shape : Tuple [ int ], out_size : int , * , key : PRNGKeyArray , padding : Union [ int , str ] = 'SAME' , stride : int = 1 , ** kwargs ): super () . __init__ ( ** kwargs ) H , W , C = input_shape self . input_shape = input_shape self . filter_shape = filter_shape self . out_size = out_size self . padding = padding self . stride = stride w_init = jax . nn . initializers . he_uniform ( in_axis =- 2 , out_axis =- 1 ) self . W = w_init ( key , shape = self . filter_shape + ( C , out_size )) self . g = jnp . array ( 1.0 ) self . b = jnp . zeros ( out_size ) def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' # Initialize g and b. W = self . W * jax . lax . rsqrt (( self . W ** 2 ) . sum ( axis = ( 0 , 1 , 2 )))[ None , None , None ,:] x = util . conv ( W , x , stride = self . stride , padding = self . padding ) std = jnp . std ( x . reshape (( - 1 , x . shape [ - 1 ])), axis = 0 ) + 1e-5 g = 1 / std x *= g mean = jnp . mean ( x . reshape (( - 1 , x . shape [ - 1 ])), axis = 0 ) b = - mean # Turn the new parameters into a new module get_g = lambda tree : tree . g get_b = lambda tree : tree . b updated_layer = eqx . tree_at ( get_g , self , g ) updated_layer = eqx . tree_at ( get_b , updated_layer , b ) return updated_layer def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' W = self . W * jax . lax . rsqrt (( self . W ** 2 ) . sum ( axis = ( 0 , 1 , 2 )))[ None , None , None ,:] x = self . g * util . conv ( W , x , stride = self . stride , padding = self . padding ) + self . b return x __init__ ( self , input_shape : Tuple [ int ], filter_shape : Tuple [ int ], out_size : int , * , key : PRNGKeyArray , padding : Union [ int , str ] = 'SAME' , stride : int = 1 , ** kwargs ) \u00a4 Source code in generax/nn/layers.py def __init__ ( self , input_shape : Tuple [ int ], # in_channels filter_shape : Tuple [ int ], out_size : int , * , key : PRNGKeyArray , padding : Union [ int , str ] = 'SAME' , stride : int = 1 , ** kwargs ): super () . __init__ ( ** kwargs ) H , W , C = input_shape self . input_shape = input_shape self . filter_shape = filter_shape self . out_size = out_size self . padding = padding self . stride = stride w_init = jax . nn . initializers . he_uniform ( in_axis =- 2 , out_axis =- 1 ) self . W = w_init ( key , shape = self . filter_shape + ( C , out_size )) self . g = jnp . array ( 1.0 ) self . b = jnp . zeros ( out_size ) data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> Module \u00a4 Initialize the parameters of the layer based on the data. Arguments : x : The data to initialize the parameters with. key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/nn/layers.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' # Initialize g and b. W = self . W * jax . lax . rsqrt (( self . W ** 2 ) . sum ( axis = ( 0 , 1 , 2 )))[ None , None , None ,:] x = util . conv ( W , x , stride = self . stride , padding = self . padding ) std = jnp . std ( x . reshape (( - 1 , x . shape [ - 1 ])), axis = 0 ) + 1e-5 g = 1 / std x *= g mean = jnp . mean ( x . reshape (( - 1 , x . shape [ - 1 ])), axis = 0 ) b = - mean # Turn the new parameters into a new module get_g = lambda tree : tree . g get_b = lambda tree : tree . b updated_layer = eqx . tree_at ( get_g , self , g ) updated_layer = eqx . tree_at ( get_b , updated_layer , b ) return updated_layer __call__ ( self , x : Array , y : Array = None ) -> Array \u00a4 Call self as a function. Source code in generax/nn/layers.py def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' W = self . W * jax . lax . rsqrt (( self . W ** 2 ) . sum ( axis = ( 0 , 1 , 2 )))[ None , None , None ,:] x = self . g * util . conv ( W , x , stride = self . stride , padding = self . padding ) + self . b return x generax.nn.layers.WeightStandardizedConv \u00a4 Weight standardized parametrized convolutional layer https://arxiv.org/pdf/1903.10520.pdf Source code in generax/nn/layers.py class WeightStandardizedConv ( eqx . Module ): \"\"\"Weight standardized parametrized convolutional layer https://arxiv.org/pdf/1903.10520.pdf \"\"\" input_shape : int = eqx . field ( static = True ) out_size : int = eqx . field ( static = True ) filter_shape : Tuple [ int ] = eqx . field ( static = True ) padding : Union [ int , str ] = eqx . field ( static = True ) stride : int = eqx . field ( static = True ) W : Array b : Array def __init__ ( self , input_shape : Tuple [ int ], # in_channels filter_shape : Tuple [ int ], out_size : int , * , key : PRNGKeyArray , padding : Union [ int , str ] = 'SAME' , stride : int = 1 , ** kwargs ): super () . __init__ ( ** kwargs ) H , W , C = input_shape self . input_shape = input_shape self . filter_shape = filter_shape self . out_size = out_size self . padding = padding self . stride = stride w_init = jax . nn . initializers . he_uniform ( in_axis =- 2 , out_axis =- 1 ) self . W = w_init ( key , shape = self . filter_shape + ( C , out_size )) self . b = jnp . zeros ( out_size ) def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" axes = ( 0 , 1 , 2 ) mean = jnp . mean ( self . W , axis = axes , keepdims = True ) var = jnp . var ( self . W , axis = axes , keepdims = True ) W_hat = ( self . W - mean ) / jnp . sqrt ( var + 1e-5 ) x = util . conv ( W_hat , x , stride = self . stride , padding = self . padding ) # Initialize b. mean = jnp . mean ( x . reshape (( - 1 , x . shape [ - 1 ])), axis = 0 ) b = - mean # Turn the new parameters into a new module get_b = lambda tree : tree . b updated_layer = eqx . tree_at ( get_b , self , b ) return updated_layer def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' axes = ( 0 , 1 , 2 ) mean = jnp . mean ( self . W , axis = axes , keepdims = True ) var = jnp . var ( self . W , axis = axes , keepdims = True ) H , W , C_in , C_out = self . W . shape fan_in = H * W * C_in W_hat = ( self . W - mean ) * jax . lax . rsqrt ( fan_in * var + 1e-5 ) x = util . conv ( W_hat , x , stride = self . stride , padding = self . padding ) + self . b return x __init__ ( self , input_shape : Tuple [ int ], filter_shape : Tuple [ int ], out_size : int , * , key : PRNGKeyArray , padding : Union [ int , str ] = 'SAME' , stride : int = 1 , ** kwargs ) \u00a4 Source code in generax/nn/layers.py def __init__ ( self , input_shape : Tuple [ int ], # in_channels filter_shape : Tuple [ int ], out_size : int , * , key : PRNGKeyArray , padding : Union [ int , str ] = 'SAME' , stride : int = 1 , ** kwargs ): super () . __init__ ( ** kwargs ) H , W , C = input_shape self . input_shape = input_shape self . filter_shape = filter_shape self . out_size = out_size self . padding = padding self . stride = stride w_init = jax . nn . initializers . he_uniform ( in_axis =- 2 , out_axis =- 1 ) self . W = w_init ( key , shape = self . filter_shape + ( C , out_size )) self . b = jnp . zeros ( out_size ) data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> Module \u00a4 Initialize the parameters of the layer based on the data. Arguments : x : The data to initialize the parameters with. key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/nn/layers.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" axes = ( 0 , 1 , 2 ) mean = jnp . mean ( self . W , axis = axes , keepdims = True ) var = jnp . var ( self . W , axis = axes , keepdims = True ) W_hat = ( self . W - mean ) / jnp . sqrt ( var + 1e-5 ) x = util . conv ( W_hat , x , stride = self . stride , padding = self . padding ) # Initialize b. mean = jnp . mean ( x . reshape (( - 1 , x . shape [ - 1 ])), axis = 0 ) b = - mean # Turn the new parameters into a new module get_b = lambda tree : tree . b updated_layer = eqx . tree_at ( get_b , self , b ) return updated_layer __call__ ( self , x : Array , y : Array = None ) -> Array \u00a4 Call self as a function. Source code in generax/nn/layers.py def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' axes = ( 0 , 1 , 2 ) mean = jnp . mean ( self . W , axis = axes , keepdims = True ) var = jnp . var ( self . W , axis = axes , keepdims = True ) H , W , C_in , C_out = self . W . shape fan_in = H * W * C_in W_hat = ( self . W - mean ) * jax . lax . rsqrt ( fan_in * var + 1e-5 ) x = util . conv ( W_hat , x , stride = self . stride , padding = self . padding ) + self . b return x generax.nn.layers.ChannelConvention \u00a4 ChannelConvention( args, *kwargs) Source code in generax/nn/layers.py class ChannelConvention ( eqx . Module ): module : eqx . Module def __init__ ( self , module : eqx . Module ): super () . __init__ () self . module = module def __call__ ( self , x ): x = einops . rearrange ( x , 'H W C -> C H W' ) x = self . module ( x ) x = einops . rearrange ( x , 'C H W -> H W C' ) return x __init__ ( self , module : Module ) \u00a4 Source code in generax/nn/layers.py def __init__ ( self , module : eqx . Module ): super () . __init__ () self . module = module __call__ ( self , x ) \u00a4 Call self as a function. Source code in generax/nn/layers.py def __call__ ( self , x ): x = einops . rearrange ( x , 'H W C -> C H W' ) x = self . module ( x ) x = einops . rearrange ( x , 'C H W -> H W C' ) return x generax.nn.layers.ConvAndGroupNorm \u00a4 Weight standardized conv + group norm Source code in generax/nn/layers.py class ConvAndGroupNorm ( eqx . Module ): \"\"\"Weight standardized conv + group norm \"\"\" input_shape : int = eqx . field ( static = True ) conv : WeightStandardizedConv norm : eqx . nn . GroupNorm def __init__ ( self , input_shape : Tuple [ int ], # in_channels filter_shape : Tuple [ int ], out_size : int , groups : int , * , key : PRNGKeyArray , padding : Union [ int , str ] = 'SAME' , stride : int = 1 , ** kwargs ): super () . __init__ ( ** kwargs ) if out_size % groups != 0 : raise ValueError ( \"The number of groups must divide the number of channels.\" ) self . conv = WeightStandardizedConv ( input_shape = input_shape , filter_shape = filter_shape , out_size = out_size , key = key , padding = padding , stride = stride ) self . norm = ChannelConvention ( eqx . nn . GroupNorm ( groups = groups , channels = out_size )) self . input_shape = self . conv . input_shape def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , shift_scale : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" new_conv = self . conv . data_dependent_init ( x , y , key = key ) get_conv = lambda tree : tree . conv updated_layer = eqx . tree_at ( get_conv , self , new_conv ) return updated_layer def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' x = self . conv ( x ) x = self . norm ( x ) return x __init__ ( self , input_shape : Tuple [ int ], filter_shape : Tuple [ int ], out_size : int , groups : int , * , key : PRNGKeyArray , padding : Union [ int , str ] = 'SAME' , stride : int = 1 , ** kwargs ) \u00a4 Source code in generax/nn/layers.py def __init__ ( self , input_shape : Tuple [ int ], # in_channels filter_shape : Tuple [ int ], out_size : int , groups : int , * , key : PRNGKeyArray , padding : Union [ int , str ] = 'SAME' , stride : int = 1 , ** kwargs ): super () . __init__ ( ** kwargs ) if out_size % groups != 0 : raise ValueError ( \"The number of groups must divide the number of channels.\" ) self . conv = WeightStandardizedConv ( input_shape = input_shape , filter_shape = filter_shape , out_size = out_size , key = key , padding = padding , stride = stride ) self . norm = ChannelConvention ( eqx . nn . GroupNorm ( groups = groups , channels = out_size )) self . input_shape = self . conv . input_shape data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , shift_scale : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> Module \u00a4 Initialize the parameters of the layer based on the data. Arguments : x : The data to initialize the parameters with. key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/nn/layers.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , shift_scale : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" new_conv = self . conv . data_dependent_init ( x , y , key = key ) get_conv = lambda tree : tree . conv updated_layer = eqx . tree_at ( get_conv , self , new_conv ) return updated_layer __call__ ( self , x : Array , y : Array = None ) -> Array \u00a4 Call self as a function. Source code in generax/nn/layers.py def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' x = self . conv ( x ) x = self . norm ( x ) return x generax.nn.layers.Upsample \u00a4 https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf Source code in generax/nn/layers.py class Upsample ( eqx . Module ): \"\"\"https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf \"\"\" input_shape : int = eqx . field ( static = True ) out_size : int = eqx . field ( static = True ) conv : WeightStandardizedConv def __init__ ( self , input_shape : Tuple [ int ], out_size : Optional [ int ] = None , * , key : PRNGKeyArray , ** kwargs ): super () . __init__ ( ** kwargs ) H , W , C = input_shape self . input_shape = input_shape self . out_size = out_size if out_size is not None else C self . conv = WeightStandardizedConv ( input_shape = ( H , W , C ), filter_shape = ( 3 , 3 ), out_size = 4 * self . out_size , key = key ) def data_dependent_init ( self , * args , ** kwargs ) -> eqx . Module : return self def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' H , W , C = x . shape x = self . conv ( x ) x = jax . nn . silu ( x ) x = einops . rearrange ( x , 'h w (c k1 k2) -> (h k1) (w k2) c' , k1 = 2 , k2 = 2 ) assert x . shape == ( H * 2 , W * 2 , self . out_size ) return x __init__ ( self , input_shape : Tuple [ int ], out_size : Optional [ int ] = None , * , key : PRNGKeyArray , ** kwargs ) \u00a4 Source code in generax/nn/layers.py def __init__ ( self , input_shape : Tuple [ int ], out_size : Optional [ int ] = None , * , key : PRNGKeyArray , ** kwargs ): super () . __init__ ( ** kwargs ) H , W , C = input_shape self . input_shape = input_shape self . out_size = out_size if out_size is not None else C self . conv = WeightStandardizedConv ( input_shape = ( H , W , C ), filter_shape = ( 3 , 3 ), out_size = 4 * self . out_size , key = key ) data_dependent_init ( self , * args , ** kwargs ) -> Module \u00a4 Source code in generax/nn/layers.py def data_dependent_init ( self , * args , ** kwargs ) -> eqx . Module : return self __call__ ( self , x : Array , y : Array = None ) -> Array \u00a4 Call self as a function. Source code in generax/nn/layers.py def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' H , W , C = x . shape x = self . conv ( x ) x = jax . nn . silu ( x ) x = einops . rearrange ( x , 'h w (c k1 k2) -> (h k1) (w k2) c' , k1 = 2 , k2 = 2 ) assert x . shape == ( H * 2 , W * 2 , self . out_size ) return x generax.nn.layers.Downsample \u00a4 Downsample( args, *kwargs) Source code in generax/nn/layers.py class Downsample ( eqx . Module ): input_shape : int = eqx . field ( static = True ) out_size : int = eqx . field ( static = True ) conv : WeightStandardizedConv def __init__ ( self , input_shape : Tuple [ int ], out_size : Optional [ int ] = None , * , key : PRNGKeyArray , ** kwargs ): super () . __init__ ( ** kwargs ) H , W , C = input_shape self . input_shape = input_shape self . out_size = out_size if out_size is not None else C self . conv = WeightStandardizedConv ( input_shape = ( H // 2 , W // 2 , C * 4 ), filter_shape = ( 3 , 3 ), out_size = self . out_size , key = key ) def data_dependent_init ( self , * args , ** kwargs ) -> eqx . Module : return self def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' H , W , C = x . shape x = einops . rearrange ( x , '(h k1) (w k2) c -> h w (c k1 k2)' , k1 = 2 , k2 = 2 ) x = self . conv ( x ) assert x . shape == ( H // 2 , W // 2 , self . out_size ) return x __init__ ( self , input_shape : Tuple [ int ], out_size : Optional [ int ] = None , * , key : PRNGKeyArray , ** kwargs ) \u00a4 Source code in generax/nn/layers.py def __init__ ( self , input_shape : Tuple [ int ], out_size : Optional [ int ] = None , * , key : PRNGKeyArray , ** kwargs ): super () . __init__ ( ** kwargs ) H , W , C = input_shape self . input_shape = input_shape self . out_size = out_size if out_size is not None else C self . conv = WeightStandardizedConv ( input_shape = ( H // 2 , W // 2 , C * 4 ), filter_shape = ( 3 , 3 ), out_size = self . out_size , key = key ) data_dependent_init ( self , * args , ** kwargs ) -> Module \u00a4 Source code in generax/nn/layers.py def data_dependent_init ( self , * args , ** kwargs ) -> eqx . Module : return self __call__ ( self , x : Array , y : Array = None ) -> Array \u00a4 Call self as a function. Source code in generax/nn/layers.py def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' H , W , C = x . shape x = einops . rearrange ( x , '(h k1) (w k2) c -> h w (c k1 k2)' , k1 = 2 , k2 = 2 ) x = self . conv ( x ) assert x . shape == ( H // 2 , W // 2 , self . out_size ) return x generax.nn.layers.GatedGlobalContext \u00a4 Modified version of https://arxiv.org/pdf/1904.11492.pdf used in imagen https://github.com/lucidrains/imagen-pytorch/ Source code in generax/nn/layers.py class GatedGlobalContext ( eqx . Module ): \"\"\"Modified version of https://arxiv.org/pdf/1904.11492.pdf used in imagen https://github.com/lucidrains/imagen-pytorch/\"\"\" input_shape : int = eqx . field ( static = True ) linear1 : WeightNormConv linear2 : WeightNormConv context_conv : WeightNormConv def __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray , ** kwargs ): super () . __init__ ( ** kwargs ) H , W , C = input_shape self . input_shape = input_shape out_size = C hidden_dim = max ( 3 , out_size // 2 ) k1 , k2 , k3 = random . split ( key , 3 ) self . linear1 = WeightNormDense ( in_size = C , out_size = hidden_dim , key = k1 ) self . linear2 = WeightNormDense ( in_size = hidden_dim , out_size = out_size , key = k2 ) self . context_conv = WeightNormConv ( input_shape = input_shape , filter_shape = ( 1 , 1 ), out_size = 1 , key = k3 ) def data_dependent_init ( self , * args , ** kwargs ) -> eqx . Module : return self def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' x_in = x H , W , C = x . shape # Reduce channels to (H, W, 1) context = self . context_conv ( x ) # Flatten c_flat = einops . rearrange ( context , 'h w c -> (h w) c' ) x_flat = einops . rearrange ( x , 'h w c -> (h w) c' ) # Context over the pixels c_sm = jax . nn . softmax ( c_flat , axis = 0 ) # Reweight the channels out = jnp . einsum ( 'tu,tv->uv' , c_sm , x_flat ) assert out . shape == ( 1 , C ) out = out [ 0 ] out = self . linear1 ( out ) out = jax . nn . silu ( out ) out = self . linear2 ( out ) out = jax . nn . sigmoid ( out ) return x_in * out [ None , None ,:] __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray , ** kwargs ) \u00a4 Source code in generax/nn/layers.py def __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray , ** kwargs ): super () . __init__ ( ** kwargs ) H , W , C = input_shape self . input_shape = input_shape out_size = C hidden_dim = max ( 3 , out_size // 2 ) k1 , k2 , k3 = random . split ( key , 3 ) self . linear1 = WeightNormDense ( in_size = C , out_size = hidden_dim , key = k1 ) self . linear2 = WeightNormDense ( in_size = hidden_dim , out_size = out_size , key = k2 ) self . context_conv = WeightNormConv ( input_shape = input_shape , filter_shape = ( 1 , 1 ), out_size = 1 , key = k3 ) data_dependent_init ( self , * args , ** kwargs ) -> Module \u00a4 Source code in generax/nn/layers.py def data_dependent_init ( self , * args , ** kwargs ) -> eqx . Module : return self __call__ ( self , x : Array , y : Array = None ) -> Array \u00a4 Call self as a function. Source code in generax/nn/layers.py def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' x_in = x H , W , C = x . shape # Reduce channels to (H, W, 1) context = self . context_conv ( x ) # Flatten c_flat = einops . rearrange ( context , 'h w c -> (h w) c' ) x_flat = einops . rearrange ( x , 'h w c -> (h w) c' ) # Context over the pixels c_sm = jax . nn . softmax ( c_flat , axis = 0 ) # Reweight the channels out = jnp . einsum ( 'tu,tv->uv' , c_sm , x_flat ) assert out . shape == ( 1 , C ) out = out [ 0 ] out = self . linear1 ( out ) out = jax . nn . silu ( out ) out = self . linear2 ( out ) out = jax . nn . sigmoid ( out ) return x_in * out [ None , None ,:] generax.nn.layers.Attention \u00a4 Attention( args, *kwargs) Source code in generax/nn/layers.py class Attention ( eqx . Module ): input_shape : int = eqx . field ( static = True ) heads : int = eqx . field ( static = True ) dim_head : int = eqx . field ( static = True ) scale : float = eqx . field ( static = True ) conv_in : eqx . nn . Conv3d conv_out : eqx . nn . Conv3d def __init__ ( self , input_shape : Tuple [ int ], heads : int = 4 , dim_head : int = 32 , scale : float = 10 , * , key : PRNGKeyArray , ** kwargs ): super () . __init__ ( ** kwargs ) H , W , C = input_shape self . input_shape = input_shape self . heads = heads self . dim_head = dim_head self . scale = scale k1 , k2 = random . split ( key , 2 ) dim = self . dim_head * self . heads self . conv_in = ChannelConvention ( eqx . nn . Conv2d ( in_channels = C , out_channels = 3 * dim , kernel_size = 1 , use_bias = False , key = k1 )) self . conv_out = ChannelConvention ( eqx . nn . Conv2d ( in_channels = dim , out_channels = C , kernel_size = 1 , use_bias = True , key = k2 )) def data_dependent_init ( self , * args , ** kwargs ) -> eqx . Module : return self def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' H , W , C = x . shape qkv = self . conv_in ( x ) # (H, W, heads*dim_head*3) qkv = einops . rearrange ( qkv , 'H W (u h d) -> (H W) h d u' , h = self . heads , d = self . dim_head , u = 3 ) q , k , v = jnp . split ( qkv , 3 , axis =- 1 ) q , k , v = q [ ... , 0 ], k [ ... , 0 ], v [ ... , 0 ] assert q . shape == k . shape == v . shape == ( H * W , self . heads , self . dim_head ) def normalize ( x ): return x / jnp . clip ( jnp . linalg . norm ( x , axis = 0 , keepdims = True ), 1e-8 ) q , k = normalize ( q ), normalize ( k ) sim = jnp . einsum ( 'ihd,jhd->hij' , q , k ) * self . scale attn = jax . nn . softmax ( sim , axis =- 1 ) assert attn . shape == ( self . heads , H * W , H * W ) out = jnp . einsum ( 'hij,jhd->hid' , attn , v ) out = einops . rearrange ( out , 'h (H W) d -> H W (h d)' , H = H , W = W , h = self . heads , d = self . dim_head ) assert out . shape == ( H , W , self . dim_head * self . heads ) out = self . conv_out ( out ) return out __init__ ( self , input_shape : Tuple [ int ], heads : int = 4 , dim_head : int = 32 , scale : float = 10 , * , key : PRNGKeyArray , ** kwargs ) \u00a4 Source code in generax/nn/layers.py def __init__ ( self , input_shape : Tuple [ int ], heads : int = 4 , dim_head : int = 32 , scale : float = 10 , * , key : PRNGKeyArray , ** kwargs ): super () . __init__ ( ** kwargs ) H , W , C = input_shape self . input_shape = input_shape self . heads = heads self . dim_head = dim_head self . scale = scale k1 , k2 = random . split ( key , 2 ) dim = self . dim_head * self . heads self . conv_in = ChannelConvention ( eqx . nn . Conv2d ( in_channels = C , out_channels = 3 * dim , kernel_size = 1 , use_bias = False , key = k1 )) self . conv_out = ChannelConvention ( eqx . nn . Conv2d ( in_channels = dim , out_channels = C , kernel_size = 1 , use_bias = True , key = k2 )) data_dependent_init ( self , * args , ** kwargs ) -> Module \u00a4 Source code in generax/nn/layers.py def data_dependent_init ( self , * args , ** kwargs ) -> eqx . Module : return self __call__ ( self , x : Array , y : Array = None ) -> Array \u00a4 Call self as a function. Source code in generax/nn/layers.py def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' H , W , C = x . shape qkv = self . conv_in ( x ) # (H, W, heads*dim_head*3) qkv = einops . rearrange ( qkv , 'H W (u h d) -> (H W) h d u' , h = self . heads , d = self . dim_head , u = 3 ) q , k , v = jnp . split ( qkv , 3 , axis =- 1 ) q , k , v = q [ ... , 0 ], k [ ... , 0 ], v [ ... , 0 ] assert q . shape == k . shape == v . shape == ( H * W , self . heads , self . dim_head ) def normalize ( x ): return x / jnp . clip ( jnp . linalg . norm ( x , axis = 0 , keepdims = True ), 1e-8 ) q , k = normalize ( q ), normalize ( k ) sim = jnp . einsum ( 'ihd,jhd->hij' , q , k ) * self . scale attn = jax . nn . softmax ( sim , axis =- 1 ) assert attn . shape == ( self . heads , H * W , H * W ) out = jnp . einsum ( 'hij,jhd->hid' , attn , v ) out = einops . rearrange ( out , 'h (H W) d -> H W (h d)' , H = H , W = W , h = self . heads , d = self . dim_head ) assert out . shape == ( H , W , self . dim_head * self . heads ) out = self . conv_out ( out ) return out generax.nn.layers.LinearAttention \u00a4 LinearAttention( args, *kwargs) Source code in generax/nn/layers.py class LinearAttention ( eqx . Module ): input_shape : int = eqx . field ( static = True ) heads : int = eqx . field ( static = True ) dim_head : int = eqx . field ( static = True ) conv_in : eqx . nn . Conv3d conv_out : eqx . nn . Conv3d norm : eqx . nn . LayerNorm def __init__ ( self , input_shape : Tuple [ int ], heads : int = 4 , dim_head : int = 32 , * , key : PRNGKeyArray , ** kwargs ): super () . __init__ ( ** kwargs ) H , W , C = input_shape self . input_shape = input_shape self . heads = heads self . dim_head = dim_head k1 , k2 = random . split ( key , 2 ) dim = self . dim_head * self . heads self . conv_in = ChannelConvention ( eqx . nn . Conv2d ( in_channels = C , out_channels = 3 * dim , kernel_size = 1 , use_bias = False , key = k1 )) self . conv_out = ChannelConvention ( eqx . nn . Conv2d ( in_channels = dim , out_channels = C , kernel_size = 1 , use_bias = True , key = k2 )) self . norm = eqx . nn . LayerNorm ( shape = ( C ,), use_bias = False ) def data_dependent_init ( self , * args , ** kwargs ) -> eqx . Module : return self def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' H , W , C = x . shape qkv = self . conv_in ( x ) # (H, W, heads*dim_head*3) qkv = einops . rearrange ( qkv , 'H W (u h d) -> (H W) h d u' , h = self . heads , d = self . dim_head , u = 3 ) q , k , v = jnp . split ( qkv , 3 , axis =- 1 ) q , k , v = q [ ... , 0 ], k [ ... , 0 ], v [ ... , 0 ] assert q . shape == k . shape == v . shape == ( H * W , self . heads , self . dim_head ) q = jax . nn . softmax ( q , axis =- 1 ) k = jax . nn . softmax ( k , axis =- 3 ) q = q / jnp . sqrt ( self . dim_head ) v = v / ( H * W ) context = jnp . einsum ( \"n h d, n h e -> h d e\" , k , v ) out = jnp . einsum ( \"h d e, n h d -> h e n\" , context , q ) out = einops . rearrange ( out , \"h e (x y) -> x y (h e)\" , x = H ) assert out . shape == ( H , W , self . dim_head * self . heads ) out = self . conv_out ( out ) out = eqx . filter_vmap ( eqx . filter_vmap ( self . norm ))( out ) return out __init__ ( self , input_shape : Tuple [ int ], heads : int = 4 , dim_head : int = 32 , * , key : PRNGKeyArray , ** kwargs ) \u00a4 Source code in generax/nn/layers.py def __init__ ( self , input_shape : Tuple [ int ], heads : int = 4 , dim_head : int = 32 , * , key : PRNGKeyArray , ** kwargs ): super () . __init__ ( ** kwargs ) H , W , C = input_shape self . input_shape = input_shape self . heads = heads self . dim_head = dim_head k1 , k2 = random . split ( key , 2 ) dim = self . dim_head * self . heads self . conv_in = ChannelConvention ( eqx . nn . Conv2d ( in_channels = C , out_channels = 3 * dim , kernel_size = 1 , use_bias = False , key = k1 )) self . conv_out = ChannelConvention ( eqx . nn . Conv2d ( in_channels = dim , out_channels = C , kernel_size = 1 , use_bias = True , key = k2 )) self . norm = eqx . nn . LayerNorm ( shape = ( C ,), use_bias = False ) data_dependent_init ( self , * args , ** kwargs ) -> Module \u00a4 Source code in generax/nn/layers.py def data_dependent_init ( self , * args , ** kwargs ) -> eqx . Module : return self __call__ ( self , x : Array , y : Array = None ) -> Array \u00a4 Call self as a function. Source code in generax/nn/layers.py def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' H , W , C = x . shape qkv = self . conv_in ( x ) # (H, W, heads*dim_head*3) qkv = einops . rearrange ( qkv , 'H W (u h d) -> (H W) h d u' , h = self . heads , d = self . dim_head , u = 3 ) q , k , v = jnp . split ( qkv , 3 , axis =- 1 ) q , k , v = q [ ... , 0 ], k [ ... , 0 ], v [ ... , 0 ] assert q . shape == k . shape == v . shape == ( H * W , self . heads , self . dim_head ) q = jax . nn . softmax ( q , axis =- 1 ) k = jax . nn . softmax ( k , axis =- 3 ) q = q / jnp . sqrt ( self . dim_head ) v = v / ( H * W ) context = jnp . einsum ( \"n h d, n h e -> h d e\" , k , v ) out = jnp . einsum ( \"h d e, n h d -> h e n\" , context , q ) out = einops . rearrange ( out , \"h e (x y) -> x y (h e)\" , x = H ) assert out . shape == ( H , W , self . dim_head * self . heads ) out = self . conv_out ( out ) out = eqx . filter_vmap ( eqx . filter_vmap ( self . norm ))( out ) return out generax.nn.layers.AttentionBlock \u00a4 AttentionBlock( args, *kwargs) Source code in generax/nn/layers.py class AttentionBlock ( eqx . Module ): input_shape : int = eqx . field ( static = True ) attn : Union [ Attention , LinearAttention ] norm : eqx . nn . LayerNorm def __init__ ( self , input_shape : Tuple [ int ], heads : int = 4 , dim_head : int = 32 , * , key : PRNGKeyArray , use_linear_attention : bool = True , ** kwargs ): super () . __init__ ( ** kwargs ) if use_linear_attention : self . attn = LinearAttention ( input_shape = input_shape , heads = heads , dim_head = dim_head , key = key ) else : self . attn = Attention ( input_shape = input_shape , heads = heads , dim_head = dim_head , key = key ) self . input_shape = self . attn . input_shape H , W , C = input_shape self . norm = eqx . nn . LayerNorm ( shape = ( C ,), use_bias = False ) def data_dependent_init ( self , * args , ** kwargs ) -> eqx . Module : return self def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' normed_x = eqx . filter_vmap ( eqx . filter_vmap ( self . norm ))( x ) out = self . attn ( normed_x ) return out + x __init__ ( self , input_shape : Tuple [ int ], heads : int = 4 , dim_head : int = 32 , * , key : PRNGKeyArray , use_linear_attention : bool = True , ** kwargs ) \u00a4 Source code in generax/nn/layers.py def __init__ ( self , input_shape : Tuple [ int ], heads : int = 4 , dim_head : int = 32 , * , key : PRNGKeyArray , use_linear_attention : bool = True , ** kwargs ): super () . __init__ ( ** kwargs ) if use_linear_attention : self . attn = LinearAttention ( input_shape = input_shape , heads = heads , dim_head = dim_head , key = key ) else : self . attn = Attention ( input_shape = input_shape , heads = heads , dim_head = dim_head , key = key ) self . input_shape = self . attn . input_shape H , W , C = input_shape self . norm = eqx . nn . LayerNorm ( shape = ( C ,), use_bias = False ) data_dependent_init ( self , * args , ** kwargs ) -> Module \u00a4 Source code in generax/nn/layers.py def data_dependent_init ( self , * args , ** kwargs ) -> eqx . Module : return self __call__ ( self , x : Array , y : Array = None ) -> Array \u00a4 Call self as a function. Source code in generax/nn/layers.py def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' normed_x = eqx . filter_vmap ( eqx . filter_vmap ( self . norm ))( x ) out = self . attn ( normed_x ) return out + x generax.nn.grad_wrapper.GradWrapper \u00a4 An easy wrapper around a function that computes the gradient of a scalar function. Source code in generax/nn/grad_wrapper.py class GradWrapper ( eqx . Module ): \"\"\"An easy wrapper around a function that computes the gradient of a scalar function.\"\"\" net : eqx . Module input_shape : Tuple [ int , ... ] def __init__ ( self , net : eqx . Module ): self . net = net self . input_shape = net . input_shape def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' out = self . net ( x , y = y , key = key ) assert out . shape == ( 1 ,) def __call__ ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"**Arguments:** - `t`: A JAX array with shape `()`. - `x`: A JAX array with shape `(input_shape,)`. - `y`: A JAX array with shape `(cond_shape,)`. **Returns:** A JAX array with shape `(input_shape,)`. \"\"\" assert x . shape == self . input_shape def net ( x ): net_out = self . net ( x , y = y , ** kwargs ) if net_out . shape != ( 1 ,): raise ValueError ( f 'Expected net to return a scalar, but got { net_out . shape } ' ) return net_out . ravel () return eqx . filter_grad ( net )( x ) @property def energy ( self ): return self . net energy property readonly \u00a4 __init__ ( self , net : Module ) \u00a4 Source code in generax/nn/grad_wrapper.py def __init__ ( self , net : eqx . Module ): self . net = net self . input_shape = net . input_shape data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> Module \u00a4 Initialize the parameters of the layer based on the data. Arguments : x : The data to initialize the parameters with. y : The conditioning information key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/nn/grad_wrapper.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' out = self . net ( x , y = y , key = key ) assert out . shape == ( 1 ,) __call__ ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Arguments: t : A JAX array with shape () . x : A JAX array with shape (input_shape,) . y : A JAX array with shape (cond_shape,) . Returns: A JAX array with shape (input_shape,) . Source code in generax/nn/grad_wrapper.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"**Arguments:** - `t`: A JAX array with shape `()`. - `x`: A JAX array with shape `(input_shape,)`. - `y`: A JAX array with shape `(cond_shape,)`. **Returns:** A JAX array with shape `(input_shape,)`. \"\"\" assert x . shape == self . input_shape def net ( x ): net_out = self . net ( x , y = y , ** kwargs ) if net_out . shape != ( 1 ,): raise ValueError ( f 'Expected net to return a scalar, but got { net_out . shape } ' ) return net_out . ravel () return eqx . filter_grad ( net )( x ) generax.nn.grad_wrapper.TimeDependentGradWrapper ( GradWrapper ) \u00a4 An easy wrapper around a function that computes the gradient of a scalar function. Source code in generax/nn/grad_wrapper.py class TimeDependentGradWrapper ( GradWrapper ): \"\"\"An easy wrapper around a function that computes the gradient of a scalar function.\"\"\" def data_dependent_init ( self , t : Array , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `t`: The time to initialize the parameters with. - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' out = self . net ( t , x , y = y , key = key ) assert out . shape == ( 1 ,) def __call__ ( self , t : Array , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"**Arguments:** - `t`: A JAX array with shape `()`. - `x`: A JAX array with shape `(input_shape,)`. - `y`: A JAX array with shape `(cond_shape,)`. **Returns:** A JAX array with shape `(input_shape,)`. \"\"\" assert x . shape == self . input_shape def net ( x ): net_out = self . net ( t , x , y = y , ** kwargs ) if net_out . shape != ( 1 ,): raise ValueError ( f 'Expected net to return a scalar, but got { net_out . shape } ' ) return net_out [ 0 ] return eqx . filter_grad ( net )( x ) @property def energy ( self ): return self . net energy property readonly \u00a4 __init__ ( self , net : Module ) \u00a4 Source code in generax/nn/grad_wrapper.py def __init__ ( self , net : eqx . Module ): self . net = net self . input_shape = net . input_shape data_dependent_init ( self , t : Array , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> Module \u00a4 Initialize the parameters of the layer based on the data. Arguments : t : The time to initialize the parameters with. x : The data to initialize the parameters with. y : The conditioning information key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/nn/grad_wrapper.py def data_dependent_init ( self , t : Array , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `t`: The time to initialize the parameters with. - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' out = self . net ( t , x , y = y , key = key ) assert out . shape == ( 1 ,) __call__ ( self , t : Array , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Arguments: t : A JAX array with shape () . x : A JAX array with shape (input_shape,) . y : A JAX array with shape (cond_shape,) . Returns: A JAX array with shape (input_shape,) . Source code in generax/nn/grad_wrapper.py def __call__ ( self , t : Array , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"**Arguments:** - `t`: A JAX array with shape `()`. - `x`: A JAX array with shape `(input_shape,)`. - `y`: A JAX array with shape `(cond_shape,)`. **Returns:** A JAX array with shape `(input_shape,)`. \"\"\" assert x . shape == self . input_shape def net ( x ): net_out = self . net ( t , x , y = y , ** kwargs ) if net_out . shape != ( 1 ,): raise ValueError ( f 'Expected net to return a scalar, but got { net_out . shape } ' ) return net_out [ 0 ] return eqx . filter_grad ( net )( x )","title":"Neural network layers"},{"location":"api/nn/layers/#neural-network-layers","text":"","title":"Neural network layers"},{"location":"api/nn/layers/#generax.nn.layers.WeightNormDense","text":"Weight normalization parametrized linear layer https://arxiv.org/pdf/1602.07868.pdf Source code in generax/nn/layers.py class WeightNormDense ( eqx . Module ): \"\"\"Weight normalization parametrized linear layer https://arxiv.org/pdf/1602.07868.pdf \"\"\" in_size : int = eqx . field ( static = True ) out_size : int = eqx . field ( static = True ) W : Array b : Array g : Array def __init__ ( self , in_size : int , out_size : int , key : PRNGKeyArray , ** kwargs ): super () . __init__ ( ** kwargs ) self . in_size = in_size self . out_size = out_size w_init = jax . nn . initializers . he_uniform ( in_axis =- 2 , out_axis =- 1 ) self . W = w_init ( key , shape = ( out_size , in_size )) self . g = jnp . array ( 1.0 ) self . b = jnp . zeros ( out_size ) def data_dependent_init ( self , x : Array , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ - 1 ] == self . in_size , 'Only works on batched data' # Initialize g and b. W = self . W * jax . lax . rsqrt (( self . W ** 2 ) . sum ( axis = 1 ))[:, None ] x = jnp . einsum ( 'ij,bj->bi' , W , x ) std = jnp . std ( x . reshape (( - 1 , x . shape [ - 1 ])), axis = 0 ) + 1e-5 g = 1 / std x *= g mean = jnp . mean ( x . reshape (( - 1 , x . shape [ - 1 ])), axis = 0 ) b = - mean # Turn the new parameters into a new module get_g = lambda tree : tree . g get_b = lambda tree : tree . b updated_layer = eqx . tree_at ( get_g , self , g ) updated_layer = eqx . tree_at ( get_b , updated_layer , b ) return updated_layer def __call__ ( self , x : Array , y : Array = None ) -> Array : W = self . W * jax . lax . rsqrt (( self . W ** 2 ) . sum ( axis = 1 ))[:, None ] x = self . g * ( W @x ) + self . b return x","title":"WeightNormDense"},{"location":"api/nn/layers/#generax.nn.layers.WeightNormConv","text":"Weight normalization parametrized convolutional layer https://arxiv.org/pdf/1602.07868.pdf Source code in generax/nn/layers.py class WeightNormConv ( eqx . Module ): \"\"\"Weight normalization parametrized convolutional layer https://arxiv.org/pdf/1602.07868.pdf \"\"\" input_shape : int = eqx . field ( static = True ) out_size : int = eqx . field ( static = True ) filter_shape : Tuple [ int ] = eqx . field ( static = True ) padding : Union [ int , str ] = eqx . field ( static = True ) stride : int = eqx . field ( static = True ) W : Array b : Array g : Array def __init__ ( self , input_shape : Tuple [ int ], # in_channels filter_shape : Tuple [ int ], out_size : int , * , key : PRNGKeyArray , padding : Union [ int , str ] = 'SAME' , stride : int = 1 , ** kwargs ): super () . __init__ ( ** kwargs ) H , W , C = input_shape self . input_shape = input_shape self . filter_shape = filter_shape self . out_size = out_size self . padding = padding self . stride = stride w_init = jax . nn . initializers . he_uniform ( in_axis =- 2 , out_axis =- 1 ) self . W = w_init ( key , shape = self . filter_shape + ( C , out_size )) self . g = jnp . array ( 1.0 ) self . b = jnp . zeros ( out_size ) def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' # Initialize g and b. W = self . W * jax . lax . rsqrt (( self . W ** 2 ) . sum ( axis = ( 0 , 1 , 2 )))[ None , None , None ,:] x = util . conv ( W , x , stride = self . stride , padding = self . padding ) std = jnp . std ( x . reshape (( - 1 , x . shape [ - 1 ])), axis = 0 ) + 1e-5 g = 1 / std x *= g mean = jnp . mean ( x . reshape (( - 1 , x . shape [ - 1 ])), axis = 0 ) b = - mean # Turn the new parameters into a new module get_g = lambda tree : tree . g get_b = lambda tree : tree . b updated_layer = eqx . tree_at ( get_g , self , g ) updated_layer = eqx . tree_at ( get_b , updated_layer , b ) return updated_layer def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' W = self . W * jax . lax . rsqrt (( self . W ** 2 ) . sum ( axis = ( 0 , 1 , 2 )))[ None , None , None ,:] x = self . g * util . conv ( W , x , stride = self . stride , padding = self . padding ) + self . b return x","title":"WeightNormConv"},{"location":"api/nn/layers/#generax.nn.layers.WeightStandardizedConv","text":"Weight standardized parametrized convolutional layer https://arxiv.org/pdf/1903.10520.pdf Source code in generax/nn/layers.py class WeightStandardizedConv ( eqx . Module ): \"\"\"Weight standardized parametrized convolutional layer https://arxiv.org/pdf/1903.10520.pdf \"\"\" input_shape : int = eqx . field ( static = True ) out_size : int = eqx . field ( static = True ) filter_shape : Tuple [ int ] = eqx . field ( static = True ) padding : Union [ int , str ] = eqx . field ( static = True ) stride : int = eqx . field ( static = True ) W : Array b : Array def __init__ ( self , input_shape : Tuple [ int ], # in_channels filter_shape : Tuple [ int ], out_size : int , * , key : PRNGKeyArray , padding : Union [ int , str ] = 'SAME' , stride : int = 1 , ** kwargs ): super () . __init__ ( ** kwargs ) H , W , C = input_shape self . input_shape = input_shape self . filter_shape = filter_shape self . out_size = out_size self . padding = padding self . stride = stride w_init = jax . nn . initializers . he_uniform ( in_axis =- 2 , out_axis =- 1 ) self . W = w_init ( key , shape = self . filter_shape + ( C , out_size )) self . b = jnp . zeros ( out_size ) def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" axes = ( 0 , 1 , 2 ) mean = jnp . mean ( self . W , axis = axes , keepdims = True ) var = jnp . var ( self . W , axis = axes , keepdims = True ) W_hat = ( self . W - mean ) / jnp . sqrt ( var + 1e-5 ) x = util . conv ( W_hat , x , stride = self . stride , padding = self . padding ) # Initialize b. mean = jnp . mean ( x . reshape (( - 1 , x . shape [ - 1 ])), axis = 0 ) b = - mean # Turn the new parameters into a new module get_b = lambda tree : tree . b updated_layer = eqx . tree_at ( get_b , self , b ) return updated_layer def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' axes = ( 0 , 1 , 2 ) mean = jnp . mean ( self . W , axis = axes , keepdims = True ) var = jnp . var ( self . W , axis = axes , keepdims = True ) H , W , C_in , C_out = self . W . shape fan_in = H * W * C_in W_hat = ( self . W - mean ) * jax . lax . rsqrt ( fan_in * var + 1e-5 ) x = util . conv ( W_hat , x , stride = self . stride , padding = self . padding ) + self . b return x","title":"WeightStandardizedConv"},{"location":"api/nn/layers/#generax.nn.layers.ChannelConvention","text":"ChannelConvention( args, *kwargs) Source code in generax/nn/layers.py class ChannelConvention ( eqx . Module ): module : eqx . Module def __init__ ( self , module : eqx . Module ): super () . __init__ () self . module = module def __call__ ( self , x ): x = einops . rearrange ( x , 'H W C -> C H W' ) x = self . module ( x ) x = einops . rearrange ( x , 'C H W -> H W C' ) return x","title":"ChannelConvention"},{"location":"api/nn/layers/#generax.nn.layers.ConvAndGroupNorm","text":"Weight standardized conv + group norm Source code in generax/nn/layers.py class ConvAndGroupNorm ( eqx . Module ): \"\"\"Weight standardized conv + group norm \"\"\" input_shape : int = eqx . field ( static = True ) conv : WeightStandardizedConv norm : eqx . nn . GroupNorm def __init__ ( self , input_shape : Tuple [ int ], # in_channels filter_shape : Tuple [ int ], out_size : int , groups : int , * , key : PRNGKeyArray , padding : Union [ int , str ] = 'SAME' , stride : int = 1 , ** kwargs ): super () . __init__ ( ** kwargs ) if out_size % groups != 0 : raise ValueError ( \"The number of groups must divide the number of channels.\" ) self . conv = WeightStandardizedConv ( input_shape = input_shape , filter_shape = filter_shape , out_size = out_size , key = key , padding = padding , stride = stride ) self . norm = ChannelConvention ( eqx . nn . GroupNorm ( groups = groups , channels = out_size )) self . input_shape = self . conv . input_shape def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , shift_scale : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" new_conv = self . conv . data_dependent_init ( x , y , key = key ) get_conv = lambda tree : tree . conv updated_layer = eqx . tree_at ( get_conv , self , new_conv ) return updated_layer def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' x = self . conv ( x ) x = self . norm ( x ) return x","title":"ConvAndGroupNorm"},{"location":"api/nn/layers/#generax.nn.layers.Upsample","text":"https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf Source code in generax/nn/layers.py class Upsample ( eqx . Module ): \"\"\"https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf \"\"\" input_shape : int = eqx . field ( static = True ) out_size : int = eqx . field ( static = True ) conv : WeightStandardizedConv def __init__ ( self , input_shape : Tuple [ int ], out_size : Optional [ int ] = None , * , key : PRNGKeyArray , ** kwargs ): super () . __init__ ( ** kwargs ) H , W , C = input_shape self . input_shape = input_shape self . out_size = out_size if out_size is not None else C self . conv = WeightStandardizedConv ( input_shape = ( H , W , C ), filter_shape = ( 3 , 3 ), out_size = 4 * self . out_size , key = key ) def data_dependent_init ( self , * args , ** kwargs ) -> eqx . Module : return self def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' H , W , C = x . shape x = self . conv ( x ) x = jax . nn . silu ( x ) x = einops . rearrange ( x , 'h w (c k1 k2) -> (h k1) (w k2) c' , k1 = 2 , k2 = 2 ) assert x . shape == ( H * 2 , W * 2 , self . out_size ) return x","title":"Upsample"},{"location":"api/nn/layers/#generax.nn.layers.Downsample","text":"Downsample( args, *kwargs) Source code in generax/nn/layers.py class Downsample ( eqx . Module ): input_shape : int = eqx . field ( static = True ) out_size : int = eqx . field ( static = True ) conv : WeightStandardizedConv def __init__ ( self , input_shape : Tuple [ int ], out_size : Optional [ int ] = None , * , key : PRNGKeyArray , ** kwargs ): super () . __init__ ( ** kwargs ) H , W , C = input_shape self . input_shape = input_shape self . out_size = out_size if out_size is not None else C self . conv = WeightStandardizedConv ( input_shape = ( H // 2 , W // 2 , C * 4 ), filter_shape = ( 3 , 3 ), out_size = self . out_size , key = key ) def data_dependent_init ( self , * args , ** kwargs ) -> eqx . Module : return self def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' H , W , C = x . shape x = einops . rearrange ( x , '(h k1) (w k2) c -> h w (c k1 k2)' , k1 = 2 , k2 = 2 ) x = self . conv ( x ) assert x . shape == ( H // 2 , W // 2 , self . out_size ) return x","title":"Downsample"},{"location":"api/nn/layers/#generax.nn.layers.GatedGlobalContext","text":"Modified version of https://arxiv.org/pdf/1904.11492.pdf used in imagen https://github.com/lucidrains/imagen-pytorch/ Source code in generax/nn/layers.py class GatedGlobalContext ( eqx . Module ): \"\"\"Modified version of https://arxiv.org/pdf/1904.11492.pdf used in imagen https://github.com/lucidrains/imagen-pytorch/\"\"\" input_shape : int = eqx . field ( static = True ) linear1 : WeightNormConv linear2 : WeightNormConv context_conv : WeightNormConv def __init__ ( self , input_shape : Tuple [ int ], * , key : PRNGKeyArray , ** kwargs ): super () . __init__ ( ** kwargs ) H , W , C = input_shape self . input_shape = input_shape out_size = C hidden_dim = max ( 3 , out_size // 2 ) k1 , k2 , k3 = random . split ( key , 3 ) self . linear1 = WeightNormDense ( in_size = C , out_size = hidden_dim , key = k1 ) self . linear2 = WeightNormDense ( in_size = hidden_dim , out_size = out_size , key = k2 ) self . context_conv = WeightNormConv ( input_shape = input_shape , filter_shape = ( 1 , 1 ), out_size = 1 , key = k3 ) def data_dependent_init ( self , * args , ** kwargs ) -> eqx . Module : return self def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' x_in = x H , W , C = x . shape # Reduce channels to (H, W, 1) context = self . context_conv ( x ) # Flatten c_flat = einops . rearrange ( context , 'h w c -> (h w) c' ) x_flat = einops . rearrange ( x , 'h w c -> (h w) c' ) # Context over the pixels c_sm = jax . nn . softmax ( c_flat , axis = 0 ) # Reweight the channels out = jnp . einsum ( 'tu,tv->uv' , c_sm , x_flat ) assert out . shape == ( 1 , C ) out = out [ 0 ] out = self . linear1 ( out ) out = jax . nn . silu ( out ) out = self . linear2 ( out ) out = jax . nn . sigmoid ( out ) return x_in * out [ None , None ,:]","title":"GatedGlobalContext"},{"location":"api/nn/layers/#generax.nn.layers.Attention","text":"Attention( args, *kwargs) Source code in generax/nn/layers.py class Attention ( eqx . Module ): input_shape : int = eqx . field ( static = True ) heads : int = eqx . field ( static = True ) dim_head : int = eqx . field ( static = True ) scale : float = eqx . field ( static = True ) conv_in : eqx . nn . Conv3d conv_out : eqx . nn . Conv3d def __init__ ( self , input_shape : Tuple [ int ], heads : int = 4 , dim_head : int = 32 , scale : float = 10 , * , key : PRNGKeyArray , ** kwargs ): super () . __init__ ( ** kwargs ) H , W , C = input_shape self . input_shape = input_shape self . heads = heads self . dim_head = dim_head self . scale = scale k1 , k2 = random . split ( key , 2 ) dim = self . dim_head * self . heads self . conv_in = ChannelConvention ( eqx . nn . Conv2d ( in_channels = C , out_channels = 3 * dim , kernel_size = 1 , use_bias = False , key = k1 )) self . conv_out = ChannelConvention ( eqx . nn . Conv2d ( in_channels = dim , out_channels = C , kernel_size = 1 , use_bias = True , key = k2 )) def data_dependent_init ( self , * args , ** kwargs ) -> eqx . Module : return self def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' H , W , C = x . shape qkv = self . conv_in ( x ) # (H, W, heads*dim_head*3) qkv = einops . rearrange ( qkv , 'H W (u h d) -> (H W) h d u' , h = self . heads , d = self . dim_head , u = 3 ) q , k , v = jnp . split ( qkv , 3 , axis =- 1 ) q , k , v = q [ ... , 0 ], k [ ... , 0 ], v [ ... , 0 ] assert q . shape == k . shape == v . shape == ( H * W , self . heads , self . dim_head ) def normalize ( x ): return x / jnp . clip ( jnp . linalg . norm ( x , axis = 0 , keepdims = True ), 1e-8 ) q , k = normalize ( q ), normalize ( k ) sim = jnp . einsum ( 'ihd,jhd->hij' , q , k ) * self . scale attn = jax . nn . softmax ( sim , axis =- 1 ) assert attn . shape == ( self . heads , H * W , H * W ) out = jnp . einsum ( 'hij,jhd->hid' , attn , v ) out = einops . rearrange ( out , 'h (H W) d -> H W (h d)' , H = H , W = W , h = self . heads , d = self . dim_head ) assert out . shape == ( H , W , self . dim_head * self . heads ) out = self . conv_out ( out ) return out","title":"Attention"},{"location":"api/nn/layers/#generax.nn.layers.LinearAttention","text":"LinearAttention( args, *kwargs) Source code in generax/nn/layers.py class LinearAttention ( eqx . Module ): input_shape : int = eqx . field ( static = True ) heads : int = eqx . field ( static = True ) dim_head : int = eqx . field ( static = True ) conv_in : eqx . nn . Conv3d conv_out : eqx . nn . Conv3d norm : eqx . nn . LayerNorm def __init__ ( self , input_shape : Tuple [ int ], heads : int = 4 , dim_head : int = 32 , * , key : PRNGKeyArray , ** kwargs ): super () . __init__ ( ** kwargs ) H , W , C = input_shape self . input_shape = input_shape self . heads = heads self . dim_head = dim_head k1 , k2 = random . split ( key , 2 ) dim = self . dim_head * self . heads self . conv_in = ChannelConvention ( eqx . nn . Conv2d ( in_channels = C , out_channels = 3 * dim , kernel_size = 1 , use_bias = False , key = k1 )) self . conv_out = ChannelConvention ( eqx . nn . Conv2d ( in_channels = dim , out_channels = C , kernel_size = 1 , use_bias = True , key = k2 )) self . norm = eqx . nn . LayerNorm ( shape = ( C ,), use_bias = False ) def data_dependent_init ( self , * args , ** kwargs ) -> eqx . Module : return self def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' H , W , C = x . shape qkv = self . conv_in ( x ) # (H, W, heads*dim_head*3) qkv = einops . rearrange ( qkv , 'H W (u h d) -> (H W) h d u' , h = self . heads , d = self . dim_head , u = 3 ) q , k , v = jnp . split ( qkv , 3 , axis =- 1 ) q , k , v = q [ ... , 0 ], k [ ... , 0 ], v [ ... , 0 ] assert q . shape == k . shape == v . shape == ( H * W , self . heads , self . dim_head ) q = jax . nn . softmax ( q , axis =- 1 ) k = jax . nn . softmax ( k , axis =- 3 ) q = q / jnp . sqrt ( self . dim_head ) v = v / ( H * W ) context = jnp . einsum ( \"n h d, n h e -> h d e\" , k , v ) out = jnp . einsum ( \"h d e, n h d -> h e n\" , context , q ) out = einops . rearrange ( out , \"h e (x y) -> x y (h e)\" , x = H ) assert out . shape == ( H , W , self . dim_head * self . heads ) out = self . conv_out ( out ) out = eqx . filter_vmap ( eqx . filter_vmap ( self . norm ))( out ) return out","title":"LinearAttention"},{"location":"api/nn/layers/#generax.nn.layers.AttentionBlock","text":"AttentionBlock( args, *kwargs) Source code in generax/nn/layers.py class AttentionBlock ( eqx . Module ): input_shape : int = eqx . field ( static = True ) attn : Union [ Attention , LinearAttention ] norm : eqx . nn . LayerNorm def __init__ ( self , input_shape : Tuple [ int ], heads : int = 4 , dim_head : int = 32 , * , key : PRNGKeyArray , use_linear_attention : bool = True , ** kwargs ): super () . __init__ ( ** kwargs ) if use_linear_attention : self . attn = LinearAttention ( input_shape = input_shape , heads = heads , dim_head = dim_head , key = key ) else : self . attn = Attention ( input_shape = input_shape , heads = heads , dim_head = dim_head , key = key ) self . input_shape = self . attn . input_shape H , W , C = input_shape self . norm = eqx . nn . LayerNorm ( shape = ( C ,), use_bias = False ) def data_dependent_init ( self , * args , ** kwargs ) -> eqx . Module : return self def __call__ ( self , x : Array , y : Array = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' normed_x = eqx . filter_vmap ( eqx . filter_vmap ( self . norm ))( x ) out = self . attn ( normed_x ) return out + x","title":"AttentionBlock"},{"location":"api/nn/layers/#generax.nn.grad_wrapper.GradWrapper","text":"An easy wrapper around a function that computes the gradient of a scalar function. Source code in generax/nn/grad_wrapper.py class GradWrapper ( eqx . Module ): \"\"\"An easy wrapper around a function that computes the gradient of a scalar function.\"\"\" net : eqx . Module input_shape : Tuple [ int , ... ] def __init__ ( self , net : eqx . Module ): self . net = net self . input_shape = net . input_shape def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' out = self . net ( x , y = y , key = key ) assert out . shape == ( 1 ,) def __call__ ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"**Arguments:** - `t`: A JAX array with shape `()`. - `x`: A JAX array with shape `(input_shape,)`. - `y`: A JAX array with shape `(cond_shape,)`. **Returns:** A JAX array with shape `(input_shape,)`. \"\"\" assert x . shape == self . input_shape def net ( x ): net_out = self . net ( x , y = y , ** kwargs ) if net_out . shape != ( 1 ,): raise ValueError ( f 'Expected net to return a scalar, but got { net_out . shape } ' ) return net_out . ravel () return eqx . filter_grad ( net )( x ) @property def energy ( self ): return self . net","title":"GradWrapper"},{"location":"api/nn/layers/#generax.nn.grad_wrapper.TimeDependentGradWrapper","text":"An easy wrapper around a function that computes the gradient of a scalar function. Source code in generax/nn/grad_wrapper.py class TimeDependentGradWrapper ( GradWrapper ): \"\"\"An easy wrapper around a function that computes the gradient of a scalar function.\"\"\" def data_dependent_init ( self , t : Array , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `t`: The time to initialize the parameters with. - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' out = self . net ( t , x , y = y , key = key ) assert out . shape == ( 1 ,) def __call__ ( self , t : Array , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"**Arguments:** - `t`: A JAX array with shape `()`. - `x`: A JAX array with shape `(input_shape,)`. - `y`: A JAX array with shape `(cond_shape,)`. **Returns:** A JAX array with shape `(input_shape,)`. \"\"\" assert x . shape == self . input_shape def net ( x ): net_out = self . net ( t , x , y = y , ** kwargs ) if net_out . shape != ( 1 ,): raise ValueError ( f 'Expected net to return a scalar, but got { net_out . shape } ' ) return net_out [ 0 ] return eqx . filter_grad ( net )( x ) @property def energy ( self ): return self . net","title":"TimeDependentGradWrapper"},{"location":"api/nn/resnet/","text":"Resnet \u00a4 generax.nn.resnet.ResNet \u00a4 ResNet for 1d data Source code in generax/nn/resnet.py class ResNet ( eqx . Module ): \"\"\"ResNet for 1d data\"\"\" n_blocks : int = eqx . field ( static = True ) blocks : tuple [ GatedResBlock , ... ] in_projection : eqx . nn . Linear out_projection : eqx . nn . Linear input_shape : int = eqx . field ( static = True ) cond_shape : int = eqx . field ( static = True ) working_size : int = eqx . field ( static = True ) hidden_size : int = eqx . field ( static = True ) out_size : int = eqx . field ( static = True ) filter_shape : Union [ Tuple [ int ], None ] = eqx . field ( static = True ) def __init__ ( self , input_shape : Tuple [ int ], working_size : int , hidden_size : int , out_size : int , n_blocks : int , filter_shape : Optional [ Tuple [ int ]] = ( 3 , 3 ), cond_shape : Optional [ Tuple [ int ]] = None , activation : Callable = jax . nn . swish , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input size. Output size is the same as input_shape. - `working_size`: The size (channels for images) of each hidden layer. - `hidden_size`: The size (channels for images) of each hidden layer. - `out_size`: The output size. For images, this is the number of output channels. - `n_blocks`: The number of residual blocks. - `cond_shape`: The size of the conditioning information. - `activation`: The activation function in each residual block. - `key`: A `jax.random.PRNGKey` for initialization. \"\"\" super () . __init__ ( ** kwargs ) if len ( input_shape ) not in [ 1 , 3 ]: raise ValueError ( f 'Expected 1d or 3d input shape' ) image = False if len ( input_shape ) == 3 : H , W , C = input_shape image = True assert filter_shape is not None , 'Must pass in filter shape when processing images' self . n_blocks = n_blocks self . working_size = working_size self . hidden_size = hidden_size self . filter_shape = filter_shape self . out_size = out_size k1 , k2 , k3 = random . split ( key , 3 ) if isinstance ( input_shape , int ): input_shape = ( input_shape ,) self . input_shape = input_shape self . cond_shape = cond_shape if image == False : self . in_projection = WeightNormDense ( in_size = input_shape [ 0 ], out_size = working_size , key = k1 ) working_shape = ( working_size ,) else : self . in_projection = ConvAndGroupNorm ( input_shape = input_shape , out_size = working_size , filter_shape = filter_shape , groups = 1 , key = k1 ) working_shape = ( H , W , working_size ) def make_resblock ( k ): return GatedResBlock ( input_shape = working_shape , hidden_size = hidden_size , cond_shape = cond_shape , activation = activation , filter_shape = filter_shape , key = k ) keys = random . split ( k2 , n_blocks ) self . blocks = eqx . filter_vmap ( make_resblock )( keys ) if image == False : self . out_projection = WeightNormDense ( in_size = working_size , out_size = out_size , key = k3 ) else : self . out_projection = ConvAndGroupNorm ( input_shape = working_shape , out_size = out_size , filter_shape = filter_shape , groups = 1 , key = k3 ) def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' k1 , k2 , k3 = random . split ( key , 3 ) # Input projection in_proj = self . in_projection . data_dependent_init ( x , key = k1 ) x = eqx . filter_vmap ( in_proj )( x ) # Scan over the vmapped blocks params , state = eqx . partition ( self . blocks , eqx . is_array ) def scan_body ( x , inputs ): key , block_params = inputs block = eqx . combine ( block_params , state ) new_block = block . data_dependent_init ( x , y , key = key ) new_x = eqx . filter_vmap ( new_block )( x , y ) new_params , _ = eqx . partition ( block , eqx . is_array ) return new_x , new_params keys = random . split ( k2 , self . n_blocks ) x , params = jax . lax . scan ( scan_body , x , ( keys , params )) blocks = eqx . combine ( params , state ) out_proj = self . out_projection . data_dependent_init ( x , key = k3 ) # Turn the new parameters into a new module get_in_proj = lambda tree : tree . in_projection get_blocks = lambda tree : tree . blocks get_out_proj = lambda tree : tree . out_projection updated_layer = eqx . tree_at ( get_in_proj , self , in_proj ) updated_layer = eqx . tree_at ( get_blocks , updated_layer , blocks ) updated_layer = eqx . tree_at ( get_out_proj , updated_layer , out_proj ) return updated_layer def __call__ ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"**Arguments:** - `t`: A JAX array with shape `()`. - `x`: A JAX array with shape `(input_shape,)`. - `y`: A JAX array with shape `(cond_shape,)`. **Returns:** A JAX array with shape `(input_shape,)`. \"\"\" assert x . shape == self . input_shape # Input projection x = self . in_projection ( x ) # Resnet blocks dynamic , static = eqx . partition ( self . blocks , eqx . is_array ) def f ( x , params ): block = eqx . combine ( params , static ) return block ( x , y ), None out , _ = jax . lax . scan ( f , x , dynamic ) # Output projection out = self . out_projection ( out ) return out __init__ ( self , input_shape : Tuple [ int ], working_size : int , hidden_size : int , out_size : int , n_blocks : int , filter_shape : Optional [ Tuple [ int ]] = ( 3 , 3 ), cond_shape : Optional [ Tuple [ int ]] = None , activation : Callable = < PjitFunction of < function silu at 0x7f979a92dfc0 >> , * , key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : input_shape : The input size. Output size is the same as input_shape. working_size : The size (channels for images) of each hidden layer. hidden_size : The size (channels for images) of each hidden layer. out_size : The output size. For images, this is the number of output channels. n_blocks : The number of residual blocks. cond_shape : The size of the conditioning information. activation : The activation function in each residual block. key : A jax.random.PRNGKey for initialization. Source code in generax/nn/resnet.py def __init__ ( self , input_shape : Tuple [ int ], working_size : int , hidden_size : int , out_size : int , n_blocks : int , filter_shape : Optional [ Tuple [ int ]] = ( 3 , 3 ), cond_shape : Optional [ Tuple [ int ]] = None , activation : Callable = jax . nn . swish , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input size. Output size is the same as input_shape. - `working_size`: The size (channels for images) of each hidden layer. - `hidden_size`: The size (channels for images) of each hidden layer. - `out_size`: The output size. For images, this is the number of output channels. - `n_blocks`: The number of residual blocks. - `cond_shape`: The size of the conditioning information. - `activation`: The activation function in each residual block. - `key`: A `jax.random.PRNGKey` for initialization. \"\"\" super () . __init__ ( ** kwargs ) if len ( input_shape ) not in [ 1 , 3 ]: raise ValueError ( f 'Expected 1d or 3d input shape' ) image = False if len ( input_shape ) == 3 : H , W , C = input_shape image = True assert filter_shape is not None , 'Must pass in filter shape when processing images' self . n_blocks = n_blocks self . working_size = working_size self . hidden_size = hidden_size self . filter_shape = filter_shape self . out_size = out_size k1 , k2 , k3 = random . split ( key , 3 ) if isinstance ( input_shape , int ): input_shape = ( input_shape ,) self . input_shape = input_shape self . cond_shape = cond_shape if image == False : self . in_projection = WeightNormDense ( in_size = input_shape [ 0 ], out_size = working_size , key = k1 ) working_shape = ( working_size ,) else : self . in_projection = ConvAndGroupNorm ( input_shape = input_shape , out_size = working_size , filter_shape = filter_shape , groups = 1 , key = k1 ) working_shape = ( H , W , working_size ) def make_resblock ( k ): return GatedResBlock ( input_shape = working_shape , hidden_size = hidden_size , cond_shape = cond_shape , activation = activation , filter_shape = filter_shape , key = k ) keys = random . split ( k2 , n_blocks ) self . blocks = eqx . filter_vmap ( make_resblock )( keys ) if image == False : self . out_projection = WeightNormDense ( in_size = working_size , out_size = out_size , key = k3 ) else : self . out_projection = ConvAndGroupNorm ( input_shape = working_shape , out_size = out_size , filter_shape = filter_shape , groups = 1 , key = k3 ) data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> Module \u00a4 Initialize the parameters of the layer based on the data. Arguments : x : The data to initialize the parameters with. y : The conditioning information key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/nn/resnet.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' k1 , k2 , k3 = random . split ( key , 3 ) # Input projection in_proj = self . in_projection . data_dependent_init ( x , key = k1 ) x = eqx . filter_vmap ( in_proj )( x ) # Scan over the vmapped blocks params , state = eqx . partition ( self . blocks , eqx . is_array ) def scan_body ( x , inputs ): key , block_params = inputs block = eqx . combine ( block_params , state ) new_block = block . data_dependent_init ( x , y , key = key ) new_x = eqx . filter_vmap ( new_block )( x , y ) new_params , _ = eqx . partition ( block , eqx . is_array ) return new_x , new_params keys = random . split ( k2 , self . n_blocks ) x , params = jax . lax . scan ( scan_body , x , ( keys , params )) blocks = eqx . combine ( params , state ) out_proj = self . out_projection . data_dependent_init ( x , key = k3 ) # Turn the new parameters into a new module get_in_proj = lambda tree : tree . in_projection get_blocks = lambda tree : tree . blocks get_out_proj = lambda tree : tree . out_projection updated_layer = eqx . tree_at ( get_in_proj , self , in_proj ) updated_layer = eqx . tree_at ( get_blocks , updated_layer , blocks ) updated_layer = eqx . tree_at ( get_out_proj , updated_layer , out_proj ) return updated_layer __call__ ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Arguments: t : A JAX array with shape () . x : A JAX array with shape (input_shape,) . y : A JAX array with shape (cond_shape,) . Returns: A JAX array with shape (input_shape,) . Source code in generax/nn/resnet.py def __call__ ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"**Arguments:** - `t`: A JAX array with shape `()`. - `x`: A JAX array with shape `(input_shape,)`. - `y`: A JAX array with shape `(cond_shape,)`. **Returns:** A JAX array with shape `(input_shape,)`. \"\"\" assert x . shape == self . input_shape # Input projection x = self . in_projection ( x ) # Resnet blocks dynamic , static = eqx . partition ( self . blocks , eqx . is_array ) def f ( x , params ): block = eqx . combine ( params , static ) return block ( x , y ), None out , _ = jax . lax . scan ( f , x , dynamic ) # Output projection out = self . out_projection ( out ) return out generax.nn.resnet.TimeDependentResNet ( ResNet ) \u00a4 A time dependent version of a 1d resnet Source code in generax/nn/resnet.py class TimeDependentResNet ( ResNet ): \"\"\"A time dependent version of a 1d resnet \"\"\" time_features : TimeFeatures def __init__ ( self , input_shape : Tuple [ int ], working_size : int , hidden_size : int , out_size : int , n_blocks : int , filter_shape : Optional [ Tuple [ int ]] = ( 3 , 3 ), cond_shape : Optional [ Tuple [ int ]] = None , activation : Callable = jax . nn . swish , embedding_size : Optional [ int ] = 16 , out_features : int = 8 , * , key : PRNGKeyArray , ** kwargs ): k1 , k2 = random . split ( key , 2 ) self . time_features = TimeFeatures ( embedding_size = embedding_size , out_features = out_features , key = k1 , ** kwargs ) total_cond_size = out_features if cond_shape is not None : if len ( cond_shape ) != 1 : raise ValueError ( f 'Expected 1d conditional input.' ) total_cond_size += cond_shape [ 0 ] super () . __init__ ( input_shape = input_shape , working_size = working_size , hidden_size = hidden_size , out_size = out_size , n_blocks = n_blocks , filter_shape = filter_shape , cond_shape = ( total_cond_size ,), activation = activation , key = k2 , ** kwargs ) def data_dependent_init ( self , t : Array , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `t`: The time to initialize the parameters with. - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert t . ndim == 1 h = eqx . filter_vmap ( self . time_features )( t ) if y is not None : h = jnp . concatenate ([ h , y ], axis =- 1 ) return super () . data_dependent_init ( x , y = h , key = key ) def __call__ ( self , t : Array , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : assert t . shape == () h = self . time_features ( t ) if y is not None : h = jnp . concatenate ([ h , y ], axis =- 1 ) return super () . __call__ ( x , y = h ) __init__ ( self , input_shape : Tuple [ int ], working_size : int , hidden_size : int , out_size : int , n_blocks : int , filter_shape : Optional [ Tuple [ int ]] = ( 3 , 3 ), cond_shape : Optional [ Tuple [ int ]] = None , activation : Callable = < PjitFunction of < function silu at 0x7f979a92dfc0 >> , embedding_size : Optional [ int ] = 16 , out_features : int = 8 , * , key : PRNGKeyArray , ** kwargs ) \u00a4 Source code in generax/nn/resnet.py def __init__ ( self , input_shape : Tuple [ int ], working_size : int , hidden_size : int , out_size : int , n_blocks : int , filter_shape : Optional [ Tuple [ int ]] = ( 3 , 3 ), cond_shape : Optional [ Tuple [ int ]] = None , activation : Callable = jax . nn . swish , embedding_size : Optional [ int ] = 16 , out_features : int = 8 , * , key : PRNGKeyArray , ** kwargs ): k1 , k2 = random . split ( key , 2 ) self . time_features = TimeFeatures ( embedding_size = embedding_size , out_features = out_features , key = k1 , ** kwargs ) total_cond_size = out_features if cond_shape is not None : if len ( cond_shape ) != 1 : raise ValueError ( f 'Expected 1d conditional input.' ) total_cond_size += cond_shape [ 0 ] super () . __init__ ( input_shape = input_shape , working_size = working_size , hidden_size = hidden_size , out_size = out_size , n_blocks = n_blocks , filter_shape = filter_shape , cond_shape = ( total_cond_size ,), activation = activation , key = k2 , ** kwargs ) data_dependent_init ( self , t : Array , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> Module \u00a4 Initialize the parameters of the layer based on the data. Arguments : t : The time to initialize the parameters with. x : The data to initialize the parameters with. y : The conditioning information key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/nn/resnet.py def data_dependent_init ( self , t : Array , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `t`: The time to initialize the parameters with. - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert t . ndim == 1 h = eqx . filter_vmap ( self . time_features )( t ) if y is not None : h = jnp . concatenate ([ h , y ], axis =- 1 ) return super () . data_dependent_init ( x , y = h , key = key ) __call__ ( self , t : Array , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array \u00a4 Arguments: t : A JAX array with shape () . x : A JAX array with shape (input_shape,) . y : A JAX array with shape (cond_shape,) . Returns: A JAX array with shape (input_shape,) . Source code in generax/nn/resnet.py def __call__ ( self , t : Array , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : assert t . shape == () h = self . time_features ( t ) if y is not None : h = jnp . concatenate ([ h , y ], axis =- 1 ) return super () . __call__ ( x , y = h )","title":"Resnet"},{"location":"api/nn/resnet/#resnet","text":"","title":"Resnet"},{"location":"api/nn/resnet/#generax.nn.resnet.ResNet","text":"ResNet for 1d data Source code in generax/nn/resnet.py class ResNet ( eqx . Module ): \"\"\"ResNet for 1d data\"\"\" n_blocks : int = eqx . field ( static = True ) blocks : tuple [ GatedResBlock , ... ] in_projection : eqx . nn . Linear out_projection : eqx . nn . Linear input_shape : int = eqx . field ( static = True ) cond_shape : int = eqx . field ( static = True ) working_size : int = eqx . field ( static = True ) hidden_size : int = eqx . field ( static = True ) out_size : int = eqx . field ( static = True ) filter_shape : Union [ Tuple [ int ], None ] = eqx . field ( static = True ) def __init__ ( self , input_shape : Tuple [ int ], working_size : int , hidden_size : int , out_size : int , n_blocks : int , filter_shape : Optional [ Tuple [ int ]] = ( 3 , 3 ), cond_shape : Optional [ Tuple [ int ]] = None , activation : Callable = jax . nn . swish , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input size. Output size is the same as input_shape. - `working_size`: The size (channels for images) of each hidden layer. - `hidden_size`: The size (channels for images) of each hidden layer. - `out_size`: The output size. For images, this is the number of output channels. - `n_blocks`: The number of residual blocks. - `cond_shape`: The size of the conditioning information. - `activation`: The activation function in each residual block. - `key`: A `jax.random.PRNGKey` for initialization. \"\"\" super () . __init__ ( ** kwargs ) if len ( input_shape ) not in [ 1 , 3 ]: raise ValueError ( f 'Expected 1d or 3d input shape' ) image = False if len ( input_shape ) == 3 : H , W , C = input_shape image = True assert filter_shape is not None , 'Must pass in filter shape when processing images' self . n_blocks = n_blocks self . working_size = working_size self . hidden_size = hidden_size self . filter_shape = filter_shape self . out_size = out_size k1 , k2 , k3 = random . split ( key , 3 ) if isinstance ( input_shape , int ): input_shape = ( input_shape ,) self . input_shape = input_shape self . cond_shape = cond_shape if image == False : self . in_projection = WeightNormDense ( in_size = input_shape [ 0 ], out_size = working_size , key = k1 ) working_shape = ( working_size ,) else : self . in_projection = ConvAndGroupNorm ( input_shape = input_shape , out_size = working_size , filter_shape = filter_shape , groups = 1 , key = k1 ) working_shape = ( H , W , working_size ) def make_resblock ( k ): return GatedResBlock ( input_shape = working_shape , hidden_size = hidden_size , cond_shape = cond_shape , activation = activation , filter_shape = filter_shape , key = k ) keys = random . split ( k2 , n_blocks ) self . blocks = eqx . filter_vmap ( make_resblock )( keys ) if image == False : self . out_projection = WeightNormDense ( in_size = working_size , out_size = out_size , key = k3 ) else : self . out_projection = ConvAndGroupNorm ( input_shape = working_shape , out_size = out_size , filter_shape = filter_shape , groups = 1 , key = k3 ) def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' k1 , k2 , k3 = random . split ( key , 3 ) # Input projection in_proj = self . in_projection . data_dependent_init ( x , key = k1 ) x = eqx . filter_vmap ( in_proj )( x ) # Scan over the vmapped blocks params , state = eqx . partition ( self . blocks , eqx . is_array ) def scan_body ( x , inputs ): key , block_params = inputs block = eqx . combine ( block_params , state ) new_block = block . data_dependent_init ( x , y , key = key ) new_x = eqx . filter_vmap ( new_block )( x , y ) new_params , _ = eqx . partition ( block , eqx . is_array ) return new_x , new_params keys = random . split ( k2 , self . n_blocks ) x , params = jax . lax . scan ( scan_body , x , ( keys , params )) blocks = eqx . combine ( params , state ) out_proj = self . out_projection . data_dependent_init ( x , key = k3 ) # Turn the new parameters into a new module get_in_proj = lambda tree : tree . in_projection get_blocks = lambda tree : tree . blocks get_out_proj = lambda tree : tree . out_projection updated_layer = eqx . tree_at ( get_in_proj , self , in_proj ) updated_layer = eqx . tree_at ( get_blocks , updated_layer , blocks ) updated_layer = eqx . tree_at ( get_out_proj , updated_layer , out_proj ) return updated_layer def __call__ ( self , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : \"\"\"**Arguments:** - `t`: A JAX array with shape `()`. - `x`: A JAX array with shape `(input_shape,)`. - `y`: A JAX array with shape `(cond_shape,)`. **Returns:** A JAX array with shape `(input_shape,)`. \"\"\" assert x . shape == self . input_shape # Input projection x = self . in_projection ( x ) # Resnet blocks dynamic , static = eqx . partition ( self . blocks , eqx . is_array ) def f ( x , params ): block = eqx . combine ( params , static ) return block ( x , y ), None out , _ = jax . lax . scan ( f , x , dynamic ) # Output projection out = self . out_projection ( out ) return out","title":"ResNet"},{"location":"api/nn/resnet/#generax.nn.resnet.TimeDependentResNet","text":"A time dependent version of a 1d resnet Source code in generax/nn/resnet.py class TimeDependentResNet ( ResNet ): \"\"\"A time dependent version of a 1d resnet \"\"\" time_features : TimeFeatures def __init__ ( self , input_shape : Tuple [ int ], working_size : int , hidden_size : int , out_size : int , n_blocks : int , filter_shape : Optional [ Tuple [ int ]] = ( 3 , 3 ), cond_shape : Optional [ Tuple [ int ]] = None , activation : Callable = jax . nn . swish , embedding_size : Optional [ int ] = 16 , out_features : int = 8 , * , key : PRNGKeyArray , ** kwargs ): k1 , k2 = random . split ( key , 2 ) self . time_features = TimeFeatures ( embedding_size = embedding_size , out_features = out_features , key = k1 , ** kwargs ) total_cond_size = out_features if cond_shape is not None : if len ( cond_shape ) != 1 : raise ValueError ( f 'Expected 1d conditional input.' ) total_cond_size += cond_shape [ 0 ] super () . __init__ ( input_shape = input_shape , working_size = working_size , hidden_size = hidden_size , out_size = out_size , n_blocks = n_blocks , filter_shape = filter_shape , cond_shape = ( total_cond_size ,), activation = activation , key = k2 , ** kwargs ) def data_dependent_init ( self , t : Array , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `t`: The time to initialize the parameters with. - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert t . ndim == 1 h = eqx . filter_vmap ( self . time_features )( t ) if y is not None : h = jnp . concatenate ([ h , y ], axis =- 1 ) return super () . data_dependent_init ( x , y = h , key = key ) def __call__ ( self , t : Array , x : Array , y : Optional [ Array ] = None , ** kwargs ) -> Array : assert t . shape == () h = self . time_features ( t ) if y is not None : h = jnp . concatenate ([ h , y ], axis =- 1 ) return super () . __call__ ( x , y = h )","title":"TimeDependentResNet"},{"location":"api/nn/resnet_blocks/","text":"Resnet Blocks \u00a4 generax.nn.resnet_blocks.GatedResBlock \u00a4 Gated residual block for 1d data or images. Source code in generax/nn/resnet_blocks.py class GatedResBlock ( eqx . Module ): \"\"\"Gated residual block for 1d data or images.\"\"\" linear_cond : Union [ Union [ WeightNormDense , ConvAndGroupNorm ], None ] linear1 : Union [ WeightNormDense , ConvAndGroupNorm ] linear2 : Union [ WeightNormDense , ConvAndGroupNorm ] activation : Callable input_shape : Tuple [ int ] = eqx . field ( static = True ) hidden_size : int = eqx . field ( static = True ) cond_shape : Tuple [ int ] = eqx . field ( static = True ) filter_shape : Union [ Tuple [ int ], None ] = eqx . field ( static = True ) groups : Union [ int , None ] = eqx . field ( static = True ) def __init__ ( self , input_shape : Tuple [ int ], hidden_size : int , groups : Optional [ int ] = None , filter_shape : Optional [ Tuple [ int ]] = None , cond_shape : Optional [ Tuple [ int ]] = None , activation : Callable = jax . nn . swish , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input size. Output size is the same as `input_shape`. - `hidden_size`: The hidden layer size. - `cond_shape`: The size of the conditioning information. - `activation`: The activation function after each hidden layer. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( ** kwargs ) if len ( input_shape ) not in [ 1 , 3 ]: raise ValueError ( f 'Expected 1d or 3d input shape' ) image = False if len ( input_shape ) == 3 : H , W , C = input_shape image = True assert filter_shape is not None , 'Must pass in filter shape when processing images' self . input_shape = input_shape self . hidden_size = hidden_size self . cond_shape = cond_shape self . filter_shape = filter_shape self . activation = activation if groups is not None : assert image if hidden_size % groups != 0 : raise ValueError ( f 'Hidden size must be divisible by groups' ) self . groups = groups k1 , k2 , k3 = random . split ( key , 3 ) # Initialize the conditioning parameters if cond_shape is not None : if len ( cond_shape ) == 1 : self . linear_cond = WeightNormDense ( in_size = cond_shape [ 0 ], out_size = 2 * hidden_size , key = k1 ) else : self . linear_cond = ConvAndGroupNorm ( input_shape = cond_shape , out_size = 2 * hidden_size , filter_shape = filter_shape , groups = groups , key = k1 ) else : self . linear_cond = None if image : self . linear1 = ConvAndGroupNorm ( input_shape = input_shape , out_size = hidden_size , filter_shape = filter_shape , groups = groups , key = k2 ) hidden_shape = ( H , W , hidden_size ) self . linear2 = WeightNormConv ( input_shape = hidden_shape , out_size = 2 * C , filter_shape = filter_shape , key = k3 ) else : self . linear1 = WeightNormDense ( in_size = input_shape [ 0 ], out_size = hidden_size , key = k2 ) self . linear2 = WeightNormDense ( in_size = hidden_size , out_size = 2 * input_shape [ 0 ], key = k3 ) def data_dependent_init ( self , x : Array , y : Array = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' k1 , k2 , k3 = random . split ( key , 3 ) # Initialize the conditioning parameters if y is not None : linear_cond = self . linear_cond . data_dependent_init ( y , key = k1 ) h = eqx . filter_vmap ( linear_cond )( y ) shift , scale = jnp . split ( h , 2 , axis =- 1 ) else : linear_cond = None # Linear + shift/scale + activation linear1 = self . linear1 . data_dependent_init ( x , key = k2 ) x = eqx . filter_vmap ( linear1 )( x ) if y is not None : x = shift + x * ( 1 + scale ) x = eqx . filter_vmap ( self . activation )( x ) # Linear + gate linear2 = self . linear2 . data_dependent_init ( x , key = k3 ) # Turn the new parameters into a new module get_linear_cond = lambda tree : tree . linear_cond get_linear1 = lambda tree : tree . linear1 get_linear2 = lambda tree : tree . linear2 updated_layer = eqx . tree_at ( get_linear_cond , self , linear_cond ) updated_layer = eqx . tree_at ( get_linear1 , updated_layer , linear1 ) updated_layer = eqx . tree_at ( get_linear2 , updated_layer , linear2 ) return updated_layer def __call__ ( self , x : Array , y : Array = None ) -> Array : \"\"\"**Arguments:** - `x`: A JAX array with shape `input_shape`. - `y`: A JAX array to condition on with shape `cond_shape`. **Returns:** A JAX array with shape `input_shape`. \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' x_in = x # The conditioning input will shift/scale x if y is not None : h = self . linear_cond ( self . activation ( y )) shift , scale = jnp . split ( h , 2 , axis =- 1 ) # Linear + shift/scale + activation x = self . linear1 ( x ) if y is not None : x = shift + x * ( 1 + scale ) x = self . activation ( x ) # Linear + gate x = self . linear2 ( x ) a , b = jnp . split ( x , 2 , axis =- 1 ) return x_in + a * jax . nn . sigmoid ( b ) __init__ ( self , input_shape : Tuple [ int ], hidden_size : int , groups : Optional [ int ] = None , filter_shape : Optional [ Tuple [ int ]] = None , cond_shape : Optional [ Tuple [ int ]] = None , activation : Callable = < PjitFunction of < function silu at 0x7f979a92dfc0 >> , * , key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : input_shape : The input size. Output size is the same as input_shape . hidden_size : The hidden layer size. cond_shape : The size of the conditioning information. activation : The activation function after each hidden layer. key : A jax.random.PRNGKey for initialization Source code in generax/nn/resnet_blocks.py def __init__ ( self , input_shape : Tuple [ int ], hidden_size : int , groups : Optional [ int ] = None , filter_shape : Optional [ Tuple [ int ]] = None , cond_shape : Optional [ Tuple [ int ]] = None , activation : Callable = jax . nn . swish , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input size. Output size is the same as `input_shape`. - `hidden_size`: The hidden layer size. - `cond_shape`: The size of the conditioning information. - `activation`: The activation function after each hidden layer. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( ** kwargs ) if len ( input_shape ) not in [ 1 , 3 ]: raise ValueError ( f 'Expected 1d or 3d input shape' ) image = False if len ( input_shape ) == 3 : H , W , C = input_shape image = True assert filter_shape is not None , 'Must pass in filter shape when processing images' self . input_shape = input_shape self . hidden_size = hidden_size self . cond_shape = cond_shape self . filter_shape = filter_shape self . activation = activation if groups is not None : assert image if hidden_size % groups != 0 : raise ValueError ( f 'Hidden size must be divisible by groups' ) self . groups = groups k1 , k2 , k3 = random . split ( key , 3 ) # Initialize the conditioning parameters if cond_shape is not None : if len ( cond_shape ) == 1 : self . linear_cond = WeightNormDense ( in_size = cond_shape [ 0 ], out_size = 2 * hidden_size , key = k1 ) else : self . linear_cond = ConvAndGroupNorm ( input_shape = cond_shape , out_size = 2 * hidden_size , filter_shape = filter_shape , groups = groups , key = k1 ) else : self . linear_cond = None if image : self . linear1 = ConvAndGroupNorm ( input_shape = input_shape , out_size = hidden_size , filter_shape = filter_shape , groups = groups , key = k2 ) hidden_shape = ( H , W , hidden_size ) self . linear2 = WeightNormConv ( input_shape = hidden_shape , out_size = 2 * C , filter_shape = filter_shape , key = k3 ) else : self . linear1 = WeightNormDense ( in_size = input_shape [ 0 ], out_size = hidden_size , key = k2 ) self . linear2 = WeightNormDense ( in_size = hidden_size , out_size = 2 * input_shape [ 0 ], key = k3 ) data_dependent_init ( self , x : Array , y : Array = None , key : PRNGKeyArray = None ) -> Module \u00a4 Initialize the parameters of the layer based on the data. Arguments : x : The data to initialize the parameters with. y : The conditioning information key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/nn/resnet_blocks.py def data_dependent_init ( self , x : Array , y : Array = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' k1 , k2 , k3 = random . split ( key , 3 ) # Initialize the conditioning parameters if y is not None : linear_cond = self . linear_cond . data_dependent_init ( y , key = k1 ) h = eqx . filter_vmap ( linear_cond )( y ) shift , scale = jnp . split ( h , 2 , axis =- 1 ) else : linear_cond = None # Linear + shift/scale + activation linear1 = self . linear1 . data_dependent_init ( x , key = k2 ) x = eqx . filter_vmap ( linear1 )( x ) if y is not None : x = shift + x * ( 1 + scale ) x = eqx . filter_vmap ( self . activation )( x ) # Linear + gate linear2 = self . linear2 . data_dependent_init ( x , key = k3 ) # Turn the new parameters into a new module get_linear_cond = lambda tree : tree . linear_cond get_linear1 = lambda tree : tree . linear1 get_linear2 = lambda tree : tree . linear2 updated_layer = eqx . tree_at ( get_linear_cond , self , linear_cond ) updated_layer = eqx . tree_at ( get_linear1 , updated_layer , linear1 ) updated_layer = eqx . tree_at ( get_linear2 , updated_layer , linear2 ) return updated_layer __call__ ( self , x : Array , y : Array = None ) -> Array \u00a4 Arguments: x : A JAX array with shape input_shape . y : A JAX array to condition on with shape cond_shape . Returns: A JAX array with shape input_shape . Source code in generax/nn/resnet_blocks.py def __call__ ( self , x : Array , y : Array = None ) -> Array : \"\"\"**Arguments:** - `x`: A JAX array with shape `input_shape`. - `y`: A JAX array to condition on with shape `cond_shape`. **Returns:** A JAX array with shape `input_shape`. \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' x_in = x # The conditioning input will shift/scale x if y is not None : h = self . linear_cond ( self . activation ( y )) shift , scale = jnp . split ( h , 2 , axis =- 1 ) # Linear + shift/scale + activation x = self . linear1 ( x ) if y is not None : x = shift + x * ( 1 + scale ) x = self . activation ( x ) # Linear + gate x = self . linear2 ( x ) a , b = jnp . split ( x , 2 , axis =- 1 ) return x_in + a * jax . nn . sigmoid ( b ) generax.nn.resnet_blocks.Block \u00a4 Group norm, (shift+scale), activation, conv Source code in generax/nn/resnet_blocks.py class Block ( eqx . Module ): \"\"\"Group norm, (shift+scale), activation, conv \"\"\" input_shape : int = eqx . field ( static = True ) conv : WeightNormConv norm : eqx . nn . GroupNorm def __init__ ( self , input_shape : Tuple [ int ], out_size : int , groups : int , * , key : PRNGKeyArray , ** kwargs ): super () . __init__ ( ** kwargs ) H , W , C = input_shape if C % groups != 0 : raise ValueError ( \"The number of groups must divide the number of channels.\" ) self . norm = ChannelConvention ( eqx . nn . GroupNorm ( groups = groups , channels = C )) self . conv = WeightNormConv ( input_shape = input_shape , filter_shape = ( 3 , 3 ), out_size = out_size , key = key ) self . input_shape = self . conv . input_shape def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> eqx . Module : return self def __call__ ( self , x : Array , y : Array = None , shift_scale : Optional [ Array ] = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' H , W , C = self . input_shape h = self . norm ( x ) if shift_scale is not None : shift , scale = shift_scale h = shift + h * ( 1 + scale ) h = jax . nn . silu ( h ) h = self . conv ( h ) return h __init__ ( self , input_shape : Tuple [ int ], out_size : int , groups : int , * , key : PRNGKeyArray , ** kwargs ) \u00a4 Source code in generax/nn/resnet_blocks.py def __init__ ( self , input_shape : Tuple [ int ], out_size : int , groups : int , * , key : PRNGKeyArray , ** kwargs ): super () . __init__ ( ** kwargs ) H , W , C = input_shape if C % groups != 0 : raise ValueError ( \"The number of groups must divide the number of channels.\" ) self . norm = ChannelConvention ( eqx . nn . GroupNorm ( groups = groups , channels = C )) self . conv = WeightNormConv ( input_shape = input_shape , filter_shape = ( 3 , 3 ), out_size = out_size , key = key ) self . input_shape = self . conv . input_shape data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> Module \u00a4 Source code in generax/nn/resnet_blocks.py def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> eqx . Module : return self __call__ ( self , x : Array , y : Array = None , shift_scale : Optional [ Array ] = None ) -> Array \u00a4 Call self as a function. Source code in generax/nn/resnet_blocks.py def __call__ ( self , x : Array , y : Array = None , shift_scale : Optional [ Array ] = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' H , W , C = self . input_shape h = self . norm ( x ) if shift_scale is not None : shift , scale = shift_scale h = shift + h * ( 1 + scale ) h = jax . nn . silu ( h ) h = self . conv ( h ) return h generax.nn.resnet_blocks.ImageResBlock \u00a4 Gated residual block for images. Source code in generax/nn/resnet_blocks.py class ImageResBlock ( eqx . Module ): \"\"\"Gated residual block for images.\"\"\" linear_cond : Union [ ConvAndGroupNorm , None ] block1 : Block block2 : Block res_conv : Union [ ConvAndGroupNorm , eqx . nn . Identity ] gca : GatedGlobalContext input_shape : Tuple [ int ] = eqx . field ( static = True ) hidden_size : int = eqx . field ( static = True ) out_size : int = eqx . field ( static = True ) cond_shape : Tuple [ int ] = eqx . field ( static = True ) groups : Union [ int , None ] = eqx . field ( static = True ) def __init__ ( self , input_shape : Tuple [ int ], hidden_size : int , out_size : int , groups : Optional [ int ] = None , cond_shape : Optional [ Tuple [ int ]] = None , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input size. Output size is the same as `input_shape`. - `hidden_size`: The hidden layer size. - `cond_shape`: The size of the conditioning information. - `activation`: The activation function after each hidden layer. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( ** kwargs ) H , W , C = input_shape self . input_shape = input_shape self . hidden_size = hidden_size self . cond_shape = cond_shape self . out_size = out_size if hidden_size % groups != 0 : raise ValueError ( f 'Hidden size must be divisible by groups' ) self . groups = groups k1 , k2 , k3 , k4 , k5 = random . split ( key , 5 ) # Initialize the conditioning parameters if cond_shape is not None : if len ( cond_shape ) != 1 : raise ValueError ( f 'Conditioning shape must be 1d' ) self . linear_cond = WeightNormDense ( in_size = cond_shape [ 0 ], out_size = 2 * hidden_size , key = k1 ) else : self . linear_cond = None self . block1 = Block ( input_shape = input_shape , out_size = hidden_size , groups = groups , key = k2 ) self . block2 = Block ( input_shape = ( H , W , hidden_size ), out_size = out_size , groups = groups , key = k3 ) self . gca = GatedGlobalContext ( input_shape = ( H , W , out_size ), key = k4 ) if out_size != C : self . res_conv = WeightNormConv ( input_shape = input_shape , out_size = out_size , filter_shape = ( 3 , 3 ), key = k5 ) else : self . res_conv = eqx . nn . Identity () def data_dependent_init ( self , x : Array , y : Array = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" return self def __call__ ( self , x : Array , y : Array = None ) -> Array : \"\"\"**Arguments:** - `x`: A JAX array with shape `input_shape`. - `y`: A JAX array to condition on with shape `cond_shape`. **Returns:** A JAX array with shape `input_shape`. \"\"\" x_in = x h = self . block1 ( x ) # The conditioning input will shift/scale x if y is not None : hh = self . linear_cond ( jax . nn . silu ( y )) shift_scale = jnp . split ( hh , 2 , axis =- 1 ) else : shift_scale = None h = self . block2 ( h , shift_scale = shift_scale ) h = self . gca ( h ) return self . res_conv ( x_in ) + h __init__ ( self , input_shape : Tuple [ int ], hidden_size : int , out_size : int , groups : Optional [ int ] = None , cond_shape : Optional [ Tuple [ int ]] = None , * , key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : input_shape : The input size. Output size is the same as input_shape . hidden_size : The hidden layer size. cond_shape : The size of the conditioning information. activation : The activation function after each hidden layer. key : A jax.random.PRNGKey for initialization Source code in generax/nn/resnet_blocks.py def __init__ ( self , input_shape : Tuple [ int ], hidden_size : int , out_size : int , groups : Optional [ int ] = None , cond_shape : Optional [ Tuple [ int ]] = None , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input size. Output size is the same as `input_shape`. - `hidden_size`: The hidden layer size. - `cond_shape`: The size of the conditioning information. - `activation`: The activation function after each hidden layer. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( ** kwargs ) H , W , C = input_shape self . input_shape = input_shape self . hidden_size = hidden_size self . cond_shape = cond_shape self . out_size = out_size if hidden_size % groups != 0 : raise ValueError ( f 'Hidden size must be divisible by groups' ) self . groups = groups k1 , k2 , k3 , k4 , k5 = random . split ( key , 5 ) # Initialize the conditioning parameters if cond_shape is not None : if len ( cond_shape ) != 1 : raise ValueError ( f 'Conditioning shape must be 1d' ) self . linear_cond = WeightNormDense ( in_size = cond_shape [ 0 ], out_size = 2 * hidden_size , key = k1 ) else : self . linear_cond = None self . block1 = Block ( input_shape = input_shape , out_size = hidden_size , groups = groups , key = k2 ) self . block2 = Block ( input_shape = ( H , W , hidden_size ), out_size = out_size , groups = groups , key = k3 ) self . gca = GatedGlobalContext ( input_shape = ( H , W , out_size ), key = k4 ) if out_size != C : self . res_conv = WeightNormConv ( input_shape = input_shape , out_size = out_size , filter_shape = ( 3 , 3 ), key = k5 ) else : self . res_conv = eqx . nn . Identity () data_dependent_init ( self , x : Array , y : Array = None , key : PRNGKeyArray = None ) -> Module \u00a4 Initialize the parameters of the layer based on the data. Arguments : x : The data to initialize the parameters with. y : The conditioning information key : A jax.random.PRNGKey for initialization Returns : A new layer with the parameters initialized. Source code in generax/nn/resnet_blocks.py def data_dependent_init ( self , x : Array , y : Array = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" return self __call__ ( self , x : Array , y : Array = None ) -> Array \u00a4 Arguments: x : A JAX array with shape input_shape . y : A JAX array to condition on with shape cond_shape . Returns: A JAX array with shape input_shape . Source code in generax/nn/resnet_blocks.py def __call__ ( self , x : Array , y : Array = None ) -> Array : \"\"\"**Arguments:** - `x`: A JAX array with shape `input_shape`. - `y`: A JAX array to condition on with shape `cond_shape`. **Returns:** A JAX array with shape `input_shape`. \"\"\" x_in = x h = self . block1 ( x ) # The conditioning input will shift/scale x if y is not None : hh = self . linear_cond ( jax . nn . silu ( y )) shift_scale = jnp . split ( hh , 2 , axis =- 1 ) else : shift_scale = None h = self . block2 ( h , shift_scale = shift_scale ) h = self . gca ( h ) return self . res_conv ( x_in ) + h","title":"Resnet Blocks"},{"location":"api/nn/resnet_blocks/#resnet-blocks","text":"","title":"Resnet Blocks"},{"location":"api/nn/resnet_blocks/#generax.nn.resnet_blocks.GatedResBlock","text":"Gated residual block for 1d data or images. Source code in generax/nn/resnet_blocks.py class GatedResBlock ( eqx . Module ): \"\"\"Gated residual block for 1d data or images.\"\"\" linear_cond : Union [ Union [ WeightNormDense , ConvAndGroupNorm ], None ] linear1 : Union [ WeightNormDense , ConvAndGroupNorm ] linear2 : Union [ WeightNormDense , ConvAndGroupNorm ] activation : Callable input_shape : Tuple [ int ] = eqx . field ( static = True ) hidden_size : int = eqx . field ( static = True ) cond_shape : Tuple [ int ] = eqx . field ( static = True ) filter_shape : Union [ Tuple [ int ], None ] = eqx . field ( static = True ) groups : Union [ int , None ] = eqx . field ( static = True ) def __init__ ( self , input_shape : Tuple [ int ], hidden_size : int , groups : Optional [ int ] = None , filter_shape : Optional [ Tuple [ int ]] = None , cond_shape : Optional [ Tuple [ int ]] = None , activation : Callable = jax . nn . swish , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input size. Output size is the same as `input_shape`. - `hidden_size`: The hidden layer size. - `cond_shape`: The size of the conditioning information. - `activation`: The activation function after each hidden layer. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( ** kwargs ) if len ( input_shape ) not in [ 1 , 3 ]: raise ValueError ( f 'Expected 1d or 3d input shape' ) image = False if len ( input_shape ) == 3 : H , W , C = input_shape image = True assert filter_shape is not None , 'Must pass in filter shape when processing images' self . input_shape = input_shape self . hidden_size = hidden_size self . cond_shape = cond_shape self . filter_shape = filter_shape self . activation = activation if groups is not None : assert image if hidden_size % groups != 0 : raise ValueError ( f 'Hidden size must be divisible by groups' ) self . groups = groups k1 , k2 , k3 = random . split ( key , 3 ) # Initialize the conditioning parameters if cond_shape is not None : if len ( cond_shape ) == 1 : self . linear_cond = WeightNormDense ( in_size = cond_shape [ 0 ], out_size = 2 * hidden_size , key = k1 ) else : self . linear_cond = ConvAndGroupNorm ( input_shape = cond_shape , out_size = 2 * hidden_size , filter_shape = filter_shape , groups = groups , key = k1 ) else : self . linear_cond = None if image : self . linear1 = ConvAndGroupNorm ( input_shape = input_shape , out_size = hidden_size , filter_shape = filter_shape , groups = groups , key = k2 ) hidden_shape = ( H , W , hidden_size ) self . linear2 = WeightNormConv ( input_shape = hidden_shape , out_size = 2 * C , filter_shape = filter_shape , key = k3 ) else : self . linear1 = WeightNormDense ( in_size = input_shape [ 0 ], out_size = hidden_size , key = k2 ) self . linear2 = WeightNormDense ( in_size = hidden_size , out_size = 2 * input_shape [ 0 ], key = k3 ) def data_dependent_init ( self , x : Array , y : Array = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" assert x . shape [ 1 :] == self . input_shape , 'Only works on batched data' k1 , k2 , k3 = random . split ( key , 3 ) # Initialize the conditioning parameters if y is not None : linear_cond = self . linear_cond . data_dependent_init ( y , key = k1 ) h = eqx . filter_vmap ( linear_cond )( y ) shift , scale = jnp . split ( h , 2 , axis =- 1 ) else : linear_cond = None # Linear + shift/scale + activation linear1 = self . linear1 . data_dependent_init ( x , key = k2 ) x = eqx . filter_vmap ( linear1 )( x ) if y is not None : x = shift + x * ( 1 + scale ) x = eqx . filter_vmap ( self . activation )( x ) # Linear + gate linear2 = self . linear2 . data_dependent_init ( x , key = k3 ) # Turn the new parameters into a new module get_linear_cond = lambda tree : tree . linear_cond get_linear1 = lambda tree : tree . linear1 get_linear2 = lambda tree : tree . linear2 updated_layer = eqx . tree_at ( get_linear_cond , self , linear_cond ) updated_layer = eqx . tree_at ( get_linear1 , updated_layer , linear1 ) updated_layer = eqx . tree_at ( get_linear2 , updated_layer , linear2 ) return updated_layer def __call__ ( self , x : Array , y : Array = None ) -> Array : \"\"\"**Arguments:** - `x`: A JAX array with shape `input_shape`. - `y`: A JAX array to condition on with shape `cond_shape`. **Returns:** A JAX array with shape `input_shape`. \"\"\" assert x . shape == self . input_shape , 'Only works on unbatched data' x_in = x # The conditioning input will shift/scale x if y is not None : h = self . linear_cond ( self . activation ( y )) shift , scale = jnp . split ( h , 2 , axis =- 1 ) # Linear + shift/scale + activation x = self . linear1 ( x ) if y is not None : x = shift + x * ( 1 + scale ) x = self . activation ( x ) # Linear + gate x = self . linear2 ( x ) a , b = jnp . split ( x , 2 , axis =- 1 ) return x_in + a * jax . nn . sigmoid ( b )","title":"GatedResBlock"},{"location":"api/nn/resnet_blocks/#generax.nn.resnet_blocks.Block","text":"Group norm, (shift+scale), activation, conv Source code in generax/nn/resnet_blocks.py class Block ( eqx . Module ): \"\"\"Group norm, (shift+scale), activation, conv \"\"\" input_shape : int = eqx . field ( static = True ) conv : WeightNormConv norm : eqx . nn . GroupNorm def __init__ ( self , input_shape : Tuple [ int ], out_size : int , groups : int , * , key : PRNGKeyArray , ** kwargs ): super () . __init__ ( ** kwargs ) H , W , C = input_shape if C % groups != 0 : raise ValueError ( \"The number of groups must divide the number of channels.\" ) self . norm = ChannelConvention ( eqx . nn . GroupNorm ( groups = groups , channels = C )) self . conv = WeightNormConv ( input_shape = input_shape , filter_shape = ( 3 , 3 ), out_size = out_size , key = key ) self . input_shape = self . conv . input_shape def data_dependent_init ( self , x : Array , y : Optional [ Array ] = None , key : PRNGKeyArray = None ) -> eqx . Module : return self def __call__ ( self , x : Array , y : Array = None , shift_scale : Optional [ Array ] = None ) -> Array : assert x . shape == self . input_shape , 'Only works on unbatched data' H , W , C = self . input_shape h = self . norm ( x ) if shift_scale is not None : shift , scale = shift_scale h = shift + h * ( 1 + scale ) h = jax . nn . silu ( h ) h = self . conv ( h ) return h","title":"Block"},{"location":"api/nn/resnet_blocks/#generax.nn.resnet_blocks.ImageResBlock","text":"Gated residual block for images. Source code in generax/nn/resnet_blocks.py class ImageResBlock ( eqx . Module ): \"\"\"Gated residual block for images.\"\"\" linear_cond : Union [ ConvAndGroupNorm , None ] block1 : Block block2 : Block res_conv : Union [ ConvAndGroupNorm , eqx . nn . Identity ] gca : GatedGlobalContext input_shape : Tuple [ int ] = eqx . field ( static = True ) hidden_size : int = eqx . field ( static = True ) out_size : int = eqx . field ( static = True ) cond_shape : Tuple [ int ] = eqx . field ( static = True ) groups : Union [ int , None ] = eqx . field ( static = True ) def __init__ ( self , input_shape : Tuple [ int ], hidden_size : int , out_size : int , groups : Optional [ int ] = None , cond_shape : Optional [ Tuple [ int ]] = None , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `input_shape`: The input size. Output size is the same as `input_shape`. - `hidden_size`: The hidden layer size. - `cond_shape`: The size of the conditioning information. - `activation`: The activation function after each hidden layer. - `key`: A `jax.random.PRNGKey` for initialization \"\"\" super () . __init__ ( ** kwargs ) H , W , C = input_shape self . input_shape = input_shape self . hidden_size = hidden_size self . cond_shape = cond_shape self . out_size = out_size if hidden_size % groups != 0 : raise ValueError ( f 'Hidden size must be divisible by groups' ) self . groups = groups k1 , k2 , k3 , k4 , k5 = random . split ( key , 5 ) # Initialize the conditioning parameters if cond_shape is not None : if len ( cond_shape ) != 1 : raise ValueError ( f 'Conditioning shape must be 1d' ) self . linear_cond = WeightNormDense ( in_size = cond_shape [ 0 ], out_size = 2 * hidden_size , key = k1 ) else : self . linear_cond = None self . block1 = Block ( input_shape = input_shape , out_size = hidden_size , groups = groups , key = k2 ) self . block2 = Block ( input_shape = ( H , W , hidden_size ), out_size = out_size , groups = groups , key = k3 ) self . gca = GatedGlobalContext ( input_shape = ( H , W , out_size ), key = k4 ) if out_size != C : self . res_conv = WeightNormConv ( input_shape = input_shape , out_size = out_size , filter_shape = ( 3 , 3 ), key = k5 ) else : self . res_conv = eqx . nn . Identity () def data_dependent_init ( self , x : Array , y : Array = None , key : PRNGKeyArray = None ) -> eqx . Module : \"\"\"Initialize the parameters of the layer based on the data. **Arguments**: - `x`: The data to initialize the parameters with. - `y`: The conditioning information - `key`: A `jax.random.PRNGKey` for initialization **Returns**: A new layer with the parameters initialized. \"\"\" return self def __call__ ( self , x : Array , y : Array = None ) -> Array : \"\"\"**Arguments:** - `x`: A JAX array with shape `input_shape`. - `y`: A JAX array to condition on with shape `cond_shape`. **Returns:** A JAX array with shape `input_shape`. \"\"\" x_in = x h = self . block1 ( x ) # The conditioning input will shift/scale x if y is not None : hh = self . linear_cond ( jax . nn . silu ( y )) shift_scale = jnp . split ( hh , 2 , axis =- 1 ) else : shift_scale = None h = self . block2 ( h , shift_scale = shift_scale ) h = self . gca ( h ) return self . res_conv ( x_in ) + h","title":"ImageResBlock"},{"location":"api/nn/time_condition/","text":"Time conditioner \u00a4 generax.nn.time_condition.GaussianFourierProjection \u00a4 GaussianFourierProjection( args, *kwargs) Source code in generax/nn/time_condition.py class GaussianFourierProjection ( eqx . Module ): embedding_size : int = eqx . field ( static = True ) W : eqx . nn . Linear def __init__ ( self , embedding_size : Optional [ int ] = 16 , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `embedding_size`: The size of the embedding. \"\"\" super () . __init__ ( ** kwargs ) self . embedding_size = embedding_size self . W = eqx . nn . Linear ( in_features = 1 , out_features = embedding_size , use_bias = False , key = key ) def __call__ ( self , t : Array ) -> Array : \"\"\"**Arguments:** - `t`: A JAX array with shape `()`. **Returns:** A JAX array with shape `(2*embedding_size,)`. \"\"\" assert t . shape == () t = jnp . expand_dims ( t , axis =- 1 ) t_proj = self . W ( t * 2 * jnp . pi ) return jnp . concatenate ([ jnp . sin ( t_proj ), jnp . cos ( t_proj )], axis =- 1 ) __init__ ( self , embedding_size : Optional [ int ] = 16 , * , key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : embedding_size : The size of the embedding. Source code in generax/nn/time_condition.py def __init__ ( self , embedding_size : Optional [ int ] = 16 , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `embedding_size`: The size of the embedding. \"\"\" super () . __init__ ( ** kwargs ) self . embedding_size = embedding_size self . W = eqx . nn . Linear ( in_features = 1 , out_features = embedding_size , use_bias = False , key = key ) __call__ ( self , t : Array ) -> Array \u00a4 Arguments: t : A JAX array with shape () . Returns: A JAX array with shape (2*embedding_size,) . Source code in generax/nn/time_condition.py def __call__ ( self , t : Array ) -> Array : \"\"\"**Arguments:** - `t`: A JAX array with shape `()`. **Returns:** A JAX array with shape `(2*embedding_size,)`. \"\"\" assert t . shape == () t = jnp . expand_dims ( t , axis =- 1 ) t_proj = self . W ( t * 2 * jnp . pi ) return jnp . concatenate ([ jnp . sin ( t_proj ), jnp . cos ( t_proj )], axis =- 1 ) generax.nn.time_condition.TimeFeatures \u00a4 TimeFeatures( args, *kwargs) Source code in generax/nn/time_condition.py class TimeFeatures ( eqx . Module ): out_features : int = eqx . field ( static = True ) projection : GaussianFourierProjection W1 : Array W2 : Array activation : Callable def __init__ ( self , embedding_size : Optional [ int ] = 16 , out_features : int = 8 , activation : Callable = jax . nn . gelu , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `embedding_size`: The size of the embedding. - `out_features`: The number of output features. - `activation`: The activation function. \"\"\" super () . __init__ ( ** kwargs ) self . out_features = out_features k1 , k2 , k3 = random . split ( key , 3 ) self . projection = GaussianFourierProjection ( embedding_size = embedding_size , key = k1 ) self . W1 = eqx . nn . Linear ( in_features = 2 * embedding_size , out_features = 4 * embedding_size , key = k2 ) self . activation = activation self . W2 = eqx . nn . Linear ( in_features = 4 * embedding_size , out_features = self . out_features , key = k3 ) def __call__ ( self , t : Array ) -> Array : \"\"\"**Arguments:** - `t`: A JAX array with shape `()`. **Returns:** A JAX array with shape `(out_features,)`. \"\"\" assert t . shape == () x = self . projection ( t ) x = self . W1 ( x ) x = self . activation ( x ) return self . W2 ( x ) __init__ ( self , embedding_size : Optional [ int ] = 16 , out_features : int = 8 , activation : Callable = < function gelu > , * , key : PRNGKeyArray , ** kwargs ) \u00a4 Arguments : embedding_size : The size of the embedding. out_features : The number of output features. activation : The activation function. Source code in generax/nn/time_condition.py def __init__ ( self , embedding_size : Optional [ int ] = 16 , out_features : int = 8 , activation : Callable = jax . nn . gelu , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `embedding_size`: The size of the embedding. - `out_features`: The number of output features. - `activation`: The activation function. \"\"\" super () . __init__ ( ** kwargs ) self . out_features = out_features k1 , k2 , k3 = random . split ( key , 3 ) self . projection = GaussianFourierProjection ( embedding_size = embedding_size , key = k1 ) self . W1 = eqx . nn . Linear ( in_features = 2 * embedding_size , out_features = 4 * embedding_size , key = k2 ) self . activation = activation self . W2 = eqx . nn . Linear ( in_features = 4 * embedding_size , out_features = self . out_features , key = k3 ) __call__ ( self , t : Array ) -> Array \u00a4 Arguments: t : A JAX array with shape () . Returns: A JAX array with shape (out_features,) . Source code in generax/nn/time_condition.py def __call__ ( self , t : Array ) -> Array : \"\"\"**Arguments:** - `t`: A JAX array with shape `()`. **Returns:** A JAX array with shape `(out_features,)`. \"\"\" assert t . shape == () x = self . projection ( t ) x = self . W1 ( x ) x = self . activation ( x ) return self . W2 ( x )","title":"Time conditioner"},{"location":"api/nn/time_condition/#time-conditioner","text":"","title":"Time conditioner"},{"location":"api/nn/time_condition/#generax.nn.time_condition.GaussianFourierProjection","text":"GaussianFourierProjection( args, *kwargs) Source code in generax/nn/time_condition.py class GaussianFourierProjection ( eqx . Module ): embedding_size : int = eqx . field ( static = True ) W : eqx . nn . Linear def __init__ ( self , embedding_size : Optional [ int ] = 16 , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `embedding_size`: The size of the embedding. \"\"\" super () . __init__ ( ** kwargs ) self . embedding_size = embedding_size self . W = eqx . nn . Linear ( in_features = 1 , out_features = embedding_size , use_bias = False , key = key ) def __call__ ( self , t : Array ) -> Array : \"\"\"**Arguments:** - `t`: A JAX array with shape `()`. **Returns:** A JAX array with shape `(2*embedding_size,)`. \"\"\" assert t . shape == () t = jnp . expand_dims ( t , axis =- 1 ) t_proj = self . W ( t * 2 * jnp . pi ) return jnp . concatenate ([ jnp . sin ( t_proj ), jnp . cos ( t_proj )], axis =- 1 )","title":"GaussianFourierProjection"},{"location":"api/nn/time_condition/#generax.nn.time_condition.TimeFeatures","text":"TimeFeatures( args, *kwargs) Source code in generax/nn/time_condition.py class TimeFeatures ( eqx . Module ): out_features : int = eqx . field ( static = True ) projection : GaussianFourierProjection W1 : Array W2 : Array activation : Callable def __init__ ( self , embedding_size : Optional [ int ] = 16 , out_features : int = 8 , activation : Callable = jax . nn . gelu , * , key : PRNGKeyArray , ** kwargs ): \"\"\"**Arguments**: - `embedding_size`: The size of the embedding. - `out_features`: The number of output features. - `activation`: The activation function. \"\"\" super () . __init__ ( ** kwargs ) self . out_features = out_features k1 , k2 , k3 = random . split ( key , 3 ) self . projection = GaussianFourierProjection ( embedding_size = embedding_size , key = k1 ) self . W1 = eqx . nn . Linear ( in_features = 2 * embedding_size , out_features = 4 * embedding_size , key = k2 ) self . activation = activation self . W2 = eqx . nn . Linear ( in_features = 4 * embedding_size , out_features = self . out_features , key = k3 ) def __call__ ( self , t : Array ) -> Array : \"\"\"**Arguments:** - `t`: A JAX array with shape `()`. **Returns:** A JAX array with shape `(out_features,)`. \"\"\" assert t . shape == () x = self . projection ( t ) x = self . W1 ( x ) x = self . activation ( x ) return self . W2 ( x )","title":"TimeFeatures"},{"location":"api/nn/unet/","text":"UNet \u00a4 generax.nn.unet.TimeDependentUNet \u00a4 TimeDependentUNet( args, *kwargs) Source code in generax/nn/unet.py class TimeDependentUNet ( eqx . Module ): input_shape : Tuple [ int ] = eqx . field ( static = True ) dim : int = eqx . field ( static = True ) dim_mults : Tuple [ int ] = eqx . field ( static = True ) in_out : Tuple [ Tuple [ int , int ]] = eqx . field ( static = True ) conv_in : WeightNormConv time_features : TimeFeatures down_blocks : Tuple [ Union [ ImageResBlock , AttentionBlock , Downsample ]] middle_blocks : Tuple [ Union [ ImageResBlock , AttentionBlock ]] up_blocks : Tuple [ Union [ ImageResBlock , AttentionBlock , Upsample ]] final_block : ImageResBlock proj_out : WeightNormConv def __init__ ( self , input_shape : Tuple [ int ], dim : int = 16 , dim_mults : Tuple [ int ] = ( 1 , 2 , 4 , 8 ), resnet_block_groups : int = 8 , attn_heads : int = 4 , attn_dim_head : int = 32 , * , key : PRNGKeyArray ): H , W , C = input_shape if H // ( 2 ** dim_mults [ - 1 ]) == 0 : raise ValueError ( f \"Image size { ( H , W ) } is too small for { len ( dim_mults ) } downsamples.\" ) self . input_shape = input_shape self . dim = dim self . dim_mults = dim_mults keys = random . split ( key , 20 ) key_iter = iter ( keys ) self . conv_in = WeightNormConv ( input_shape = input_shape , out_size = self . dim , filter_shape = ( 7 , 7 ), padding = 3 , key = next ( key_iter )) self . time_features = TimeFeatures ( embedding_size = self . dim , out_features = 4 * self . dim , key = next ( key_iter )) time_shape = ( 4 * self . dim ,) def make_resblock ( key , input_shape , dim_out ): return ImageResBlock ( input_shape = input_shape , hidden_size = dim_out , out_size = dim_out , groups = resnet_block_groups , cond_shape = time_shape , key = key ) def make_attention ( key , input_shape , linear = True ): return AttentionBlock ( input_shape = input_shape , heads = attn_heads , dim_head = attn_dim_head , key = key , use_linear_attention = linear ) # Downsampling down_blocks = [] dims = [ self . dim ] + [ self . dim * mult for mult in self . dim_mults ] self . in_out = list ( zip ( dims [: - 1 ], dims [ 1 :])) keys = random . split ( next ( key_iter ), len ( self . in_out )) for i , ( key , ( dim_in , dim_out )) in enumerate ( zip ( keys , self . in_out )): k1 , k2 = random . split ( key , 2 ) down_blocks . append ( make_resblock ( k1 , ( H , W , dim_in ), dim_in )) down_blocks . append ( make_resblock ( k2 , ( H , W , dim_in ), dim_in )) down_blocks . append ( make_attention ( key , ( H , W , dim_in ))) down = Downsample ( input_shape = ( H , W , dim_in ), out_size = dim_out , key = key ) down_blocks . append ( down ) H , W = H // 2 , W // 2 self . down_blocks = down_blocks # Middle middle_blocks = [] middle_blocks . append ( make_resblock ( next ( key_iter ), ( H , W , dim_out ), dim_out )) middle_blocks . append ( make_attention ( next ( key_iter ), ( H , W , dim_out ), linear = False )) middle_blocks . append ( make_resblock ( next ( key_iter ), ( H , W , dim_out ), dim_out )) self . middle_blocks = middle_blocks # Upsampling keys = random . split ( next ( key_iter ), len ( self . in_out )) up_blocks = [] last_dim = dim_out for i , ( key , ( dim_in , dim_out )) in enumerate ( zip ( keys , self . in_out [:: - 1 ])): k1 , k2 = random . split ( key , 2 ) up = Upsample ( input_shape = ( H , W , dim_out ), out_size = dim_in , key = key ) up_blocks . append ( up ) H , W = H * 2 , W * 2 # Skip connections contribute a dim_in up_blocks . append ( make_resblock ( k1 , ( H , W , dim_in + dim_in ), dim_in )) up_blocks . append ( make_resblock ( k2 , ( H , W , dim_in + dim_in ), dim_in )) up_blocks . append ( make_attention ( key , ( H , W , dim_in ))) self . up_blocks = up_blocks # Final self . final_block = make_resblock ( next ( key_iter ), ( H , W , dim_in + dim_in ), dim_in ) self . proj_out = WeightNormConv ( input_shape = ( H , W , dim_in ), out_size = C , filter_shape = ( 1 , 1 ), key = next ( key_iter )) def __call__ ( self , t : Array , x : Array , y : Array = None ) -> Array : assert t . shape == () assert x . shape == self . input_shape # Time embedding time_emb = self . time_features ( t ) hs = [] # Initial convolution h = self . conv_in ( x ) hs . append ( h ) # Downsampling block_iter = iter ( self . down_blocks ) for i , ( dim_in , dim_out ) in enumerate ( self . in_out ): # Resnet block h = next ( block_iter )( h , time_emb ) hs . append ( h ) # Resnet block + attention block h = next ( block_iter )( h , time_emb ) h = next ( block_iter )( h ) hs . append ( h ) # Downsample h = next ( block_iter )( h ) # Middle res_block1 , attn_block , res_block2 = self . middle_blocks h = res_block1 ( h ) h = attn_block ( h ) h = res_block2 ( h ) # Upsampling block_iter = iter ( self . up_blocks ) for i , ( dim_in , dim_out ) in enumerate ( self . in_out [:: - 1 ]): # Upsample h = next ( block_iter )( h ) # Resnet block h = jnp . concatenate ([ h , hs . pop ()], axis =- 1 ) h = next ( block_iter )( h , time_emb ) # Resnet block h = jnp . concatenate ([ h , hs . pop ()], axis =- 1 ) h = next ( block_iter )( h , time_emb ) # Attention block h = next ( block_iter )( h ) # Final h_in = hs . pop () h = jnp . concatenate ([ h , h_in ], axis =- 1 ) h = self . final_block ( h , time_emb ) h = self . proj_out ( h ) assert len ( hs ) == 0 return h __init__ ( self , input_shape : Tuple [ int ], dim : int = 16 , dim_mults : Tuple [ int ] = ( 1 , 2 , 4 , 8 ), resnet_block_groups : int = 8 , attn_heads : int = 4 , attn_dim_head : int = 32 , * , key : PRNGKeyArray ) \u00a4 Source code in generax/nn/unet.py def __init__ ( self , input_shape : Tuple [ int ], dim : int = 16 , dim_mults : Tuple [ int ] = ( 1 , 2 , 4 , 8 ), resnet_block_groups : int = 8 , attn_heads : int = 4 , attn_dim_head : int = 32 , * , key : PRNGKeyArray ): H , W , C = input_shape if H // ( 2 ** dim_mults [ - 1 ]) == 0 : raise ValueError ( f \"Image size { ( H , W ) } is too small for { len ( dim_mults ) } downsamples.\" ) self . input_shape = input_shape self . dim = dim self . dim_mults = dim_mults keys = random . split ( key , 20 ) key_iter = iter ( keys ) self . conv_in = WeightNormConv ( input_shape = input_shape , out_size = self . dim , filter_shape = ( 7 , 7 ), padding = 3 , key = next ( key_iter )) self . time_features = TimeFeatures ( embedding_size = self . dim , out_features = 4 * self . dim , key = next ( key_iter )) time_shape = ( 4 * self . dim ,) def make_resblock ( key , input_shape , dim_out ): return ImageResBlock ( input_shape = input_shape , hidden_size = dim_out , out_size = dim_out , groups = resnet_block_groups , cond_shape = time_shape , key = key ) def make_attention ( key , input_shape , linear = True ): return AttentionBlock ( input_shape = input_shape , heads = attn_heads , dim_head = attn_dim_head , key = key , use_linear_attention = linear ) # Downsampling down_blocks = [] dims = [ self . dim ] + [ self . dim * mult for mult in self . dim_mults ] self . in_out = list ( zip ( dims [: - 1 ], dims [ 1 :])) keys = random . split ( next ( key_iter ), len ( self . in_out )) for i , ( key , ( dim_in , dim_out )) in enumerate ( zip ( keys , self . in_out )): k1 , k2 = random . split ( key , 2 ) down_blocks . append ( make_resblock ( k1 , ( H , W , dim_in ), dim_in )) down_blocks . append ( make_resblock ( k2 , ( H , W , dim_in ), dim_in )) down_blocks . append ( make_attention ( key , ( H , W , dim_in ))) down = Downsample ( input_shape = ( H , W , dim_in ), out_size = dim_out , key = key ) down_blocks . append ( down ) H , W = H // 2 , W // 2 self . down_blocks = down_blocks # Middle middle_blocks = [] middle_blocks . append ( make_resblock ( next ( key_iter ), ( H , W , dim_out ), dim_out )) middle_blocks . append ( make_attention ( next ( key_iter ), ( H , W , dim_out ), linear = False )) middle_blocks . append ( make_resblock ( next ( key_iter ), ( H , W , dim_out ), dim_out )) self . middle_blocks = middle_blocks # Upsampling keys = random . split ( next ( key_iter ), len ( self . in_out )) up_blocks = [] last_dim = dim_out for i , ( key , ( dim_in , dim_out )) in enumerate ( zip ( keys , self . in_out [:: - 1 ])): k1 , k2 = random . split ( key , 2 ) up = Upsample ( input_shape = ( H , W , dim_out ), out_size = dim_in , key = key ) up_blocks . append ( up ) H , W = H * 2 , W * 2 # Skip connections contribute a dim_in up_blocks . append ( make_resblock ( k1 , ( H , W , dim_in + dim_in ), dim_in )) up_blocks . append ( make_resblock ( k2 , ( H , W , dim_in + dim_in ), dim_in )) up_blocks . append ( make_attention ( key , ( H , W , dim_in ))) self . up_blocks = up_blocks # Final self . final_block = make_resblock ( next ( key_iter ), ( H , W , dim_in + dim_in ), dim_in ) self . proj_out = WeightNormConv ( input_shape = ( H , W , dim_in ), out_size = C , filter_shape = ( 1 , 1 ), key = next ( key_iter )) __call__ ( self , t : Array , x : Array , y : Array = None ) -> Array \u00a4 Call self as a function. Source code in generax/nn/unet.py def __call__ ( self , t : Array , x : Array , y : Array = None ) -> Array : assert t . shape == () assert x . shape == self . input_shape # Time embedding time_emb = self . time_features ( t ) hs = [] # Initial convolution h = self . conv_in ( x ) hs . append ( h ) # Downsampling block_iter = iter ( self . down_blocks ) for i , ( dim_in , dim_out ) in enumerate ( self . in_out ): # Resnet block h = next ( block_iter )( h , time_emb ) hs . append ( h ) # Resnet block + attention block h = next ( block_iter )( h , time_emb ) h = next ( block_iter )( h ) hs . append ( h ) # Downsample h = next ( block_iter )( h ) # Middle res_block1 , attn_block , res_block2 = self . middle_blocks h = res_block1 ( h ) h = attn_block ( h ) h = res_block2 ( h ) # Upsampling block_iter = iter ( self . up_blocks ) for i , ( dim_in , dim_out ) in enumerate ( self . in_out [:: - 1 ]): # Upsample h = next ( block_iter )( h ) # Resnet block h = jnp . concatenate ([ h , hs . pop ()], axis =- 1 ) h = next ( block_iter )( h , time_emb ) # Resnet block h = jnp . concatenate ([ h , hs . pop ()], axis =- 1 ) h = next ( block_iter )( h , time_emb ) # Attention block h = next ( block_iter )( h ) # Final h_in = hs . pop () h = jnp . concatenate ([ h , h_in ], axis =- 1 ) h = self . final_block ( h , time_emb ) h = self . proj_out ( h ) assert len ( hs ) == 0 return h","title":"UNet"},{"location":"api/nn/unet/#unet","text":"","title":"UNet"},{"location":"api/nn/unet/#generax.nn.unet.TimeDependentUNet","text":"TimeDependentUNet( args, *kwargs) Source code in generax/nn/unet.py class TimeDependentUNet ( eqx . Module ): input_shape : Tuple [ int ] = eqx . field ( static = True ) dim : int = eqx . field ( static = True ) dim_mults : Tuple [ int ] = eqx . field ( static = True ) in_out : Tuple [ Tuple [ int , int ]] = eqx . field ( static = True ) conv_in : WeightNormConv time_features : TimeFeatures down_blocks : Tuple [ Union [ ImageResBlock , AttentionBlock , Downsample ]] middle_blocks : Tuple [ Union [ ImageResBlock , AttentionBlock ]] up_blocks : Tuple [ Union [ ImageResBlock , AttentionBlock , Upsample ]] final_block : ImageResBlock proj_out : WeightNormConv def __init__ ( self , input_shape : Tuple [ int ], dim : int = 16 , dim_mults : Tuple [ int ] = ( 1 , 2 , 4 , 8 ), resnet_block_groups : int = 8 , attn_heads : int = 4 , attn_dim_head : int = 32 , * , key : PRNGKeyArray ): H , W , C = input_shape if H // ( 2 ** dim_mults [ - 1 ]) == 0 : raise ValueError ( f \"Image size { ( H , W ) } is too small for { len ( dim_mults ) } downsamples.\" ) self . input_shape = input_shape self . dim = dim self . dim_mults = dim_mults keys = random . split ( key , 20 ) key_iter = iter ( keys ) self . conv_in = WeightNormConv ( input_shape = input_shape , out_size = self . dim , filter_shape = ( 7 , 7 ), padding = 3 , key = next ( key_iter )) self . time_features = TimeFeatures ( embedding_size = self . dim , out_features = 4 * self . dim , key = next ( key_iter )) time_shape = ( 4 * self . dim ,) def make_resblock ( key , input_shape , dim_out ): return ImageResBlock ( input_shape = input_shape , hidden_size = dim_out , out_size = dim_out , groups = resnet_block_groups , cond_shape = time_shape , key = key ) def make_attention ( key , input_shape , linear = True ): return AttentionBlock ( input_shape = input_shape , heads = attn_heads , dim_head = attn_dim_head , key = key , use_linear_attention = linear ) # Downsampling down_blocks = [] dims = [ self . dim ] + [ self . dim * mult for mult in self . dim_mults ] self . in_out = list ( zip ( dims [: - 1 ], dims [ 1 :])) keys = random . split ( next ( key_iter ), len ( self . in_out )) for i , ( key , ( dim_in , dim_out )) in enumerate ( zip ( keys , self . in_out )): k1 , k2 = random . split ( key , 2 ) down_blocks . append ( make_resblock ( k1 , ( H , W , dim_in ), dim_in )) down_blocks . append ( make_resblock ( k2 , ( H , W , dim_in ), dim_in )) down_blocks . append ( make_attention ( key , ( H , W , dim_in ))) down = Downsample ( input_shape = ( H , W , dim_in ), out_size = dim_out , key = key ) down_blocks . append ( down ) H , W = H // 2 , W // 2 self . down_blocks = down_blocks # Middle middle_blocks = [] middle_blocks . append ( make_resblock ( next ( key_iter ), ( H , W , dim_out ), dim_out )) middle_blocks . append ( make_attention ( next ( key_iter ), ( H , W , dim_out ), linear = False )) middle_blocks . append ( make_resblock ( next ( key_iter ), ( H , W , dim_out ), dim_out )) self . middle_blocks = middle_blocks # Upsampling keys = random . split ( next ( key_iter ), len ( self . in_out )) up_blocks = [] last_dim = dim_out for i , ( key , ( dim_in , dim_out )) in enumerate ( zip ( keys , self . in_out [:: - 1 ])): k1 , k2 = random . split ( key , 2 ) up = Upsample ( input_shape = ( H , W , dim_out ), out_size = dim_in , key = key ) up_blocks . append ( up ) H , W = H * 2 , W * 2 # Skip connections contribute a dim_in up_blocks . append ( make_resblock ( k1 , ( H , W , dim_in + dim_in ), dim_in )) up_blocks . append ( make_resblock ( k2 , ( H , W , dim_in + dim_in ), dim_in )) up_blocks . append ( make_attention ( key , ( H , W , dim_in ))) self . up_blocks = up_blocks # Final self . final_block = make_resblock ( next ( key_iter ), ( H , W , dim_in + dim_in ), dim_in ) self . proj_out = WeightNormConv ( input_shape = ( H , W , dim_in ), out_size = C , filter_shape = ( 1 , 1 ), key = next ( key_iter )) def __call__ ( self , t : Array , x : Array , y : Array = None ) -> Array : assert t . shape == () assert x . shape == self . input_shape # Time embedding time_emb = self . time_features ( t ) hs = [] # Initial convolution h = self . conv_in ( x ) hs . append ( h ) # Downsampling block_iter = iter ( self . down_blocks ) for i , ( dim_in , dim_out ) in enumerate ( self . in_out ): # Resnet block h = next ( block_iter )( h , time_emb ) hs . append ( h ) # Resnet block + attention block h = next ( block_iter )( h , time_emb ) h = next ( block_iter )( h ) hs . append ( h ) # Downsample h = next ( block_iter )( h ) # Middle res_block1 , attn_block , res_block2 = self . middle_blocks h = res_block1 ( h ) h = attn_block ( h ) h = res_block2 ( h ) # Upsampling block_iter = iter ( self . up_blocks ) for i , ( dim_in , dim_out ) in enumerate ( self . in_out [:: - 1 ]): # Upsample h = next ( block_iter )( h ) # Resnet block h = jnp . concatenate ([ h , hs . pop ()], axis =- 1 ) h = next ( block_iter )( h , time_emb ) # Resnet block h = jnp . concatenate ([ h , hs . pop ()], axis =- 1 ) h = next ( block_iter )( h , time_emb ) # Attention block h = next ( block_iter )( h ) # Final h_in = hs . pop () h = jnp . concatenate ([ h , h_in ], axis =- 1 ) h = self . final_block ( h , time_emb ) h = self . proj_out ( h ) assert len ( hs ) == 0 return h","title":"TimeDependentUNet"},{"location":"notebooks/cnf/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); import jax import jax.numpy as jnp from jax import random from functools import partial from generax.trainer import Trainer import matplotlib.pyplot as plt import equinox as eqx from jaxtyping import Array , PRNGKeyArray import generax as gx from generax.distributions.flow_models import TimeDependentNormalizingFlow , ContinuousNormalizingFlow Here we'll show how to train continuous normalizing flows using flow matching \u00a4 class EmpiricalDistribution ( gx . ProbabilityDistribution ): data : Array def __init__ ( self , data ): self . data = data x_shape = data . shape [ 1 :] super () . __init__ ( input_shape = x_shape ) def sample_and_log_prob ( self ): assert 0 , \"Can't compute\" def log_prob ( self ): assert 0 , \"Can't compute\" def sample ( self , key ): return random . choice ( key , self . data , shape = ( 1 ,))[ 0 ] def train_iterator ( self , key , batch_size ): total_choices = jnp . arange ( self . data . shape [ 0 ]) while True : key , _ = random . split ( key , 2 ) idx = random . choice ( key , total_choices , shape = ( batch_size ,), replace = True ) yield dict ( x = self . data [ idx ]) from sklearn.datasets import make_swiss_roll data , y = make_swiss_roll ( n_samples = 100000 , noise = 0.5 ) data = data [:, [ 0 , 2 ]] data = data - data . mean ( axis = 0 ) data = data / data . std ( axis = 0 ) p1 = EmpiricalDistribution ( data ) key = random . PRNGKey ( 0 ) keys = random . split ( key , 1000 ) samples = eqx . filter_vmap ( p1 . sample )( keys ) plt . scatter ( * samples . T ) <matplotlib.collections.PathCollection at 0x7f936d78d240> A probability path is a time dependent probability distribution \\(p_t\\) from \\(t=0\\) to \\(t=1\\) . We want \\(p_0 = N(0,I)\\) and \\(p_1 = p_{\\text{data}}\\) . We can construct this path as the expectation of a conditional probability path: $$ p_t(x_t) = \\int p_1(x_1)p_t(x_t|x_1)dx_1 $$ One choice for the conditional distribution is \\(p_t(x_t|x_1) = N(x_t|tx_1,(1-t)^2I)\\) , which is a linear interpolation between \\(x_0=0\\) and \\(x_1\\) (conditional optimal transport). x_shape = p1 . input_shape transform = gx . ConditionalOptionalTransport ( input_shape = x_shape , key = key ) cond_ppath = TimeDependentNormalizingFlow ( transform = transform , prior = gx . Gaussian ( input_shape = x_shape )) p0 = gx . Gaussian ( input_shape = x_shape ) def sample_xt ( t , key ): k1 , k2 = random . split ( key , 2 ) x1 = p1 . sample ( k1 ) x0 = p0 . sample ( k2 ) xt = cond_ppath . to_data_space ( t , x0 , x1 ) return xt ts = jnp . linspace ( 0 , 1 , 6 ) keys = random . split ( key , 1000 ) xt_samples = jax . vmap ( jax . vmap ( sample_xt , in_axes = ( 0 , None )), in_axes = ( None , 0 ))( ts , keys ) n_rows , n_cols = 1 , ts . shape [ 0 ] size = 4 fig , axes = plt . subplots ( n_rows , n_cols , figsize = ( n_cols * size , n_rows * size )) for i in range ( n_cols ): axes [ i ] . scatter ( * xt_samples [:, i ] . T ) axes [ i ] . set_title ( f \"t= { ts [ i ] : .2f } \" ) Without using any parameters, we constructed a path between a Gaussian and our data. Next, we'll learn a continuous normalizing flow that will learn this probability path. # Construct the neural network that learn the score net = gx . TimeDependentResNet ( input_shape = x_shape , working_size = 16 , hidden_size = 32 , out_size = x_shape [ - 1 ], n_blocks = 5 , embedding_size = 16 , out_features = 32 , key = key ) flow = ContinuousNormalizingFlow ( input_shape = x_shape , net = net , key = key , controller_atol = 1e-5 , controller_rtol = 1e-5 ) def loss ( flow , data , key ): def unbatched_loss ( data , key ): k1 , k2 = random . split ( key , 2 ) # Sample x1 = data [ 'x' ] x0 = cond_ppath . prior . sample ( k1 ) t = random . uniform ( k2 ) # Compute f_t(x_0; x_1) def ft ( t ): return cond_ppath . to_data_space ( t , x0 , x1 ) xt , ut = jax . jvp ( ft , ( t ,), ( jnp . ones_like ( t ),)) # Compute the parametric vector field vt = flow . net ( t , xt ) # Compute the loss return jnp . sum (( ut - vt ) ** 2 ) keys = random . split ( key , data [ 'x' ] . shape [ 0 ]) objective = jax . vmap ( unbatched_loss )( data , keys ) . mean () aux = dict ( objective = objective ) return objective , aux # Create the optimizer import optax schedule = optax . warmup_cosine_decay_schedule ( init_value = 0.0 , peak_value = 1.0 , warmup_steps = 1000 , decay_steps = 3e5 , end_value = 0.1 , exponent = 1.0 ) chain = [] chain . append ( optax . clip_by_global_norm ( 15.0 )) chain . append ( optax . adamw ( 1e-3 )) chain . append ( optax . scale_by_schedule ( schedule )) optimizer = optax . chain ( * chain ) # Create the trainer and optimize trainer = Trainer ( checkpoint_path = 'tmp/flow/flow_matching' ) flow = trainer . train ( model = flow , objective = loss , evaluate_model = lambda x : x , optimizer = optimizer , num_steps = 30000 , double_batch = 1000 , data_iterator = p1 . train_iterator ( key , batch_size = 128 ), checkpoint_every = 5000 , test_every =- 1 , retrain = True ) loss: 2.9090: 20%|\u2588\u2588 | 6000/30000 [00:22<01:10, 340.63it/s] Checkpointed model loss: 2.8846: 37%|\u2588\u2588\u2588\u258b | 11000/30000 [00:38<01:01, 310.04it/s] Checkpointed model loss: 2.8767: 53%|\u2588\u2588\u2588\u2588\u2588\u258e | 16000/30000 [00:51<00:35, 397.60it/s] Checkpointed model loss: 2.8872: 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 21000/30000 [01:01<00:18, 482.02it/s] Checkpointed model loss: 2.8765: 87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 26000/30000 [01:11<00:07, 502.76it/s] Checkpointed model loss: 2.8664: 0%| | 30/30000 [01:18<21:54:47, 2.63s/it] Checkpointed model ts = jnp . linspace ( 0 , 1 , 6 ) def ode_solve ( x0 ): z , log_det = flow . transform . neural_ode ( x0 , inverse = True , log_likelihood = True , save_at = ts ) return z , log_det n_samples = 10000 keys = random . split ( key , n_samples ) x0 , log_p0s = eqx . filter_vmap ( flow . prior . sample_and_log_prob )( keys ) xts , log_dets = jax . vmap ( ode_solve )( x0 ) log_pxs = log_p0s [:, None ] - log_dets n_rows , n_cols = 1 , ts . shape [ 0 ] size = 4 fig , axes = plt . subplots ( n_rows , n_cols , figsize = ( n_cols * size , n_rows * size )) for i in range ( n_cols ): axes [ i ] . scatter ( * xts [:, i ] . T , c = jnp . exp ( log_pxs [:, i ]), alpha = 0.5 , s = 10 ) axes [ i ] . set_title ( f \"t= { ts [ i ] : .2f } \" ) axes [ i ] . set_aspect ( 'equal' , 'box' )","title":"Flow matching tutorial"},{"location":"notebooks/cnf/#here-well-show-how-to-train-continuous-normalizing-flows-using-flow-matching","text":"class EmpiricalDistribution ( gx . ProbabilityDistribution ): data : Array def __init__ ( self , data ): self . data = data x_shape = data . shape [ 1 :] super () . __init__ ( input_shape = x_shape ) def sample_and_log_prob ( self ): assert 0 , \"Can't compute\" def log_prob ( self ): assert 0 , \"Can't compute\" def sample ( self , key ): return random . choice ( key , self . data , shape = ( 1 ,))[ 0 ] def train_iterator ( self , key , batch_size ): total_choices = jnp . arange ( self . data . shape [ 0 ]) while True : key , _ = random . split ( key , 2 ) idx = random . choice ( key , total_choices , shape = ( batch_size ,), replace = True ) yield dict ( x = self . data [ idx ]) from sklearn.datasets import make_swiss_roll data , y = make_swiss_roll ( n_samples = 100000 , noise = 0.5 ) data = data [:, [ 0 , 2 ]] data = data - data . mean ( axis = 0 ) data = data / data . std ( axis = 0 ) p1 = EmpiricalDistribution ( data ) key = random . PRNGKey ( 0 ) keys = random . split ( key , 1000 ) samples = eqx . filter_vmap ( p1 . sample )( keys ) plt . scatter ( * samples . T ) <matplotlib.collections.PathCollection at 0x7f936d78d240> A probability path is a time dependent probability distribution \\(p_t\\) from \\(t=0\\) to \\(t=1\\) . We want \\(p_0 = N(0,I)\\) and \\(p_1 = p_{\\text{data}}\\) . We can construct this path as the expectation of a conditional probability path: $$ p_t(x_t) = \\int p_1(x_1)p_t(x_t|x_1)dx_1 $$ One choice for the conditional distribution is \\(p_t(x_t|x_1) = N(x_t|tx_1,(1-t)^2I)\\) , which is a linear interpolation between \\(x_0=0\\) and \\(x_1\\) (conditional optimal transport). x_shape = p1 . input_shape transform = gx . ConditionalOptionalTransport ( input_shape = x_shape , key = key ) cond_ppath = TimeDependentNormalizingFlow ( transform = transform , prior = gx . Gaussian ( input_shape = x_shape )) p0 = gx . Gaussian ( input_shape = x_shape ) def sample_xt ( t , key ): k1 , k2 = random . split ( key , 2 ) x1 = p1 . sample ( k1 ) x0 = p0 . sample ( k2 ) xt = cond_ppath . to_data_space ( t , x0 , x1 ) return xt ts = jnp . linspace ( 0 , 1 , 6 ) keys = random . split ( key , 1000 ) xt_samples = jax . vmap ( jax . vmap ( sample_xt , in_axes = ( 0 , None )), in_axes = ( None , 0 ))( ts , keys ) n_rows , n_cols = 1 , ts . shape [ 0 ] size = 4 fig , axes = plt . subplots ( n_rows , n_cols , figsize = ( n_cols * size , n_rows * size )) for i in range ( n_cols ): axes [ i ] . scatter ( * xt_samples [:, i ] . T ) axes [ i ] . set_title ( f \"t= { ts [ i ] : .2f } \" ) Without using any parameters, we constructed a path between a Gaussian and our data. Next, we'll learn a continuous normalizing flow that will learn this probability path. # Construct the neural network that learn the score net = gx . TimeDependentResNet ( input_shape = x_shape , working_size = 16 , hidden_size = 32 , out_size = x_shape [ - 1 ], n_blocks = 5 , embedding_size = 16 , out_features = 32 , key = key ) flow = ContinuousNormalizingFlow ( input_shape = x_shape , net = net , key = key , controller_atol = 1e-5 , controller_rtol = 1e-5 ) def loss ( flow , data , key ): def unbatched_loss ( data , key ): k1 , k2 = random . split ( key , 2 ) # Sample x1 = data [ 'x' ] x0 = cond_ppath . prior . sample ( k1 ) t = random . uniform ( k2 ) # Compute f_t(x_0; x_1) def ft ( t ): return cond_ppath . to_data_space ( t , x0 , x1 ) xt , ut = jax . jvp ( ft , ( t ,), ( jnp . ones_like ( t ),)) # Compute the parametric vector field vt = flow . net ( t , xt ) # Compute the loss return jnp . sum (( ut - vt ) ** 2 ) keys = random . split ( key , data [ 'x' ] . shape [ 0 ]) objective = jax . vmap ( unbatched_loss )( data , keys ) . mean () aux = dict ( objective = objective ) return objective , aux # Create the optimizer import optax schedule = optax . warmup_cosine_decay_schedule ( init_value = 0.0 , peak_value = 1.0 , warmup_steps = 1000 , decay_steps = 3e5 , end_value = 0.1 , exponent = 1.0 ) chain = [] chain . append ( optax . clip_by_global_norm ( 15.0 )) chain . append ( optax . adamw ( 1e-3 )) chain . append ( optax . scale_by_schedule ( schedule )) optimizer = optax . chain ( * chain ) # Create the trainer and optimize trainer = Trainer ( checkpoint_path = 'tmp/flow/flow_matching' ) flow = trainer . train ( model = flow , objective = loss , evaluate_model = lambda x : x , optimizer = optimizer , num_steps = 30000 , double_batch = 1000 , data_iterator = p1 . train_iterator ( key , batch_size = 128 ), checkpoint_every = 5000 , test_every =- 1 , retrain = True ) loss: 2.9090: 20%|\u2588\u2588 | 6000/30000 [00:22<01:10, 340.63it/s] Checkpointed model loss: 2.8846: 37%|\u2588\u2588\u2588\u258b | 11000/30000 [00:38<01:01, 310.04it/s] Checkpointed model loss: 2.8767: 53%|\u2588\u2588\u2588\u2588\u2588\u258e | 16000/30000 [00:51<00:35, 397.60it/s] Checkpointed model loss: 2.8872: 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 21000/30000 [01:01<00:18, 482.02it/s] Checkpointed model loss: 2.8765: 87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 26000/30000 [01:11<00:07, 502.76it/s] Checkpointed model loss: 2.8664: 0%| | 30/30000 [01:18<21:54:47, 2.63s/it] Checkpointed model ts = jnp . linspace ( 0 , 1 , 6 ) def ode_solve ( x0 ): z , log_det = flow . transform . neural_ode ( x0 , inverse = True , log_likelihood = True , save_at = ts ) return z , log_det n_samples = 10000 keys = random . split ( key , n_samples ) x0 , log_p0s = eqx . filter_vmap ( flow . prior . sample_and_log_prob )( keys ) xts , log_dets = jax . vmap ( ode_solve )( x0 ) log_pxs = log_p0s [:, None ] - log_dets n_rows , n_cols = 1 , ts . shape [ 0 ] size = 4 fig , axes = plt . subplots ( n_rows , n_cols , figsize = ( n_cols * size , n_rows * size )) for i in range ( n_cols ): axes [ i ] . scatter ( * xts [:, i ] . T , c = jnp . exp ( log_pxs [:, i ]), alpha = 0.5 , s = 10 ) axes [ i ] . set_title ( f \"t= { ts [ i ] : .2f } \" ) axes [ i ] . set_aspect ( 'equal' , 'box' )","title":"Here we'll show how to train continuous normalizing flows using flow matching"},{"location":"notebooks/score_and_flow_matching/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); import jax import jax.numpy as jnp from jax import random from functools import partial from generax.trainer import Trainer import matplotlib.pyplot as plt import equinox as eqx from jaxtyping import Array , PRNGKeyArray import generax as gx from generax.distributions.flow_models import TimeDependentNormalizingFlow , ContinuousNormalizingFlow Here we'll show how to train continuous normalizing flows using flow matching \u00a4 class EmpiricalDistribution ( gx . ProbabilityDistribution ): data : Array def __init__ ( self , data ): self . data = data x_shape = data . shape [ 1 :] super () . __init__ ( input_shape = x_shape ) def sample_and_log_prob ( self ): assert 0 , \"Can't compute\" def log_prob ( self ): assert 0 , \"Can't compute\" def sample ( self , key ): return random . choice ( key , self . data , shape = ( 1 ,))[ 0 ] def train_iterator ( self , key , batch_size ): total_choices = jnp . arange ( self . data . shape [ 0 ]) while True : key , _ = random . split ( key , 2 ) idx = random . choice ( key , total_choices , shape = ( batch_size ,), replace = True ) yield dict ( x = self . data [ idx ]) from sklearn.datasets import make_swiss_roll data , y = make_swiss_roll ( n_samples = 100000 , noise = 0.5 ) data = data [:, [ 0 , 2 ]] data = data - data . mean ( axis = 0 ) data = data / data . std ( axis = 0 ) p1 = EmpiricalDistribution ( data ) key = random . PRNGKey ( 0 ) keys = random . split ( key , 1000 ) samples = eqx . filter_vmap ( p1 . sample )( keys ) plt . scatter ( * samples . T ) <matplotlib.collections.PathCollection at 0x7fee85b91540> A probability path is a time dependent probability distribution \\(p_t\\) from \\(t=0\\) to \\(t=1\\) . We want \\(p_0 = N(0,I)\\) and \\(p_1 = p_{\\text{data}}\\) . We can construct this path as the expectation of a conditional probability path: $$ p_t(x_t) = \\int p_1(x_1)p_t(x_t|x_1)dx_1 $$ One choice for the conditional distribution is \\(p_t(x_t|x_1) = N(x_t|tx_1,(1-t)^2I)\\) , which is a linear interpolation between \\(x_0=0\\) and \\(x_1\\) (conditional optimal transport). x_shape = p1 . input_shape transform = gx . ConditionalOptionalTransport ( input_shape = x_shape , key = key ) cond_ppath = TimeDependentNormalizingFlow ( transform = transform , prior = gx . Gaussian ( input_shape = x_shape )) p0 = gx . Gaussian ( input_shape = x_shape ) def sample_xt ( t , key ): k1 , k2 = random . split ( key , 2 ) x1 = p1 . sample ( k1 ) x0 = p0 . sample ( k2 ) xt = cond_ppath . to_data_space ( t , x0 , x1 ) return xt ts = jnp . linspace ( 0 , 1 , 6 ) keys = random . split ( key , 1000 ) xt_samples = jax . vmap ( jax . vmap ( sample_xt , in_axes = ( 0 , None )), in_axes = ( None , 0 ))( ts , keys ) n_rows , n_cols = 1 , ts . shape [ 0 ] size = 4 fig , axes = plt . subplots ( n_rows , n_cols , figsize = ( n_cols * size , n_rows * size )) for i in range ( n_cols ): axes [ i ] . scatter ( * xt_samples [:, i ] . T ) axes [ i ] . set_title ( f \"t= { ts [ i ] : .2f } \" ) Without using any parameters, we constructed a path between a Gaussian and our data. Next, we'll learn a continuous normalizing flow that will learn this probability path. # Construct the neural network that learn the score net = gx . TimeDependentResNet ( input_shape = x_shape , working_size = 16 , hidden_size = 32 , out_size = 2 * x_shape [ - 1 ], n_blocks = 5 , embedding_size = 16 , out_features = 32 , key = key ) from generax.distributions.coupling import OTTCoupling def loss ( net , data , key ): k1 , k2 = random . split ( key , 2 ) # Sample x1 = data [ 'x' ] keys = random . split ( k1 , x1 . shape [ 0 ]) x0 = eqx . filter_vmap ( cond_ppath . prior . sample )( keys ) t = random . uniform ( k2 , shape = ( x1 . shape [ 0 ],)) # Resample from the coupling coupling = OTTCoupling ( x0 , x1 ) x0 = coupling . sample_x0_given_x1 ( k1 ) # Compute f_t(x_0; x_1) def ft ( t ): return eqx . filter_vmap ( cond_ppath . to_data_space )( t , x0 , x1 ) xt , ut = jax . jvp ( ft , ( t ,), ( jnp . ones_like ( t ),)) # Compute the score def log_prob ( xt , x1 , t ): return cond_ppath . log_prob ( t , xt , x1 ) grad_logptx = eqx . filter_vmap ( eqx . filter_grad ( log_prob ))( xt , x1 , t ) # Compute the parametric vector field net_out = eqx . filter_vmap ( net )( t , xt ) st , vt = jnp . split ( net_out , 2 , axis =- 1 ) # Compute the loss objective = jnp . sum (( ut - vt ) ** 2 ) . mean () + 0.01 * jnp . sum (( grad_logptx - st ) ** 2 ) . mean () aux = dict ( objective = objective ) return objective , aux # Create the optimizer import optax schedule = optax . warmup_cosine_decay_schedule ( init_value = 0.0 , peak_value = 1.0 , warmup_steps = 1000 , decay_steps = 3e5 , end_value = 0.1 , exponent = 1.0 ) chain = [] chain . append ( optax . clip_by_global_norm ( 15.0 )) chain . append ( optax . adamw ( 1e-3 )) chain . append ( optax . scale_by_schedule ( schedule )) optimizer = optax . chain ( * chain ) # Create the trainer and optimize trainer = Trainer ( checkpoint_path = 'tmp/flow/flow_and_score_matching' ) net = trainer . train ( model = net , objective = loss , evaluate_model = lambda x : x , optimizer = optimizer , num_steps = 30000 , double_batch = 1000 , data_iterator = p1 . train_iterator ( key , batch_size = 128 ), checkpoint_every = 5000 , test_every =- 1 , retrain = True ) 0%| | 0/30000 [00:00<?, ?it/s]loss: 354807.7500: 20%|\u2588\u2588 | 6000/30000 [01:12<04:41, 85.29it/s] Checkpointed model loss: 1640012.5000: 37%|\u2588\u2588\u2588\u258b | 11009/30000 [02:11<03:39, 86.41it/s] Checkpointed model loss: 339471.9375: 53%|\u2588\u2588\u2588\u2588\u2588\u258e | 16009/30000 [03:08<02:37, 89.08it/s] Checkpointed model loss: 3653279.5000: 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 21009/30000 [04:07<01:44, 86.13it/s] Checkpointed model loss: 10576435.0000: 87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 26009/30000 [05:02<00:45, 87.48it/s] Checkpointed model loss: 11467755.0000: 0%| | 30/30000 [05:49<96:52:20, 11.64s/it] Checkpointed model class Split ( eqx . Module ): net : eqx . Module keepdims : list [ int ] input_shape : tuple [ int ] def __init__ ( self , net , keepdims = None ): self . net = net self . keepdims = jnp . array ( keepdims ) self . input_shape = self . net . input_shape def __call__ ( self , t , x , * args , ** kwargs ): net_out = self . net ( t , x ) return net_out [ self . keepdims ] score = Split ( net , keepdims = [ 0 , 1 ]) vf = Split ( net , keepdims = [ 2 , 3 ]) ts = jnp . linspace ( 0 , 1 , 6 ) node = gx . NeuralODE ( vf = vf ) p0 = gx . Gaussian ( input_shape = x_shape ) def ode_solve ( x0 ): z , log_det = node ( x0 , inverse = True , log_likelihood = True , save_at = ts ) return z , log_det n_samples = 10000 keys = random . split ( key , n_samples ) x0 , log_p0s = eqx . filter_vmap ( p0 . sample_and_log_prob )( keys ) xts , log_dets = jax . vmap ( ode_solve )( x0 ) log_pxs = log_p0s [:, None ] - log_dets n_rows , n_cols = 1 , ts . shape [ 0 ] size = 4 fig , axes = plt . subplots ( n_rows , n_cols , figsize = ( n_cols * size , n_rows * size )) for i in range ( n_cols ): axes [ i ] . scatter ( * xts [:, i ] . T , c = jnp . exp ( log_pxs [:, i ]), alpha = 0.5 , s = 10 ) axes [ i ] . set_title ( f \"t= { ts [ i ] : .2f } \" ) axes [ i ] . set_aspect ( 'equal' , 'box' ) s = eqx . filter_vmap ( score , in_axes = ( None , 0 ))( jnp . array ( 1.0 ), xts [:, - 1 ]) plt . quiver ( * xts [:, - 1 ] . T , * s . T , alpha = 0.5 ) <matplotlib.quiver.Quiver at 0x7fee5d2325f0>","title":"Score and flow matching"},{"location":"notebooks/score_and_flow_matching/#here-well-show-how-to-train-continuous-normalizing-flows-using-flow-matching","text":"class EmpiricalDistribution ( gx . ProbabilityDistribution ): data : Array def __init__ ( self , data ): self . data = data x_shape = data . shape [ 1 :] super () . __init__ ( input_shape = x_shape ) def sample_and_log_prob ( self ): assert 0 , \"Can't compute\" def log_prob ( self ): assert 0 , \"Can't compute\" def sample ( self , key ): return random . choice ( key , self . data , shape = ( 1 ,))[ 0 ] def train_iterator ( self , key , batch_size ): total_choices = jnp . arange ( self . data . shape [ 0 ]) while True : key , _ = random . split ( key , 2 ) idx = random . choice ( key , total_choices , shape = ( batch_size ,), replace = True ) yield dict ( x = self . data [ idx ]) from sklearn.datasets import make_swiss_roll data , y = make_swiss_roll ( n_samples = 100000 , noise = 0.5 ) data = data [:, [ 0 , 2 ]] data = data - data . mean ( axis = 0 ) data = data / data . std ( axis = 0 ) p1 = EmpiricalDistribution ( data ) key = random . PRNGKey ( 0 ) keys = random . split ( key , 1000 ) samples = eqx . filter_vmap ( p1 . sample )( keys ) plt . scatter ( * samples . T ) <matplotlib.collections.PathCollection at 0x7fee85b91540> A probability path is a time dependent probability distribution \\(p_t\\) from \\(t=0\\) to \\(t=1\\) . We want \\(p_0 = N(0,I)\\) and \\(p_1 = p_{\\text{data}}\\) . We can construct this path as the expectation of a conditional probability path: $$ p_t(x_t) = \\int p_1(x_1)p_t(x_t|x_1)dx_1 $$ One choice for the conditional distribution is \\(p_t(x_t|x_1) = N(x_t|tx_1,(1-t)^2I)\\) , which is a linear interpolation between \\(x_0=0\\) and \\(x_1\\) (conditional optimal transport). x_shape = p1 . input_shape transform = gx . ConditionalOptionalTransport ( input_shape = x_shape , key = key ) cond_ppath = TimeDependentNormalizingFlow ( transform = transform , prior = gx . Gaussian ( input_shape = x_shape )) p0 = gx . Gaussian ( input_shape = x_shape ) def sample_xt ( t , key ): k1 , k2 = random . split ( key , 2 ) x1 = p1 . sample ( k1 ) x0 = p0 . sample ( k2 ) xt = cond_ppath . to_data_space ( t , x0 , x1 ) return xt ts = jnp . linspace ( 0 , 1 , 6 ) keys = random . split ( key , 1000 ) xt_samples = jax . vmap ( jax . vmap ( sample_xt , in_axes = ( 0 , None )), in_axes = ( None , 0 ))( ts , keys ) n_rows , n_cols = 1 , ts . shape [ 0 ] size = 4 fig , axes = plt . subplots ( n_rows , n_cols , figsize = ( n_cols * size , n_rows * size )) for i in range ( n_cols ): axes [ i ] . scatter ( * xt_samples [:, i ] . T ) axes [ i ] . set_title ( f \"t= { ts [ i ] : .2f } \" ) Without using any parameters, we constructed a path between a Gaussian and our data. Next, we'll learn a continuous normalizing flow that will learn this probability path. # Construct the neural network that learn the score net = gx . TimeDependentResNet ( input_shape = x_shape , working_size = 16 , hidden_size = 32 , out_size = 2 * x_shape [ - 1 ], n_blocks = 5 , embedding_size = 16 , out_features = 32 , key = key ) from generax.distributions.coupling import OTTCoupling def loss ( net , data , key ): k1 , k2 = random . split ( key , 2 ) # Sample x1 = data [ 'x' ] keys = random . split ( k1 , x1 . shape [ 0 ]) x0 = eqx . filter_vmap ( cond_ppath . prior . sample )( keys ) t = random . uniform ( k2 , shape = ( x1 . shape [ 0 ],)) # Resample from the coupling coupling = OTTCoupling ( x0 , x1 ) x0 = coupling . sample_x0_given_x1 ( k1 ) # Compute f_t(x_0; x_1) def ft ( t ): return eqx . filter_vmap ( cond_ppath . to_data_space )( t , x0 , x1 ) xt , ut = jax . jvp ( ft , ( t ,), ( jnp . ones_like ( t ),)) # Compute the score def log_prob ( xt , x1 , t ): return cond_ppath . log_prob ( t , xt , x1 ) grad_logptx = eqx . filter_vmap ( eqx . filter_grad ( log_prob ))( xt , x1 , t ) # Compute the parametric vector field net_out = eqx . filter_vmap ( net )( t , xt ) st , vt = jnp . split ( net_out , 2 , axis =- 1 ) # Compute the loss objective = jnp . sum (( ut - vt ) ** 2 ) . mean () + 0.01 * jnp . sum (( grad_logptx - st ) ** 2 ) . mean () aux = dict ( objective = objective ) return objective , aux # Create the optimizer import optax schedule = optax . warmup_cosine_decay_schedule ( init_value = 0.0 , peak_value = 1.0 , warmup_steps = 1000 , decay_steps = 3e5 , end_value = 0.1 , exponent = 1.0 ) chain = [] chain . append ( optax . clip_by_global_norm ( 15.0 )) chain . append ( optax . adamw ( 1e-3 )) chain . append ( optax . scale_by_schedule ( schedule )) optimizer = optax . chain ( * chain ) # Create the trainer and optimize trainer = Trainer ( checkpoint_path = 'tmp/flow/flow_and_score_matching' ) net = trainer . train ( model = net , objective = loss , evaluate_model = lambda x : x , optimizer = optimizer , num_steps = 30000 , double_batch = 1000 , data_iterator = p1 . train_iterator ( key , batch_size = 128 ), checkpoint_every = 5000 , test_every =- 1 , retrain = True ) 0%| | 0/30000 [00:00<?, ?it/s]loss: 354807.7500: 20%|\u2588\u2588 | 6000/30000 [01:12<04:41, 85.29it/s] Checkpointed model loss: 1640012.5000: 37%|\u2588\u2588\u2588\u258b | 11009/30000 [02:11<03:39, 86.41it/s] Checkpointed model loss: 339471.9375: 53%|\u2588\u2588\u2588\u2588\u2588\u258e | 16009/30000 [03:08<02:37, 89.08it/s] Checkpointed model loss: 3653279.5000: 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 21009/30000 [04:07<01:44, 86.13it/s] Checkpointed model loss: 10576435.0000: 87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 26009/30000 [05:02<00:45, 87.48it/s] Checkpointed model loss: 11467755.0000: 0%| | 30/30000 [05:49<96:52:20, 11.64s/it] Checkpointed model class Split ( eqx . Module ): net : eqx . Module keepdims : list [ int ] input_shape : tuple [ int ] def __init__ ( self , net , keepdims = None ): self . net = net self . keepdims = jnp . array ( keepdims ) self . input_shape = self . net . input_shape def __call__ ( self , t , x , * args , ** kwargs ): net_out = self . net ( t , x ) return net_out [ self . keepdims ] score = Split ( net , keepdims = [ 0 , 1 ]) vf = Split ( net , keepdims = [ 2 , 3 ]) ts = jnp . linspace ( 0 , 1 , 6 ) node = gx . NeuralODE ( vf = vf ) p0 = gx . Gaussian ( input_shape = x_shape ) def ode_solve ( x0 ): z , log_det = node ( x0 , inverse = True , log_likelihood = True , save_at = ts ) return z , log_det n_samples = 10000 keys = random . split ( key , n_samples ) x0 , log_p0s = eqx . filter_vmap ( p0 . sample_and_log_prob )( keys ) xts , log_dets = jax . vmap ( ode_solve )( x0 ) log_pxs = log_p0s [:, None ] - log_dets n_rows , n_cols = 1 , ts . shape [ 0 ] size = 4 fig , axes = plt . subplots ( n_rows , n_cols , figsize = ( n_cols * size , n_rows * size )) for i in range ( n_cols ): axes [ i ] . scatter ( * xts [:, i ] . T , c = jnp . exp ( log_pxs [:, i ]), alpha = 0.5 , s = 10 ) axes [ i ] . set_title ( f \"t= { ts [ i ] : .2f } \" ) axes [ i ] . set_aspect ( 'equal' , 'box' ) s = eqx . filter_vmap ( score , in_axes = ( None , 0 ))( jnp . array ( 1.0 ), xts [:, - 1 ]) plt . quiver ( * xts [:, - 1 ] . T , * s . T , alpha = 0.5 ) <matplotlib.quiver.Quiver at 0x7fee5d2325f0>","title":"Here we'll show how to train continuous normalizing flows using flow matching"}]}